2/10 sims tracking

- extremely low p_0 = 0.1 - earliest started
  - 10k: tr 2.85, te 3.09, kl 0.069, ssq 1e-5
  - 20k: tr 2.85, te 3.07, kl 0.053, ssq 0
  - 30k: tr 2.85, te 3.08, kl 0.049, ssq 0
  - 40k: tr 2.85, te 3.07, kl 0.048, ssq 0
  - 50k: tr 2.85, te 3.07, kl 0.048, ssq 0
  - 60k: tr 2.85, te 3.09, kl 0.048, ssq 0
  - 70k: tr  , te  , kl  , ssq 0
  - 80k: tr  , te  , kl  , ssq 0
  - 90k: tr  , te  , kl  , ssq 0
  - 00k: tr  , te  , kl  , ssq 0
  - network not learning.  Stop at 100k if similar.


- agnostic tau, fewer covs (40 total) 
  - 10k: tr 2.58, te 2.82, kl 0.431, ssq 0.029
  - 20k: tr 2.27, te 2.62, kl 0.369, ssq 0.033
  - 30k: tr 1.91, te 2.33, kl 0.431, ssq 0.030
  - 40k: tr 1.73, te 2.35, kl 0.530, ssq 0.029
  - 50k: tr 1.62, te 2.40, kl 0.580, ssq 0.028
  - 60k: tr  , te  , kl  , ssq 
  - 70k: tr  , te  , kl  , ssq 
  - 80k: tr  , te  , kl  , ssq 
  - 90k: tr  , te  , kl  , ssq 
  - 00k: tr  , te  , kl  , ssq 


- optimistic p_0 = 10 of 104, scaled
  - 10k: tr 2.30, te 3.00, kl 0.879, ssq 0.00232
  - 20k: tr 1.50, te 2.47, kl 0.935, ssq 0.00213
  - 30k: tr  , te  , kl  , ssq 
  - 40k: tr  , te  , kl  , ssq 
  - 50k: tr  , te  , kl  , ssq 
  - 60k: tr  , te  , kl  , ssq 
  - 70k: tr  , te  , kl  , ssq 
  - 80k: tr  , te  , kl  , ssq 
  - 90k: tr  , te  , kl  , ssq 
  - 00k: tr  , te  , kl  , ssq 




sim_ind = 1
learning_rate = 0.001
nn_model <- MLHS
verbose = TRUE
want_plots = TRUE
want_fcn_plots = TRUE
save_fcn_plots = FALSE
want_all_params = FALSE
local_only = FALSE
save_mod = TRUE
save_results = TRUE










###################################################
SIMULATION NOTES
###################################################
Sims started 10/1:
 - fcnal data, 2-layer, 16-8
 - CUDA on home computer
 - first simulation needs more training (did not converge within max epochs)
 - 2nd simulation seems good so far


























#################################################

article abstract

#################################################

FROM CHATGPT:
We introduce a novel Bayesian framework for variable selection in deep neural networks by imposing a group Horseshoe prior on the weight matrices of fully connected layers. Our approach targets structured sparsity at the level of input covariates by pruning entire rows of weights, rather than individual neurons or connections. This row-wise pruning induces an interpretable selection mechanism directly aligned with the relevance of input variables, enabling scalable non-linear modeling while maintaining statistical parsimony. By leveraging the global-local shrinkage behavior of the group Horseshoe prior, our method adaptively distinguishes relevant signals from noise without requiring manual regularization tuning. We develop an efficient variational inference algorithm tailored to the hierarchical structure of the prior and establish theoretical support for variable selection consistency under mild conditions. Empirical results across simulated and real-world datasets demonstrate that our method achieves competitive predictive performance while recovering sparse, interpretable models that accurately identify influential covariates. This work bridges modern deep learning with Bayesian variable selection, offering a principled and computationally tractable solution for high-dimensional nonlinear inference.



My tries:
We augment the Bayesian Compression algorithm of Louizos, Ullrich, & Welling 2017 with the posterior-based Wald statistics proposed in Liu, Li, Yu, & Zeng 2022 to obtain statistically principled, sparse variable selection within expressive models.

goes beyond simple predictive accuracy to capture complex, non-linear relationships.


statistically principled variable selection 



introduce a novel Bayesian framework for variable selection in deep neural networks by augmenting the Bayesian Compression algorithm (Louizos, Ullrich, and Welling 2017) with a posterior-based Wald statistic


a group Horseshoe prior on the weight matrices of fully connected layers. 




We introduce a novel Bayesian framework for statistically principled variable selection in deep neural networks by combining a group Horseshoe prior on the weight matrices of fully connected layers (Louizos, Ullrich, Welling 2017) with posterior-based Wald Statistics (Liu, Li, Yu, Zeng 2022).  Rather than targeting neural network compression via pruning neurons, we induce sparsity in the input covariates by pruning rows of weights. This row-wise pruning induces an interpretable selection mechanism directly aligned with the relevance of input variables, enabling scalable non-linear modeling while maintaining statistical parsimony. By leveraging the global-local shrinkage behavior of the group Horseshoe prior, our method adaptively distinguishes relevant signals from noise without requiring manual regularization tuning. 

We develop an efficient variational inference algorithm tailored to the hierarchical structure of the prior and establish theoretical support for variable selection consistency under mild conditions. [[Empirical results across simulated and real-world datasets demonstrate that our method achieves competitive predictive performance while recovering sparse, interpretable models that accurately identify influential covariates.]] This work bridges modern deep learning with Bayesian variable selection, offering a principled and computationally tractable solution for high-dimensional nonlinear inference.



































Grad Plan:


Current project:
Structured Bayesian variable selection in deep learning: by applying a structured group sparsity prior in neural networks, we obtain competitive predictive performance simultaneously with statistically principled variable selection on the input features.  While there is much other work on applying Bayesian sparsity priors in deep neural networks, most (if not all) of the literature is focused on network compression or predictive performance, with little to no examination of procedures, characteristics, or performance related to inference.

We suggest a procedure for controlling the Bayesian False Discovery Rate and show in simulation that our method not only achieves network compression and competitive predictive performance, but also
1) exhibits control over the Bayesian FDR nearly matching the nominal FDR,
2) successfully recovers sparse models,
3) and captures flexible, non-linear relationships between covariates, similarly to varying coefficient models.

Progress:  
- finished simulations: simulations for sparse linear regression settings with both single-perceptron and deep networks
- Currently working on simulations for varying-coefficient-type data-generating mechanisms with sparse truth (e.g. 4 covariates truly related to outcome, 100 unrelated).
- Next up: applying model to local null testing problems (that is, data similar to that motivating my previous project, where we want to detect group differences in brain activation data, and in particular, regions where there may not be any group differences).  Likely to use this data as the real-world application dataset.


Plan:
- October: work paper draft detailing completed work; continue working on varying coefficient simulations.  Work on problem discussed below.
- November-January: apply to simulated data in local null testing problem, look for other options for real-world applications.  


Existing / potential difficulties: (in the interest of transparency, as well as looking for guidance / suggestions)
- BFDR control matching nominal rate in simulation is currently achieved using the variational approximations to the horseshoe's *local* shrinkage parameter $\lambda_i$ (via $\Kappa_i = (1 + \lambda_i^2)^{-1}$).  
    - The typical formulation of $\Kappa_i = (1 + \lambda_i^2 \tau^2)^{-1}$ (i.e. including the *global* shrinkage parameter $\tau$ as well as the local $\lambda_i$) does not work nearly as well.  Most of the $\Kappa_i$'s using $\tau$ end up close to 1 because the variational posterior mean for $\tau$ ends up extremely small.  
    - I have a few guesses as to why this happens: 
        1. may be an issue fundamental to the variational inference procedure (mode-seeking behavior of reverse KL underestimates variances)
        2. Overparametrization / overspecification of DNN inclines model towards overestimating how much global shrinkage is needed.  Need to investigate if this is still a problem with smaller / shallower networks.
    - primary theoretical problem with not using the global shrinkage parameter $\tau$ in BFDR calculation: presence of global shrinkage is one of the theoretical justifications for control over multiplicity and interpretation of $\Kappa = (1 + \lambda_i^2 \tau^2)^{-1}$ as a pseudo-PIP.
    - Alternatively, can I justify using only the local shrinkage parameters?  


- half-Cauchy decomposition:
Matthew P Wand, John T Ormerod, Simone A Padoan, Rudolf Fuhrwirth, et al. Mean field variational Bayes for elaborate distributions. Bayesian Analysis, 6(4), 2011.






