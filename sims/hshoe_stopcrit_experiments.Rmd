---
title: "hshoe stopping criteria investigation - 250k training epochs"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    number_sections: true
    code_fold: hide
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)
library(DT)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

ascii_colorize <- function(txt, style = 1, color = 36){
  paste0(
    "\033[0;",
    style, ";",
    color, "m",
    txt,
    "\033[0m"
  )  
}

cat_color <- function(txt, style = 1, color = 36, sep = " "){
  if (is.null(getOption("knitr.in.progress"))){
    cat(
      ascii_colorize(txt, style, color),
      sep = sep
    )  
  } else {
    print(txt)
  }
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "torch_horseshoe.R"))
source(here("Rcode", "sim_functions.R"))
```


<style>

  <!-- changes TOC section number color -->
  .header-section-number {
    color: #032285; 
  } 
  
  <!-- changes header color -->
  h1, h2{
      .font-weight: bolder
    }
</style>




# Purpose - investigate performance of different stopping criteria

2 simulation set results used, both using the same setting, but collecting results at different times, either every 1000 training epochs (set of 200 simulations), and every single epoch (set of 5 only)

Data generation:  

- 4 covariates truly related to outcome (__linearly related__), 100 nuisance variables.  Covariate values all generated from N(0, 1) via `torch_randn()`.  N(0,1) noise

  - i.e. $y = -0.5 x_1 + 1x_2 -2 x_3 + 4 x_4 + \epsilon$
  
- 100 observations in training set, 25 in test set

- true coefficients: -0.5, 1, -2, 4, 0, 0, 0, 0, 0 .... (4 non-zero coefficients, 100 0 coefficients)

- only stopping criteria employed was max number of epochs (training epochs = __250k__)

```{r LOAD_COMBINE_250k_SIMS}
# combine results files
fname_stem <- here::here("sims", "results", "hshoe_linreg_maxepochs_250k")
sim_seeds <- c()
res250 <- list()

for (i in 1:8){
  fname <- paste0(fname_stem, i, ".Rdata")
  load(fname)
  sim_seeds <- c(sim_seeds, contents$sim_params$sim_seeds)
  res250 <- append(res250, contents$res)
}

res250 <- setNames(res250, paste0("sim_", 1:length(res250)))
final_alphas <- t(sapply(res250,
       function(X) X$alpha_mat[nrow(X$alpha_mat),]
       ))

load(here::here("sims", "results", "hshoe_linreg_maxepochs_granular.RData"))
```


```{r QUICK_VIEW_FCNS}

portion_vec <- function(
    vec, 
    portion_size = 1E3, 
    anal_fcn = mean,
    ...
  ){
  n_portions <- length(vec) %/% portion_size
  rem <- length(vec) %% portion_size
  if (rem) {cat_color(paste0(rem, " observations left over \n"))}
  res <- c()
  for (i in 1:n_portions){
    portion_vec <- vec[1:portion_size + portion_size * (i-1)]
    res <- c(res, anal_fcn(portion_vec, ...))
  }
  return(res)
}

ggmatplot <- function(mat, start_row = 1, alpha = 0.75){
  df <- data.frame(mat)
  df$x <- 1:nrow(mat)
  plt <- df %>% 
    filter(x >= start_row) %>% 
    pivot_longer(cols = -x, names_to = "group") %>% 
    ggplot(aes(y = value, x = x, color = group)) +
    geom_line(alpha = alpha)
  return(plt)
}

mad <- function(x){mean(abs(diff(x)))}
sdd <- function(x){sd(diff(x))}



# checks if vector is monotonic (increasing) for k elements ----
monotonic_k <- function(vec, k = 100, increasing = TRUE){
  ind = 1
  mono <- FALSE
  
  if (increasing){
    while ((!mono) & (ind <= (length(vec)-k - 1))){
      mono <- !any(diff(test_mse[ind + 0:k]) < 0)
      ind <- ind + 1
    } 
  } else {
    while ((!mono) & (ind <= (length(vec)-k - 1))){
      mono <- !any(diff(test_mse[ind + 0:k]) > 0)
      ind <- ind + 1
    }
  }
  
  res <- ifelse(mono, ind + k, FALSE)
  return(res)
}
```




# decision criteria based on posterior wald

## max epochs = 250k

```{r}
wald_thresh <- 1 / qchisq(1 - (0.05 / 104), df = 1)
t2_rate <- mean(final_alphas[, 1:4] > wald_thresh)
t1_rate <- mean(final_alphas[, 5:104] < wald_thresh)
print(paste("Type I error at 250k epochs:", t1_rate))
print(paste("Type II error at 250k epochs:", t2_rate))
```

(4 possible true positives, 100 possible true negatives, in each of 200 simulations)


## by epoch

```{r}
get_alphas_by_row <- function(row_num){
 t(sapply(res250,
         function(X) X$alpha_mat[row_num,]
         )) 
}

rowdim <- nrow(res250[[1]]$alpha_mat)
n_sims <- length(res250)

errs_by_epoch <- matrix(NA, nrow = rowdim, ncol = 3)
colnames(errs_by_epoch) <- c("epoch", "T1", "T2")

for (epoch_row in 1:rowdim){
  alphs <- get_alphas_by_row(epoch_row)
  errs_by_epoch[epoch_row, 1] <- epoch_row * 1000
  errs_by_epoch[epoch_row, 2] <- sum(alphs[, 5:104] < wald_thresh)  # FP
  errs_by_epoch[epoch_row, 3] <- sum(alphs[, 1:4] > wald_thresh)    # FN
}

err_rates_by_epoch <- errs_by_epoch
err_rates_by_epoch[, 2] <- errs_by_epoch[, 2] / (100 * n_sims)
err_rates_by_epoch[, 3] <- errs_by_epoch[, 3] / (4 * n_sims)
mykable(err_rates_by_epoch[1:(rowdim/10)* 10, ], cap = "err by epoch")
```


# experiments: Test_mse

## Test_mse minimum

Previous examination suggested that test MSE appears to stabilize and then simply jitter around the same time that T1/T2 rates do as well.  So perhaps tracking minimum test_mse makes sense, and if it does not rise for some number of epochs, we stop training.

- difficulties with implementing this experiment --- 100k length vectors appear too hard for R to handle (at least on this machine).  Break into 10k length vectors to see.


```{r}
# converts vector into streaming minimum
streaming_min <- function(vec, burn_in = 0){
  for (i in 2:length(vec) + burn_in){
    vec[i] <- ifelse(
      vec[i] < vec[i-1], 
      vec[i], 
      vec[i-1] 
    )
  }
  vec
}

# # zoo::rollmax also times out badly around 10k length vector
# rollmin <- -zoo::rollmax(-test_mse[1:10000], k = 100)

# try it out
# kl <- contents$res$sim_1$loss_mat[1:1E5, 1]
# train_mse <- contents$res$sim_1$loss_mat[1:1E5, 2]

piece_num <- 3:24
for (i in piece_num){
  test_mse <- contents$res$sim_1$loss_mat[1:1E4 + 1E4 * i, 3]
  minvec <- streaming_min(test_mse, burn_in = 0)
  # print(table(minvec))
}

```

- tracking min test mse and seeing if doesn't get any lower for $k$ epochs doesn't work very well - would hit stop criteria too soon (commented out print command when knitting)


## test_mse MA increasing for k epochs

```{r}
piece_num <- 3:24
for (i in piece_num){
  test_mse <- contents$res$sim_1$loss_mat[1:1E4 + 1E4 * i, 3]
  ma50 <- zoo::rollmean(test_mse, k = 20)
  ma50_increasing <- monotonic_k(ma50, k = 50)
  print(ma50_increasing)
}
```


- probably too noisy for this (checking if MA is monotonic increasing) to work


## diff(test_mse) < rolling sd(test_mse)?

- window defined by sample standard deviation from last 100?
  - maybe check if stays within 1 std dev?

```{r}
roll_fcn <- function(vec, k = 50, anal_fcn = mean){
  stop_ind <- length(vec) - k
  res <- rep(NA, stop_ind)
  for (i in 1:stop_ind){
    res[i] <- anal_fcn(vec[i:(i + k)]) 
  }
  res
}



portion_size <- 1E4
piece_num <- 25E4 / portion_size

sd_k <- 100    # window used to calculate sd
streak <- 25        # diff in test_mse must be < sd this many times in a row
burn = 5          # times 10k epochs burn-in (e.g. burn = 5 --> 50k epochs)

stop_epochs <- rep(NA, 5)

for (simnum in 1:5){
  cat_color(txt = paste0("\n \n sim ", simnum))
  stop_while = FALSE
  i = burn
  while (i <= piece_num & !stop_while){
    test_mse <- contents$res[[simnum]]$loss_mat[1:portion_size + portion_size * (i-1), 3]
    sd50 <- roll_fcn(test_mse, k = sd_k, anal_fcn = sd)
    smaller <- abs(sd50) > abs(diff(test_mse[(streak):length(test_mse)]))
    stopcrit <- roll_fcn(smaller, k = streak, anal_fcn = all)
    stop_while <- i >= burn & any(stopcrit)
    if (stop_while){
      # cat_color(txt = paste0("\n piece number: ", i, " \n"))
      # print(which(stopcrit))
      stop_epochs[simnum] <- (i-1) * 1e4 + sd_k + streak + min(which(stopcrit))
    }
    i = i+1
  }
}

stop_epochs

```

above is the first time in each simulation that the stop criteria (below) was reached after burn-in of ~ 50k
- sd window = 100, diff(test_mse) < sd(last 100) for 25 consecutive epochs


```{r}

k25_sd100_alphas <- matrix(NA, nrow = length(stop_epochs), ncol = 104) 
for (i in 1:5){
  k25_sd100_alphas[i, ] <- contents$res[[i]]$alpha_mat[stop_epochs[i], ]
}

t1_rate <- mean(k25_sd100_alphas[, 5:104] < wald_thresh)
t2_rate <- mean(k25_sd100_alphas[, 1:4] > wald_thresh)

print(paste0("T1 rate: ", t1_rate))
print(paste0("T2 rate: ", t2_rate))
```

**OK.  this seems to work, at least in this setting.**

- what if not using Bonferroni?

```{r}
wald_thresh <- 1 / qchisq(1 - (0.05 / 104), df = 1)
wald_thresh_nobonf <- 1 / qchisq(.95, df = 1)
t1_rate_no_bonf <- mean(k25_sd100_alphas[, 5:104] < wald_thresh_nobonf)
t2_rate_no_bonf <- mean(k25_sd100_alphas[, 1:4] > wald_thresh_nobonf)
print(paste0("T1 rate, no Bonferroni correction: ", t1_rate_no_bonf))
print(paste0("T2 rate, no Bonferroni correction: ", t2_rate_no_bonf))

```


**Still good.** Not using Bonferroni leads to t1 err of 0.008, (i.e. 4 false positives out of 500 possible)



# experiments: KL

## visual summaries {.tabset .tabset-pills}

```{r}

klmad <- sapply(
  contents$res, 
  function(X) portion_vec(
    vec = X$loss_mat[, 1],
    portion_size = 1E3,
    anal_fcn = mad
  )
)


klsdd <- sapply(
  contents$res, 
  function(X) portion_vec(
    vec = X$loss_mat[, 1],
    portion_size = 1E3,
    anal_fcn = sdd
  )
)
```

### mean abs diff & sd of diffs 

- in  1000 epoch chunks

```{r}

klmad_plt <- ggmatplot(klmad, alpha = .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "mean(abs(diff(kl)))"
  )

klsdd_plt <- ggmatplot(klsdd, alpha = .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "sd(diff(kl))"
  )

grid.arrange(
  klmad_plt + theme(legend.position = "none"), 
  klsdd_plt + theme(legend.position = "none"),
  ncol = 2
)

```


```{r}


ggmatplot(klmad,  start_row = 50, alpha = .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "mean(abs(diff(kl))) after 50k epochs"
  )


ggmatplot(klmad, start_row = 150, alpha = .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "mean(abs(diff(kl))) after 150k epochs"
  )

```


- certainly appears to stabilize and follow similar trajectory across obs



```{r}
ggmatplot(klsdd, start_row = 50, alpha = .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "sd(diff(kl)) after 50k epochs"
  )

ggmatplot(klsdd, start_row = 150, alpha = .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "sd(diff(kl)) after 150k epochs"
  )


```

- sd(diff(kl)) drops quickly, slope tapers off


## kl sd < diff

- is sd over last 100 obs > diff(kl) for 25 in a row?

```{r}
portion_size <- 1E4
piece_num <- 25E4 / portion_size

sd_nobs <- 100    # window used to calculate sd
sd_k <- 50        # diff in test_mse must be < sd this many times in a row
burn = 1          # times 10k epochs burn-in (e.g. burn = 5 --> 50k epochs)

kl_stop_epochs <- rep(NA, 5)

for (simnum in 1:5){
  cat_color(txt = paste0("\n \n sim ", simnum))
  stop_while = FALSE
  i = burn
  while (i <= piece_num & !stop_while){
    kl <- contents$res[[simnum]]$loss_mat[1:portion_size + portion_size * (i-1), 1]
    sd50 <- roll_fcn(kl, k = sd_nobs, anal_fcn = sd)
    smaller <- (.5) * abs(sd50) > abs(diff(kl[sd_nobs:length(kl)]))
    stopcrit <- roll_fcn(smaller, k = sd_k, anal_fcn = all)
    stop_while <- i >= burn & any(stopcrit)
    if (stop_while){
      # cat_color(txt = paste0("\n piece number: ", i, " \n"))
      # print(which(stopcrit))
      kl_stop_epochs[simnum] <- i * portion_size + sd_nobs + sd_k + min(which(stopcrit))
    }
    i = i+1
  }
}

kl_stop_epochs

kl_alphas <- matrix(NA, nrow = length(kl_stop_epochs), ncol = 104) 
for (i in 1:5){
  kl_alphas[i, ] <- contents$res[[i]]$alpha_mat[kl_stop_epochs[i], ]
}

t1_rate <- mean(kl_alphas[, 5:104] < wald_thresh)
t2_rate <- mean(kl_alphas[, 1:4] > wald_thresh)

print(paste0("T1 rate: ", t1_rate))
print(paste0("T2 rate: ", t2_rate))

```


Occurs immediately; Type II error rate is v. high even when making criteria more difficult (hard to figure out principled version of how hard to make it).  Stop criteria based on diff(kl) < $c \times$ sd(kl) is in general too easy, probably because KL is consistently decreasing with just a bit of noise. 



## kl diff < constant `tol` for `streak` epochs

```{r}
tol <- 1E-5
portion_size <- 1E4
piece_num <- 25E4 / portion_size

streak <- 25        # diff in test_mse must be < sd this many times in a row
burn = 1          # times 10k epochs burn-in (e.g. burn = 5 --> 50k epochs)

kl_stop_epochs <- rep(NA, 5)

for (simnum in 1:5){
  cat_color(txt = paste0("\n \n sim ", simnum))
  stop_while = FALSE
  i = burn
  while (i <= piece_num & !stop_while){
    kl <- contents$res[[simnum]]$loss_mat[1:portion_size + portion_size * (i-1), 1]
    smaller <- abs(diff(kl)) < tol
    stopcrit <- roll_fcn(smaller, k = streak, anal_fcn = all)
    stop_while <- i >= burn & any(stopcrit)
    if (stop_while){
      # cat_color(txt = paste0("\n piece number: ", i, " \n"))
      # print(which(stopcrit))
      kl_stop_epochs[simnum] <- (i-1) * portion_size + sd_nobs + sd_k + min(which(stopcrit))
    }
    i = i+1
  }
}

kl_stop_epochs

kl_alphas <- matrix(NA, nrow = length(kl_stop_epochs), ncol = 104) 
for (i in 1:5){
  kl_alphas[i, ] <- contents$res[[i]]$alpha_mat[kl_stop_epochs[i], ]
}

t1_rate <- mean(kl_alphas[, 5:104] < wald_thresh)
t2_rate <- mean(kl_alphas[, 1:4] > wald_thresh)

print(paste0("T1 rate: ", t1_rate))
print(paste0("T2 rate: ", t2_rate))
```


- stop criteria: `diff(kl)` < `tol` for `streak` epochs.
  - downsides ---- `tol` and `streak` need to be calibrated, but unclear how to calibrate.  Changing these a bit leads to wildly different stopping points.



## kl increasing for `streak` epochs?

```{r}
streak <- 10        # diff in test_mse must be < sd this many times in a row
burn = 1          # times 10k epochs burn-in (e.g. burn = 5 --> 50k epochs)

kl_stop_epochs <- rep(NA, 5)

for (simnum in 1:5){
  cat_color(txt = paste0("\n \n sim ", simnum))
  stop_while = FALSE
  i = burn
  while (i <= piece_num & !stop_while){
    kl <- contents$res[[simnum]]$loss_mat[1:portion_size + portion_size * (i-1), 1]
    
    kl_inc <- monotonic_k(kl, k = streak)
    
    stop_while <- i >= burn & kl_inc != FALSE
    if (stop_while){
      # cat_color(txt = paste0("\n piece number: ", i, " \n"))
      # print(which(stopcrit))
      print(kl_inc)
      kl_stop_epochs[simnum] <- (i-1) * portion_size + streak + kl_inc
    }
    i = i+1
  }
}

kl_stop_epochs
```


- tried with `streak` = 50, 25, 10 --- none stopped.



## kl MA(k) increasing for `streak` epochs?

```{r}
ma_k <- 50
streak <- 10        # diff in test_mse must be < sd this many times in a row
burn = 1          # times 10k epochs burn-in (e.g. burn = 5 --> 50k epochs)

kl_stop_epochs <- rep(NA, 5)

for (simnum in 1:5){
  cat_color(txt = paste0("\n sim ", simnum, "\n"))
  stop_while = FALSE
  i = burn
  while (i <= piece_num & !stop_while){
    kl <- contents$res[[simnum]]$loss_mat[1:portion_size + portion_size * (i-1), 1]
    
    kl_ma <- zoo::rollmean(kl, k = ma_k)
    kl_inc <- monotonic_k(kl_ma, k = streak)
    
    stop_while <- i >= burn & kl_inc != FALSE
    if (stop_while){
      # cat_color(txt = paste0("\n piece number: ", i, " \n"))
      # print(which(stopcrit))
      print(kl_inc)
      kl_stop_epochs[simnum] <- (i-1) * portion_size + ma_k + streak + kl_inc
    }
    i = i+1
  }
}

kl_stop_epochs
```

- stop criteria: MA(`ma_k`) of KL increasing for `streak` epochs in a row
- hard to calibrate `ma_k` and `streak`.  Casual search doesn't really find values that work well.





# refining what works

Only thing that seemed to work as stop criteria was based on test MSE, specifically stopping when `abs(diff(test_mse)) < sd` for `streak` epochs in a row, where `sd` is calculated using the last `sd_k` epochs

Basic idea: figure out when gradient steps have stopped making much difference to test_mse.  


```{r}

# try diff values of sd_k and streak

explore_stopcrit <- function(sd_k, streak, burn = 5){
  portion_size <- 1E4
  piece_num <- 25E4 / portion_size
  
  stop_epochs <- rep(NA, 5)
  
  for (simnum in 1:5){
    # cat_color(txt = paste0("\n \n sim ", simnum))
    stop_while = FALSE
    i = burn
    while (i <= piece_num & !stop_while){
      test_mse <- contents$res[[simnum]]$loss_mat[1:portion_size + portion_size * (i-1), 3]
      sd50 <- roll_fcn(test_mse, k = sd_k, anal_fcn = sd)
      mse_diff <- diff(test_mse[sd_k:length(test_mse)])
      smaller <- abs(sd50) > abs(mse_diff)
      stopcrit <- roll_fcn(smaller, k = streak, anal_fcn = all)
      stop_while <- i >= burn & any(stopcrit)
      if (stop_while){
        # cat_color(txt = paste0("\n piece number: ", i, " \n"))
        # print(which(stopcrit))
        stop_epochs[simnum] <- (i-1) * 1e4 + sd_k + streak + min(which(stopcrit))
      }
      i = i+1
    }
  }
  return(stop_epochs)
}
  
retrieve_alphas <- function(stop_epochs, thresh){
  alphs <- matrix(NA, nrow = length(stop_epochs), ncol = 104) 
  for (i in 1:5){
    alphs[i, ] <- contents$res[[i]]$alpha_mat[stop_epochs[i], ]
  }
  
  # calc errs
  t1_rate <- mean(alphs[, 5:104] < thresh)
  t2_rate <- mean(alphs[, 1:4] > thresh)
  
  # results
  return(c(t1_rate, t2_rate))
}

# 
# sd_k_vec <- c(25, 50, 100, 150, 200)
# streak_vec <- c(10, 25, 50, 100)
# 
# for (sdk in sd_k_vec){
#   for (strk in streak_vec){
#     print(paste0("sd_k = ", sdk, "; streak = ", strk))
#     stop_epochs <- explore_stopcrit(sdk, streak = strk, burn = 5)
#     print(stop_epochs)
#     # errs <- retrieve_alphas(stop_epochs, thresh = wald_thresh)
#     # print(errs)
#   }
# }


```



- possible problem: this criteria is most often triggered at the start of training (hence the burn-in period), because the large decreases in `test_mse` at that time lead the std dev calculated using the previous `sd_k` epochs to be large.  
  - i.e. it is triggered right after making large improvements.
  - correspondingly, it's likely that exploration of the parameter space will trigger this stop criteria, particularly in a much more complex network (this is based on the simplest possible NN).

We don't want to discourage network exploration.  In any case, let's see how it works in simulation.  It's possible that it will still work fairly well.


# TO DO:

- investigate using "hot starts," i.e. train a regular DNN with same architecture, then plug in weights.  Right now appear to be wasting slow starting epochs with gigantic MSE.




<!-- ### MAs -->




<!-- ```{r} -->
<!-- # FPs -->
<!-- sum(final_alphas[, 5:104] < wald_thresh) -->
<!-- # TPs -->
<!-- sum(final_alphas[, 1:4] < wald_thresh) -->
<!-- ``` -->

<!-- Since we have 374 positives (FP + TP), we have a large number of repeat crossings.  When do these repeat crossings occur? -->

<!-- ```{r} -->

<!-- ``` -->








<!-- # Granular sim -->

<!-- Ran 5 simulations collecting info **every epoch for 250k epochs** (previous sims only collected every 1000 epochs). -->

<!-- ```{r LOAD_GRANULAR} -->
<!-- load(here::here("sims", "results", "hshoe_linreg_maxepochs_granular.RData")) # 410 MB -->

<!-- loss_arr <- sapply( -->
<!--   contents$res,  -->
<!--   function(X) X$loss_mat,  -->
<!--   simplify = "array" -->
<!-- )  # returns array with dims: 1: epoch;   2: kl, mse_train, mse_test;   3: sim number -->

<!-- alph_arr <- sapply( -->
<!--   contents$res, -->
<!--   function(X) X$alpha_mat, -->
<!--   simplify = "array" -->
<!-- ) -->

<!-- loss_arr <- sapply( -->
<!--   contents$res, -->
<!--   function(X) X$loss_mat, -->
<!--   simplify = "array" -->
<!-- ) -->

<!-- final_alphas <- t(alph_arr[25E4, , ]) -->

<!-- FN <- apply(final_alphas[, 1:4], 1, function(X) any(X > wald_thresh)) -->
<!-- FP <- apply(final_alphas[, 5:104], 1, function(X) any(X < wald_thresh)) -->
<!-- ``` -->

<!-- based on final training epoch alphas for 5 simulations, no false positives, only 1 false negative -->


<!-- ## plots -->

<!-- ```{r} -->

<!-- kl_mat <- loss_arr[, 1, ] -->
<!-- mse_train <- loss_arr[, 2, ] -->
<!-- mse_test <- loss_arr[, 3, ] -->
<!-- mse_diff <- mse_test - mse_train -->
<!-- st <- 125000 -->
<!-- en <- 250000 -->
<!-- matplot(y = kl_mat[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_train[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_test[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_diff[st:en, ], x = st:en, type = "l") -->



<!-- tp <- apply( -->
<!--   alph_arr[, 1:4, ],  -->
<!--   3, -->
<!--   function(X) X < wald_thresh -->
<!-- ) -->

<!-- dim(tp) -->


<!-- ``` -->



<!-- # Rerun simulations: 250K max epochs -->




<!-- - check when KL gets noisy -->
<!-- - check if (unscaled) test / train KL same -->







<!-- s5 <- contents$res$sim_5$alpha_mat[, 2] -->
<!-- s6 <- contents$res$sim_6$alpha_mat[, 2] -->
<!-- s8 <- contents$res$sim_8$alpha_mat[, 2] -->
<!-- s9 <- contents$res$sim_9$alpha_mat[, 2] -->
<!-- df <- data.frame( -->
<!--   s5, s6, s8, s9, -->
<!--   "x" = 1:length(s9) -->
<!-- ) -->

<!-- df %>%  -->
<!--   # filter(x > 60) %>%  -->
<!--   pivot_longer(cols = -"x") %>%  -->
<!--   ggplot(aes(y = value, x = x, color = name)) +  -->
<!--   geom_line() -->



<!-- loss_arr <- sapply(contents$res, -->
<!--                  function(X) X$loss_mat, -->
<!--                  simplify = "array") -->
<!-- kl_mat <- loss_arr[, 1, ] -->
<!-- tr_mat <- loss_arr[, 2, ] -->
<!-- te_mat <- loss_arr[, 3, ] -->
<!-- sp_mat <- tr_mat - te_mat -->



<!-- kl_roll <- apply(kl_mat, 2, function(X) zoo::rollmean(X, k = 10)) -->
<!-- tr_roll <- apply(loss_arr[, 2, ], 2, function(X) zoo::rollmean(X, k = 10)) -->
<!-- te_roll <- apply(loss_arr[, 3, ], 2, function(X) zoo::rollmean(X, k = 10)) -->












