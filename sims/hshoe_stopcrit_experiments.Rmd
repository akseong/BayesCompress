---
title: "hshoe stopping criteria investigation - 250k training epochs"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    number_sections: true
    code_fold: hide
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)
library(DT)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

ascii_colorize <- function(txt, style = 1, color = 36){
  paste0(
    "\033[0;",
    style, ";",
    color, "m",
    txt,
    "\033[0m"
  )  
}

cat_color <- function(txt, style = 1, color = 36, sep = " "){
  if (is.null(getOption("knitr.in.progress"))){
    cat(
      ascii_colorize(txt, style, color),
      sep = sep
    )  
  } else {
    print(txt)
  }
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "torch_horseshoe.R"))
source(here("Rcode", "sim_functions.R"))
```


<style>

  <!-- changes TOC section number color -->
  .header-section-number {
    color: #032285; 
  } 
  
  <!-- changes header color -->
  h1, h2{
      .font-weight: bolder
    }
</style>




# Purpose - investigate performance of different stopping criteria

2 simulation set results used, both using the same setting, but collecting results at different times, either every 1000 training epochs (set of 200 simulations), and every single epoch (set of 5 only)

Data generation:  

- 4 covariates truly related to outcome (__linearly related__), 100 nuisance variables.  Covariate values all generated from N(0, 1) via `torch_randn()`.  N(0,1) noise

  - i.e. $y = -0.5 x_1 + 1x_2 -2 x_3 + 4 x_4 + \epsilon$
  
- 100 observations in training set, 25 in test set

- true coefficients: -0.5, 1, -2, 4, 0, 0, 0, 0, 0 .... (4 non-zero coefficients, 100 0 coefficients)

- only stopping criteria employed was max number of epochs (training epochs = __250k__)

```{r LOAD_COMBINE_250k_SIMS}
# combine results files
fname_stem <- here::here("sims", "results", "hshoe_linreg_maxepochs_250k")
sim_seeds <- c()
res250 <- list()

for (i in 1:8){
  fname <- paste0(fname_stem, i, ".Rdata")
  load(fname)
  sim_seeds <- c(sim_seeds, contents$sim_params$sim_seeds)
  res250 <- append(res250, contents$res)
}

res250 <- setNames(res250, paste0("sim_", 1:length(res250)))
final_alphas <- t(sapply(res250,
       function(X) X$alpha_mat[nrow(X$alpha_mat),]
       ))

load(here::here("sims", "results", "hshoe_linreg_maxepochs_granular.RData"))
```


```{r QUICK_VIEW_FCNS}

portion_vec <- function(
    vec, 
    portion_size = 1E3, 
    anal_fcn = mean,
    ...
  ){
  n_portions <- length(vec) %/% portion_size
  rem <- length(vec) %% portion_size
  if (rem) {cat_color(paste0(rem, " observations left over \n"))}
  res <- c()
  for (i in 1:n_portions){
    portion_vec <- vec[1:portion_size + portion_size * (i-1)]
    res <- c(res, anal_fcn(portion_vec, ...))
  }
  return(res)
}

ggmatplot <- function(mat, start_row = 1, alpha = 0.75){
  df <- data.frame(mat)
  df$x <- 1:nrow(mat)
  plt <- df %>% 
    filter(x >= start_row) %>% 
    pivot_longer(cols = -x, names_to = "group") %>% 
    ggplot(aes(y = value, x = x, color = group)) +
    geom_line(alpha = alpha)
  return(plt)
}

mad <- function(x){mean(abs(diff(x)))}
sdd <- function(x){sd(diff(x))}



# checks if vector is monotonic (increasing) for inc_k elements ----
monotonic_k <- function(vec, inc_k = 100, increasing = TRUE){
  ind = 1
  mono <- FALSE
  
  if (increasing){
    while ((!mono) & (ind <= (length(vec)-inc_k - 1))){
      mono <- !any(diff(test_mse[ind:(ind+inc_k)]) < 0)
      ind <- ind + 1
    } 
  } else {
    while ((!mono) & (ind <= (length(vec)-inc_k - 1))){
      mono <- !any(diff(test_mse[ind:(ind+inc_k)]) > 0)
      ind <- ind + 1
    }
  }
  
  res <- ifelse(mono, ind + k, FALSE)
  return(res)
}
```




# decision criteria based on posterior wald

## max epochs = 250k

```{r}
wald_thresh <- 1 / qchisq(1 - (0.05 / 104), df = 1)
t2_rate <- mean(final_alphas[, 1:4] > wald_thresh)
t1_rate <- mean(final_alphas[, 5:104] < wald_thresh)
print(paste("Type I error at 250k epochs:", t1_rate))
print(paste("Type II error at 250k epochs:", t2_rate))
```

(4 possible true positives, 100 possible true negatives, in each of 200 simulations)


## by epoch

```{r}
get_alphas_by_row <- function(row_num){
 t(sapply(res250,
         function(X) X$alpha_mat[row_num,]
         )) 
}

rowdim <- nrow(res250[[1]]$alpha_mat)
n_sims <- length(res250)

errs_by_epoch <- matrix(NA, nrow = rowdim, ncol = 3)
colnames(errs_by_epoch) <- c("epoch", "T1", "T2")

for (epoch_row in 1:rowdim){
  alphs <- get_alphas_by_row(epoch_row)
  errs_by_epoch[epoch_row, 1] <- epoch_row * 1000
  errs_by_epoch[epoch_row, 2] <- sum(alphs[, 5:104] < wald_thresh)  # FP
  errs_by_epoch[epoch_row, 3] <- sum(alphs[, 1:4] > wald_thresh)    # FN
}

err_rates_by_epoch <- errs_by_epoch
err_rates_by_epoch[, 2] <- errs_by_epoch[, 2] / (100 * n_sims)
err_rates_by_epoch[, 3] <- errs_by_epoch[, 3] / (4 * n_sims)
mykable(err_rates_by_epoch[1:(rowdim/10)* 10, ], cap = "err by epoch")
```


# experiments: Test_mse

## Test_mse minimum

Previous examination suggested that test MSE appears to stabilize and then simply jitter around the same time that T1/T2 rates do as well.  So perhaps tracking minimum test_mse makes sense, and if it does not rise for some number of epochs, we stop training.

- difficulties with implementing this experiment --- 100k length vectors appear too hard for R to handle (at least on this machine).  Break into 10k length vectors to see.


```{r}
# converts vector into streaming minimum
streaming_min <- function(vec, burn_in = 0){
  for (i in 2:length(vec) + burn_in){
    vec[i] <- ifelse(
      vec[i] < vec[i-1], 
      vec[i], 
      vec[i-1] 
    )
  }
  vec
}

# # zoo::rollmax also times out badly around 10k length vector
# rollmin <- -zoo::rollmax(-test_mse[1:10000], k = 100)

# try it out
# kl <- contents$res$sim_1$loss_mat[1:1E5, 1]
# train_mse <- contents$res$sim_1$loss_mat[1:1E5, 2]

piece_num <- 3:24
for (i in piece_num){
  test_mse <- contents$res$sim_1$loss_mat[1:1E4 + 1E4 * i, 3]
  minvec <- streaming_min(test_mse, burn_in = 0)
  # print(table(minvec))
}

```

- tracking min test mse and seeing if doesn't get any lower for $k$ epochs doesn't work very well - would hit stop criteria too soon (commented out print command when knitting)


## test_mse MA increasing for k epochs

```{r}
piece_num <- 3:24
for (i in piece_num){
  test_mse <- contents$res$sim_1$loss_mat[1:1E4 + 1E4 * i, 3]
  ma50 <- zoo::rollmean(test_mse, k = 20)
  ma50_increasing <- monotonic_k(ma50, inc_k = 50)
  print(ma50_increasing)
}
```


- probably too noisy for this (checking if MA is monotonic increasing) to work


## diff(test_mse) < rolling sd(test_mse)?

- window defined by sample standard deviation from last 100?
  - maybe check if stays within 1 std dev?

```{r}
roll_fcn <- function(vec, k = 50, anal_fcn = mean){
  stop_ind <- length(vec) - k
  res <- rep(NA, stop_ind)
  for (i in 1:stop_ind){
    res[i] <- anal_fcn(vec[i:(i + k)]) 
  }
  res
}



portion_size <- 1E4
piece_num <- 25E4 / portion_size

sd_nobs <- 100    # window used to calculate sd
sd_k <- 25        # diff in test_mse must be < sd this many times in a row
burn = 5          # times 10k epochs burn-in (e.g. burn = 5 --> 50k epochs)

stop_epochs <- rep(NA, 5)

for (simnum in 1:5){
  cat_color(txt = paste0("\n \n sim ", simnum))
  stop_while = FALSE
  i = burn
  while (i <= piece_num & !stop_while){
    test_mse <- contents$res[[simnum]]$loss_mat[1:portion_size + portion_size * (i-1), 3]
    sd50 <- roll_fcn(test_mse, k = sd_nobs, anal_fcn = sd)
    smaller <- abs(sd50) > abs(diff(test_mse[(sd_k):length(test_mse)]))
    stopcrit <- roll_fcn(smaller, k = sd_k, anal_fcn = all)
    stop_while <- i >= burn & any(stopcrit)
    if (stop_while){
      # cat_color(txt = paste0("\n piece number: ", i, " \n"))
      # print(which(stopcrit))
      stop_epochs[simnum] <- i * 1e4 + min(which(stopcrit))
    }
    i = i+1
  }
}

stop_epochs

```

above is the first time in each simulation that the stop criteria (below) was reached after burn-in of ~ 50k
- sd window = 100, diff(test_mse) < sd(last 100) for 25 consecutive epochs


```{r}

k25_sd100_alphas <- matrix(NA, nrow = length(stop_epochs), ncol = 104) 
for (i in 1:5){
  k25_sd100_alphas[i, ] <- contents$res[[i]]$alpha_mat[stop_epochs[i], ]
}

t1_rate <- mean(k25_sd100_alphas[, 5:104] < wald_thresh)
t2_rate <- mean(k25_sd100_alphas[, 1:4] > wald_thresh)

print(paste0("T1 rate: ", t1_rate))
print(paste0("T2 rate: ", t2_rate))
```

OK.  this seems to work, at least in this setting.

- what if not using Bonferroni?

```{r}
wald_thresh <- 1 / qchisq(1 - (0.05 / 104), df = 1)
wald_thresh_nobonf <- 1 / qchisq(.95, df = 1)
t1_rate_no_bonf <- mean(k25_sd100_alphas[, 5:104] < wald_thresh_nobonf)
t2_rate_no_bonf <- mean(k25_sd100_alphas[, 1:4] > wald_thresh_nobonf)
print(paste0("T1 rate, no Bonferroni correction: ", t1_rate_no_bonf))
print(paste0("T2 rate, no Bonferroni correction: ", t2_rate_no_bonf))

```


not using Bonferroni leads to t1 err of 0.008, (i.e. 4 false positives out of 500 possible)



# experiments: KL

## checking kl moving average

```{r}

portion_size <- 1E3
n_portions <- nrow(contents$res$sim_1$loss_mat) %/% portion_size

klmat <- matrix(NA, nrow = n_portions, ncol = length(contents$res))


for (sim_ind in 1:length(contents$res)){
  for (i in 1:n_portions){
    kl <- contents$res[[sim_ind]]$loss_mat[1:portion_size + portion_size * (i-1), 1]
    klmat[i, sim_ind] <- mean(abs(diff(kl)))
  }
}

# mean difference for every 1000 obs
matplot(
  y = klmat, 
  x = 1:n_portions, 
  type = "l",
  main = paste0("mean(abs(diff(KL))) per ", portion_size, " epochs")
)

matplot(
  y = klmat[70: n_portions,], 
  x = 70:n_portions, 
  type = "l",
  main = paste0("mean(abs(diff(KL))) per ", portion_size, " epochs")
)

```

- certainly appears to stabilize and follow similar trajectory across obs


```{r}
klma1 <- sapply(
  contents$res, 
  function(X) portion_vec(
    vec = X$loss_mat[, 1],
    portion_size = 1E3,
    anal_fcn = zoo::rollmean,
    k = 50
  )
)



klmad <- sapply(
  contents$res, 
  function(X) portion_vec(
    vec = X$loss_mat[, 1],
    portion_size = 1E3,
    anal_fcn = mad
  )
)
# testing
# all.equal(klmad[,1], klmat[,1])
# all.equal(klmad[,5], klmat[,5])


klsdd <- sapply(
  contents$res, 
  function(X) portion_vec(
    vec = X$loss_mat[, 1],
    portion_size = 1E3,
    anal_fcn = sdd
  )
)



ggmatplot(klsdd[1:nrow(klsdd),], .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "sd(diff(kl))"
  )

ggmatplot(klsdd, start_row = 50, .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "sd(diff(kl)) after 50k epochs"
  )

ggmatplot(klsdd, start_row = 150, .5) + 
  geom_line(stat = "smooth", se = FALSE) + 
  labs(
    title = "sd(diff(kl)) after 50k epochs"
  )


```

- sd(diff(kl)) drops quickly, slope tapers off




<!-- ### MAs -->




<!-- ```{r} -->
<!-- # FPs -->
<!-- sum(final_alphas[, 5:104] < wald_thresh) -->
<!-- # TPs -->
<!-- sum(final_alphas[, 1:4] < wald_thresh) -->
<!-- ``` -->

<!-- Since we have 374 positives (FP + TP), we have a large number of repeat crossings.  When do these repeat crossings occur? -->

<!-- ```{r} -->

<!-- ``` -->








<!-- # Granular sim -->

<!-- Ran 5 simulations collecting info **every epoch for 250k epochs** (previous sims only collected every 1000 epochs). -->

<!-- ```{r LOAD_GRANULAR} -->
<!-- load(here::here("sims", "results", "hshoe_linreg_maxepochs_granular.RData")) # 410 MB -->

<!-- loss_arr <- sapply( -->
<!--   contents$res,  -->
<!--   function(X) X$loss_mat,  -->
<!--   simplify = "array" -->
<!-- )  # returns array with dims: 1: epoch;   2: kl, mse_train, mse_test;   3: sim number -->

<!-- alph_arr <- sapply( -->
<!--   contents$res, -->
<!--   function(X) X$alpha_mat, -->
<!--   simplify = "array" -->
<!-- ) -->

<!-- loss_arr <- sapply( -->
<!--   contents$res, -->
<!--   function(X) X$loss_mat, -->
<!--   simplify = "array" -->
<!-- ) -->

<!-- final_alphas <- t(alph_arr[25E4, , ]) -->

<!-- FN <- apply(final_alphas[, 1:4], 1, function(X) any(X > wald_thresh)) -->
<!-- FP <- apply(final_alphas[, 5:104], 1, function(X) any(X < wald_thresh)) -->
<!-- ``` -->

<!-- based on final training epoch alphas for 5 simulations, no false positives, only 1 false negative -->


<!-- ## plots -->

<!-- ```{r} -->

<!-- kl_mat <- loss_arr[, 1, ] -->
<!-- mse_train <- loss_arr[, 2, ] -->
<!-- mse_test <- loss_arr[, 3, ] -->
<!-- mse_diff <- mse_test - mse_train -->
<!-- st <- 125000 -->
<!-- en <- 250000 -->
<!-- matplot(y = kl_mat[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_train[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_test[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_diff[st:en, ], x = st:en, type = "l") -->



<!-- tp <- apply( -->
<!--   alph_arr[, 1:4, ],  -->
<!--   3, -->
<!--   function(X) X < wald_thresh -->
<!-- ) -->

<!-- dim(tp) -->


<!-- ``` -->



<!-- # Rerun simulations: 250K max epochs -->




<!-- - check when KL gets noisy -->
<!-- - check if (unscaled) test / train KL same -->







<!-- s5 <- contents$res$sim_5$alpha_mat[, 2] -->
<!-- s6 <- contents$res$sim_6$alpha_mat[, 2] -->
<!-- s8 <- contents$res$sim_8$alpha_mat[, 2] -->
<!-- s9 <- contents$res$sim_9$alpha_mat[, 2] -->
<!-- df <- data.frame( -->
<!--   s5, s6, s8, s9, -->
<!--   "x" = 1:length(s9) -->
<!-- ) -->

<!-- df %>%  -->
<!--   # filter(x > 60) %>%  -->
<!--   pivot_longer(cols = -"x") %>%  -->
<!--   ggplot(aes(y = value, x = x, color = name)) +  -->
<!--   geom_line() -->



<!-- loss_arr <- sapply(contents$res, -->
<!--                  function(X) X$loss_mat, -->
<!--                  simplify = "array") -->
<!-- kl_mat <- loss_arr[, 1, ] -->
<!-- tr_mat <- loss_arr[, 2, ] -->
<!-- te_mat <- loss_arr[, 3, ] -->
<!-- sp_mat <- tr_mat - te_mat -->



<!-- kl_roll <- apply(kl_mat, 2, function(X) zoo::rollmean(X, k = 10)) -->
<!-- tr_roll <- apply(loss_arr[, 2, ], 2, function(X) zoo::rollmean(X, k = 10)) -->
<!-- te_roll <- apply(loss_arr[, 3, ], 2, function(X) zoo::rollmean(X, k = 10)) -->












