---
title: "hshoe stopping criteria investigation - 250k training epochs"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    number_sections: true
    code_fold: hide
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)
library(DT)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

ascii_colorize <- function(txt, style = 1, color = 36){
  paste0(
    "\033[0;",
    style, ";",
    color, "m",
    txt,
    "\033[0m"
  )
}

cat_color <- function(txt, style = 1, color = 36, sep = " "){
  cat(
    ascii_colorize(text, style, color),
    sep = sep
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "torch_horseshoe.R"))
source(here("Rcode", "sim_functions.R"))
```


<style>

  <!-- changes TOC section number color -->
  .header-section-number {
    color: #032285; 
  } 
  
  <!-- changes header color -->
  h1, h2{
      .font-weight: bolder
    }
</style>




# Purpose - redo of 100k epochs analysis
trying to figure out stopping criteria to use.  Based on 10 simulations of basic linear regression horseshoe architecture (i.e. only 1 neuron).

Data generation:  

- 4 covariates truly related to outcome (__linearly related__), 100 nuisance variables.  Covariate values all generated from N(0, 1) via `torch_randn()`.  N(0,1) noise

  - i.e. $y = -0.5 x_1 + 1x_2 -2 x_3 + 4 x_4 + \epsilon$
  
- 100 observations in training set, 25 in test set

- true coefficients: -0.5, 1, -2, 4, 0, 0, 0, 0, 0 .... (4 non-zero coefficients, 100 0 coefficients)

- only stopping criteria employed was max number of epochs (training epochs = __250k__)

```{r LOAD_COMBINE_250k_SIMS}
# combine results files
fname_stem <- here::here("sims", "results", "hshoe_linreg_maxepochs_250k")
sim_seeds <- c()
res_comp <- list()

for (i in 1:8){
  fname <- paste0(fname_stem, i, ".Rdata")
  load(fname)
  sim_seeds <- c(sim_seeds, contents$sim_params$sim_seeds)
  res_comp <- append(res_comp, contents$res)
}

res_comp <- setNames(res_comp, paste0("sim_", 1:length(res_comp)))
final_alphas <- t(sapply(res_comp,
       function(X) X$alpha_mat[nrow(X$alpha_mat),]
       ))
```

# Mainline results: 250k training epochs

## Using naive threshold of 0.05

Criteria for model inclusion: is the "dropout rate parameter" $\alpha$ lower than our T1 error threshold.  This interprets $\alpha$ as a probability, which is not great ($\alpha$ is commonly > 1 for the nuisance variables)

### Type II errors:

Total count of Type II errors and T2 error rate over __200__ simulated datasets:

```{r}
# count
sum(final_alphas[, 1:4] > 0.05)
# Type II error rate:
mean(final_alphas[, 1:4] > 0.05)
```


All errors are from the first covariate ($\beta = 0.5$).  Table below shows $\alpha$ for first 4 covariates for all 200 simulated datasets.

```{r T2_err, echo = FALSE}
dt <- DT::datatable(data.frame(round(final_alphas[, 1:4], 3)))
dt %>% formatStyle(1:4, color = styleInterval(0.05, c("black","red")))
```



### T1 error

No spurious variables chosen when setting alpha threshold at 0.05.

```{r T1_err}
# count alphas < 0.05 among nuisance variables
sum(final_alphas[, 5:104] < 0.05)
```



## Interpreting $\alpha$ as posterior-based Wald statistic

A more principled approach might be to compare the dropout parameter $\alpha$ against the inverse of a $\chi^2$ distribution with 1 degree of freedom and applying Bonferroni.  This is based on the idea that

1. **(to justify using $\chi^2 (1)$ distribution)** the $\alpha$ parameter is the inverse of the posterior-based Wald statistic discussed in [Liu, Li, Yu 2020](http://www.mysmu.edu/faculty/yujun/Research/ABT33.pdf) (referred to as LLY 2020);  


1. **(to justify Bonferroni)** the mean-field assumption used in variational inference assumes independence between the individual $\alpha$ parameters ($\alpha_i = \dfrac{Var(\tilde{z_i})}{ \left[ E(\tilde{z_i}) \right] ^2}$.


### LLY 2020

LLY 2020 propose the posterior-based Wald statistic

<!-- $$\begin{aligned} -->
<!--   T(y, \theta_0)  -->
<!--     & = \int (\theta - \theta_0)'[V_{\theta \theta} (\bar\nu)]^{-1} (\theta - \theta_0) p(\nu | y) d\theta -->
<!--     \\ -->
<!--     & = q_\theta + (\bar\theta - \theta_0)'[V_{\theta \theta} (\bar\nu)]^{-1} (\bar\theta - \theta_0) -->
<!--     \\ -->
<!--     & = q_\theta + W -->
<!-- \end{aligned}$$ -->

$$W = (\bar\theta - \theta_0)'[V_{\theta \theta} (\bar\nu)]^{-1} (\bar\theta - \theta_0) \overset{d}{\rightarrow} \chi^2(q_\theta)$$

- $\theta$ is the parameter(s) of interest ($\bar\theta$ refers to the posterior mean), 

- $q_\theta$ the dimension of $\theta$, 

- $V_{\theta \theta} (\bar\nu)$ the portion of the posterior covariance matrix relevant to $\theta$ ($\nu$ refers to all estimated parameters),


### Results for posterior-based Wald interpretation of $\alpha$

#### Type II error

Type II error count & rate:
```{r}
wald_thresh <- 1 / qchisq(1 - (0.05 / 104), df = 1)
t2_sum <- sum(final_alphas[, 1:4] > wald_thresh)
t2_sum # count T2 errors
mean(final_alphas[, 1:4] > wald_thresh) # rate
```

Results: `r t2_sum` errors total out of a possible 800 (4 true covariates, 200 simulations)

I.e. T2 error rate of `r mean(final_alphas[, 1:4] > wald_thresh)`.


#### Type I error

T1 error count & rate (out of 100 nuisance variables * 200 simulations):

```{r}
sum(final_alphas[, 5:104] < wald_thresh)
mean(final_alphas[, 5:104] < wald_thresh)
```

so a T1 error rate of `r mean(final_alphas[, 5:104] < wald_thresh)`









### Problem with posterior Wald interpretation?

Below is a histogram of the $\alpha$ parameters, _for the 100 nuisance parameters_,  appearing in the last training epoch of the 100 simulations, followed by a histogram of 1000 draws from a $\chi^2(1)$.

```{r ALPHA_HISTOGRAMS, out.height="60%", out.width="60%"}
hist(1/final_alphas[, 5:104], xlim = c(0, 5), breaks = 250)
hist(rchisq(1000, df=1), xlim = c(0, 5), breaks = 250)
```

The two do not match well.  However, there are a few possible explanations:

1. variational inference is known to underestimate variance (which would push the mode to the right);

1. maybe explaining why there are NO values below 0.7: in LLY 2020, Theorem 3.1, it is clarified that the Bayesian $W$ and the Frequentist $Wald$ are not quite the same:

$$W = Wald + o_p(1) \overset{d}{\rightarrow} \chi^2(q_\theta)$$


Last, we are just looking at the final training epochs.  These simulations simply set a maximum number of 100k training epochs, so it's possible that the network should have trained for a longer period of time.





### notes / thoughts:

- any way to get ELBOs for competing models to approximate bayes factors?  Is this even useful / desirable?  Avoiding this kind of computation is kind of the reason we're using NN's in the first place....



# Examination for reducing FP / FN?

Looking at different stopping criteria for training the network:

1. "convergence" 

  - **in NEXT analysis** (the results so far save results every 1000 epochs, not every epoch)

  - of test MSE  
  
  - of train MSE  
  
  - of alphas  

1. **test, train, test - train** MSE increasing

  - particularly with test MSE > train MSE

1. rolling mean of the above,



## test, train, test-train MSE increasing {.tabset .tabset-pills}

```{r}
# did simulation have false positive / false negative?
FP_mat <- final_alphas[, 5:104] < wald_thresh
FN_mat <- final_alphas[, 1:4] > wald_thresh
err_df <- data.frame(
  "FP" = apply(FP_mat, 1, any),
  "FN" = apply(FN_mat, 1, any),
  "sim" = paste0("sim_", 1:nrow(FP_mat))
)

err_df$err_type <- ifelse(err_df$FN, "t2" , NA)
err_df$err_type <- ifelse(err_df$FP, "t1" , err_df$err_type)
err_df$err_type <- ifelse(err_df$FP + err_df$FN == 0, "none" , err_df$err_type)

# compile loss_mats from all simulations
res_arr <- sapply(
  res_comp,
  function(X) cbind(X$alpha_mat, X$loss_mat),
  simplify = "array"
)

# loss_arr dims:   1: epoch;   2: kl, mse_train, mse_test;   3: sim_#
loss_arr <- sapply(
  res_comp,
  function(X) X$loss_mat,
  simplify = "array"
)
# alph_arr dims:   1: epoch;   2: coefs   3: sim_#
alph_arr <- sapply(
  res_comp,
  function(X) X$alpha_mat,
  simplify = "array"
)

if (length(unique(sapply(res_arr, nrow))) > 1) {
  res_arr <- res_arr[11:100]
  res_arr <- simplify2array(res_arr)
  loss_arr <- loss_arr[11:100]
  loss_arr <- simplify2array(loss_arr)
  alph_arr <- alph_arr[11:100]
  alph_arr <- simplify2array(alph_arr)
  err_df <- err_df[11:100, ]
}

dimnames(res_arr)[[2]] <- c(
  paste0("alpha0", 1:9),
  paste0("alpha", 10:104),
  "kl", "mse_train", "mse_test" 
)

# colnames(res_arr[, , 1])[105:107]
kl_mat <- res_arr[, 105, ]
train_mat <- res_arr[, 106, ]
test_mat <- res_arr[, 107, ]
tetr_mat <- test_mat - train_mat
```


### train MSE plot

```{r TRAIN_MSE_CONVERGENCE}
ma_k <- 5

train_MA_df <- data.frame(
  apply(
    loss_arr[, 2, ], 
    2, 
    function(X) zoo::rollmean(X, k = ma_k)
  )
)

train_df <- data.frame(train_mat)
train_df$epoch <- as.numeric(rownames(train_mat))
train_MA_df$epoch <- train_df$epoch[ma_k : length(train_df$epoch)]

train_df_long <- train_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "mse_train")
train_MA_df_long <- train_MA_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "MA_mse_train")

train_df_long <- inner_join(train_df_long, err_df, by = "sim")
train_MA_df_long <- inner_join(train_MA_df_long, err_df, by = "sim")

# sample sims to be able to see
display_sims <- paste0("sim_", sample(1:100, 25))


train_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% display_sims) %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = mse_train, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = "train_MSE by error type (70-250k epochs)"
  )
  


train_MA_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% display_sims) %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = MA_mse_train, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0(
      "moving average (",
      ma_k,
      ") train_MSE by error type (70-250k epochs)"
    )
  )

```


### test MSE

```{r}

test_MA_df <- data.frame(
  apply(
    loss_arr[, 3, ], 
    2, 
    function(X) zoo::rollmean(X, k = ma_k)
  )
)

test_df <- data.frame(test_mat)
test_df$epoch <- as.numeric(rownames(test_mat))
test_MA_df$epoch <- test_df$epoch[ma_k : length(test_df$epoch)]

test_df_long <- test_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "mse_test")
test_MA_df_long <- test_MA_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "MA_mse_test")

test_df_long <- inner_join(test_df_long, err_df, by = "sim")
test_MA_df_long <- inner_join(test_MA_df_long, err_df, by = "sim")

test_df_long %>%
  # filter(err_type != "none") %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = mse_test, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = "test_MSE by error type (70-250k epochs)"
  )
  


test_MA_df_long %>%
  # filter(err_type != "none") %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = MA_mse_test, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0(
      "moving average (",
      ma_k,
      ") test_MSE by error type (70-250k epochs)"
    )
  )
```



### test - train

```{r}

tetr_MA_df <- data.frame(
  apply(
    loss_arr[, 3, ] - loss_arr[, 2, ], 
    2, 
    function(X) zoo::rollmean(X, k = ma_k)
  )
)

tetr_df <- data.frame(tetr_mat)
tetr_df$epoch <- as.numeric(rownames(tetr_mat))
tetr_MA_df$epoch <- tetr_df$epoch[ma_k : length(tetr_df$epoch)]

tetr_df_long <- tetr_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "mse_tetr")
tetr_MA_df_long <- tetr_MA_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "MA_mse_tetr")

tetr_df_long <- inner_join(tetr_df_long, err_df, by = "sim")
tetr_MA_df_long <- inner_join(tetr_MA_df_long, err_df, by = "sim")

tetr_df_long %>%
  # filter(err_type != "none") %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = mse_tetr, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = "tetr_MSE by error type (70-250k epochs)"
  )
  


tetr_MA_df_long %>%
  # filter(err_type != "none") %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = MA_mse_tetr, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0(
      "moving average (",
      ma_k,
      ") tetr_MSE by error type (70-250k epochs)"
    )
  )
```




### KL

```{r}
kl_MA_df <- data.frame(
  apply(
    loss_arr[, 1, ], 
    2, 
    function(X) zoo::rollmean(X, k = ma_k)
  )
)

kl_df <- data.frame(kl_mat)
kl_df$epoch <- as.numeric(rownames(kl_mat))
kl_MA_df$epoch <- kl_df$epoch[ma_k : length(kl_df$epoch)]

kl_df_long <- kl_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "kl")
kl_MA_df_long <- kl_MA_df %>%
  pivot_longer(
    cols = !epoch,
    names_to = "sim",
    values_to = "MA_kl")

kl_df_long <- inner_join(kl_df_long, err_df, by = "sim")
kl_MA_df_long <- inner_join(kl_MA_df_long, err_df, by = "sim")

kl_df_long %>%
  # filter(err_type != "none") %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = kl, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = "kl by error type (70-250k epochs)"
  )
  


kl_MA_df_long %>%
  # filter(err_type != "none") %>% 
  filter(epoch > 70000) %>% 
  ggplot(
    aes(
      y = MA_kl, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0(
      "moving average (",
      ma_k,
      ") kl by error type (70-250k epochs)"
    )
  )

```



### conclusions from 100k training epoch sims

KL, train MSE, test MSE not really offering good clues as to T1 / T2 errors, at least looking at results from every 1k epochs for 100k training epochs





## Reverse examination

- look at point when alphas reach threshold and stop changing decisions

```{r}

above_w <- apply(alph_arr, c(2, 3), function(X) X>wald_thresh)
dim(above_w)
change_w <- apply(above_w, c(2, 3), function(X) diff(X))
change_count_by_epoch <- apply(change_w, c(1, 3), function(X) sum(abs(X)))

rowSums(change_count_by_epoch)
sum(change_count_by_epoch[17:81, ])
sum(change_count_by_epoch)
```


Most crosses of the wald-stat threshold occur between epochs 18k - __81k__ (`r sum(change_count_by_epoch[17:81, ])` crossings out of a total of `r sum(change_count_by_epoch)` crossings).

- What is the behavior after 61k epochs?

```{r post_61k_epochs}
# count changes from False Neg to True Post (good)
sum(change_w[61:99, 1:4,] == -1)

# count changes from True Pos to False Neg (bad)
sum(change_w[61:99, 1:4,] == 1)

# how many of these changes are the same variable flipping?
varflips <- apply(
  change_w[61:99, 1:4, ], 
  3, 
  function(X) colSums(abs(X))
)
flips <- apply(varflips, 1, function(X) which(X != 0))
flips
```

So these flips are all occurring in variable 1 (behavior to be expected).  Simulations `r flips[[1]]`


### comparison of flips vs noflips {.tabset .tabset-pills}

#### train_mse 

```{r}
flip_sims <- paste0("sim_", flips[[1]])
display_sims_noflips <- paste0("sim_", sample(setdiff(1:100, flips[[1]]), 20))

lo_epoch <- 30000
hi_epoch <- 250000
epoch_label <- paste0(lo_epoch/1000, "k-", hi_epoch/1000, "k epochs")

train_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% flip_sims) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = mse_train, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  geom_line(stat = "smooth", alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("flipsims: train_MSE by error type; ", epoch_label)
  ) 


train_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% display_sims_noflips) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = mse_train, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  geom_line(stat = "smooth", alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("noflips: train_MSE by error type; ", epoch_label)
  ) 
```

- __interestingly, all of the Type 1 errors (4) are in the simulations which flip__.  However, not all of the flipping simulations contained T1 errors at 250k epochs.


#### test_mse

```{r}

test_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% flip_sims) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = mse_test, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  geom_line(stat = "smooth", alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("flipsims: test_MSE by error type; ", epoch_label)
  ) 

test_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% display_sims_noflips) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = mse_test, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  geom_line(stat = "smooth", alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("noflips: test_MSE by error type; ", epoch_label)
  )
```



__Note:__ test mse appears to level off around 100k epochs.



#### kl

```{r}
kl_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% flip_sims) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = kl, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("flipsims: kl by error type; ", epoch_label)
  ) 

kl_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% display_sims_noflips) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = kl, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("noflips: kl by error type; ", epoch_label)
  )
```


#### test - train

```{r}
tetr_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% flip_sims) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = mse_tetr, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  geom_line(stat = "smooth", alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("flipsims: test-train MSE by error type; ", epoch_label)
  )

tetr_df_long %>%
  # filter(err_type != "none") %>% 
  filter(sim %in% display_sims_noflips) %>% 
  filter(epoch > lo_epoch & epoch < hi_epoch) %>% 
  ggplot(
    aes(
      y = mse_tetr, 
      x = epoch, 
      group = sim,
      color = sim)
  ) + 
  geom_line(alpha = 0.25) + 
  geom_line(stat = "smooth", alpha = 0.5) + 
  facet_wrap(~err_type, nrow = 3) + 
  theme(legend.position = "none") + 
  labs(
    title = paste0("noflips: test-train MSE by error type; ", epoch_label)
  )
```


## Conclusions

- train / test mse seems a bit too noisy to give us clues here as to when alphas stop flipping across the wald threshold.  

  - Need to look at every iteration instead of every 1000 iterations
  
  - or look at the first-order differences directly?

- in any case, KL looks like the thing to look at --- smoothly decreasing.  

  - Epsilon used to assess convergence should be scaled by $n$.

- overall, training metrics appear fairly stable around 60k epochs


### at 60k epochs:

```{r alphas_60k}
get_alphas_by_row <- function(row_num){
 t(sapply(res_comp,
         function(X) X$alpha_mat[row_num,]
         )) 
}

alphas_60k <- get_alphas_by_row(60)
sum(alphas_60k[, 5:104] < wald_thresh)  # T1
sum(alphas_60k[, 1:4] > wald_thresh)    # T2
```



### over epochs

```{r}
rowdim <- nrow(res_comp[[1]]$loss_mat)
errs_by_epoch <- matrix(NA, nrow = rowdim, ncol = 3)
colnames(errs_by_epoch) <- c("epoch", "T1", "T2")

for (epoch_row in 1:rowdim){
  alphs <- get_alphas_by_row(epoch_row)
  errs_by_epoch[epoch_row, 1] <- epoch_row * 1000
  errs_by_epoch[epoch_row, 2] <- sum(alphs[, 5:104] < wald_thresh)  # FP
  errs_by_epoch[epoch_row, 3] <- sum(alphs[, 1:4] > wald_thresh)    # FN
}

err_rates_by_epoch <- errs_by_epoch
err_rates_by_epoch[, 2] <- errs_by_epoch[, 2] / (100 * 200)
err_rates_by_epoch[, 3] <- errs_by_epoch[, 3] / (4 * 200)

mykable(err_rates_by_epoch[1:(rowdim/10)* 10, ], cap = "err by epoch")
```

Only marginal improvements when stopping after ~~50k epochs~~ 150k epochs.


<!-- # Exploration for figuring out convergence -->

```{r}

mykable(err_rates_by_epoch, cap = "err by epoch")

err_df <- as.data.frame(err_rates_by_epoch)
err_df %>% 
  filter(epoch > 5E4) %>% 
  pivot_longer(
    cols = -epoch,
    names_to = "err_type",
    values_to = "rate"
  ) %>% 
  ggplot(
    aes(
      y = rate,
      x = epoch,
      color = err_type
    )
  ) + 
  geom_line() + 
  labs(title = "error rate by epoch over 200 simulations")
```


__Note:__ appears to level off around 150k epochs.









<!-- ### MAs -->




<!-- ```{r} -->
<!-- # FPs -->
<!-- sum(final_alphas[, 5:104] < wald_thresh) -->
<!-- # TPs -->
<!-- sum(final_alphas[, 1:4] < wald_thresh) -->
<!-- ``` -->

<!-- Since we have 374 positives (FP + TP), we have a large number of repeat crossings.  When do these repeat crossings occur? -->

<!-- ```{r} -->

<!-- ``` -->








<!-- # Granular sim -->

<!-- Ran 5 simulations collecting info **every epoch for 250k epochs** (previous sims only collected every 1000 epochs). -->

<!-- ```{r LOAD_GRANULAR} -->
<!-- load(here::here("sims", "results", "hshoe_linreg_maxepochs_granular.RData")) # 410 MB -->

<!-- loss_arr <- sapply( -->
<!--   contents$res,  -->
<!--   function(X) X$loss_mat,  -->
<!--   simplify = "array" -->
<!-- )  # returns array with dims: 1: epoch;   2: kl, mse_train, mse_test;   3: sim number -->

<!-- alph_arr <- sapply( -->
<!--   contents$res, -->
<!--   function(X) X$alpha_mat, -->
<!--   simplify = "array" -->
<!-- ) -->

<!-- loss_arr <- sapply( -->
<!--   contents$res, -->
<!--   function(X) X$loss_mat, -->
<!--   simplify = "array" -->
<!-- ) -->

<!-- final_alphas <- t(alph_arr[25E4, , ]) -->

<!-- FN <- apply(final_alphas[, 1:4], 1, function(X) any(X > wald_thresh)) -->
<!-- FP <- apply(final_alphas[, 5:104], 1, function(X) any(X < wald_thresh)) -->
<!-- ``` -->

<!-- based on final training epoch alphas for 5 simulations, no false positives, only 1 false negative -->


<!-- ## plots -->

<!-- ```{r} -->

<!-- kl_mat <- loss_arr[, 1, ] -->
<!-- mse_train <- loss_arr[, 2, ] -->
<!-- mse_test <- loss_arr[, 3, ] -->
<!-- mse_diff <- mse_test - mse_train -->
<!-- st <- 125000 -->
<!-- en <- 250000 -->
<!-- matplot(y = kl_mat[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_train[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_test[st:en, ], x = st:en, type = "l") -->
<!-- matplot(y = mse_diff[st:en, ], x = st:en, type = "l") -->



<!-- tp <- apply( -->
<!--   alph_arr[, 1:4, ],  -->
<!--   3, -->
<!--   function(X) X < wald_thresh -->
<!-- ) -->

<!-- dim(tp) -->


<!-- ``` -->



<!-- # Rerun simulations: 250K max epochs -->




<!-- - check when KL gets noisy -->
<!-- - check if (unscaled) test / train KL same -->







<!-- s5 <- contents$res$sim_5$alpha_mat[, 2] -->
<!-- s6 <- contents$res$sim_6$alpha_mat[, 2] -->
<!-- s8 <- contents$res$sim_8$alpha_mat[, 2] -->
<!-- s9 <- contents$res$sim_9$alpha_mat[, 2] -->
<!-- df <- data.frame( -->
<!--   s5, s6, s8, s9, -->
<!--   "x" = 1:length(s9) -->
<!-- ) -->

<!-- df %>%  -->
<!--   # filter(x > 60) %>%  -->
<!--   pivot_longer(cols = -"x") %>%  -->
<!--   ggplot(aes(y = value, x = x, color = name)) +  -->
<!--   geom_line() -->



<!-- loss_arr <- sapply(contents$res, -->
<!--                  function(X) X$loss_mat, -->
<!--                  simplify = "array") -->
<!-- kl_mat <- loss_arr[, 1, ] -->
<!-- tr_mat <- loss_arr[, 2, ] -->
<!-- te_mat <- loss_arr[, 3, ] -->
<!-- sp_mat <- tr_mat - te_mat -->



<!-- kl_roll <- apply(kl_mat, 2, function(X) zoo::rollmean(X, k = 10)) -->
<!-- tr_roll <- apply(loss_arr[, 2, ], 2, function(X) zoo::rollmean(X, k = 10)) -->
<!-- te_roll <- apply(loss_arr[, 3, ], 2, function(X) zoo::rollmean(X, k = 10)) -->












