---
title: "hshoe stopping criteria investigation"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    number_sections: true
    # code_fold: show
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)
library(DT)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

ascii_colorize <- function(txt, style = 1, color = 36){
  paste0(
    "\033[0;",
    style, ";",
    color, "m",
    txt,
    "\033[0m"
  )
}

cat_color <- function(txt, style = 1, color = 36, sep = " "){
  cat(
    ascii_colorize(text, style, color),
    sep = sep
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "torch_horseshoe.R"))
source(here("Rcode", "sim_functions.R"))
```


<style>

  <!-- changes TOC section number color -->
  .header-section-number {
    color: #032285; 
  } 
  
  <!-- changes header color -->
  h1, h2{
      .font-weight: bolder
    }
</style>




# Purpose
trying to figure out stopping criteria to use.  Based on 10 simulations of basic linear regression horseshoe architecture (i.e. only 1 neuron).

Data generation:  

- 4 covariates truly related to outcome (__linearly related__), 100 nuisance variables.  Covariate values all generated from N(0, 1) via `torch_randn()`.  N(0,1) noise

  - i.e. $y = -0.5 x_1 + 1x_2 -2 x_3 + 4 x_4 + \epsilon$
  
- 100 observations in training set, 25 in test set

- true coefficients: -0.5, 1, -2, 4, 0, 0, 0, 0, 0 .... (4 non-zero coefficients, 100 0 coefficients)

- only stopping criteria employed was max number of epochs (training epochs = 70k)

```{r}
# combine results files
fname_stem <- here::here("sims", "results", "hshoe_linreg_maxepochs")
sim_seeds <- c()
res_comp <- list()

for (i in 1:10){
  fname <- paste0(fname_stem, i, ".Rdata")
  load(fname)
  sim_seeds <- c(sim_seeds, contents$sim_params$sim_seeds)
  res_comp <- append(res_comp, contents$res)
}

res_comp <- setNames(res_comp, paste0("sim_", 1:length(res_comp)))
final_alphas <- t(sapply(res_comp,
       function(X) X$alpha_mat[nrow(X$alpha_mat),]
       ))
```

# Mainline results: 70k training epochs

## Using naive threshold of 0.05

Criteria for model inclusion: is the "dropout rate parameter" $\alpha$ lower than our T1 error threshold.  This interprets $\alpha$ as a probability, which is not great ($\alpha$ is commonly > 1 for the nuisance variables)

### Type II errors:

Total count of Type II errors over 100 simulated datasets:

```{r}
sum(final_alphas[, 1:4] > 0.05)
```

Type II error rate:
```{r}
mean(final_alphas[, 1:4] > 0.05)
```


All errors are from the first covariate ($\beta = 0.5$).  Table below shows $\alpha$ for first 4 covariates for all 100 simulated datasets.

```{r T2_err, echo = FALSE}
dt <- DT::datatable(data.frame(round(final_alphas[, 1:4], 3)))
dt %>% formatStyle(1:4, color = styleInterval(0.05, c("black","red")))
```



### T1 error

No spurious variables chosen when setting alpha threshold at 0.05.

```{r T1_err}
# count alphas < 0.05 among nuisance variables
sum(final_alphas[, 5:104] < 0.05)
```



## Interpreting $\alpha$ as posterior-based Wald statistic

A more principled approach might be to compare the dropout parameter $\alpha$ against the inverse of a $\chi^2$ distribution with 1 degree of freedom and applying Bonferroni.  This is based on the idea that

1. **(to justify using $\chi^2 (1)$ distribution)** the $\alpha$ parameter is the inverse of the posterior-based Wald statistic discussed in [Liu, Li, Yu 2020](http://www.mysmu.edu/faculty/yujun/Research/ABT33.pdf) (referred to as LLY 2020);  


1. **(to justify Bonferroni)** the mean-field assumption used in variational inference assumes independence between the individual $\alpha$ parameters ($\alpha_i = \dfrac{Var(\tilde{z_i})}{ \left[ E(\tilde{z_i}) \right] ^2}$.


### LLY 2020

LLY 2020 propose the posterior-based Wald statistic

<!-- $$\begin{aligned} -->
<!--   T(y, \theta_0)  -->
<!--     & = \int (\theta - \theta_0)'[V_{\theta \theta} (\bar\nu)]^{-1} (\theta - \theta_0) p(\nu | y) d\theta -->
<!--     \\ -->
<!--     & = q_\theta + (\bar\theta - \theta_0)'[V_{\theta \theta} (\bar\nu)]^{-1} (\bar\theta - \theta_0) -->
<!--     \\ -->
<!--     & = q_\theta + W -->
<!-- \end{aligned}$$ -->

$$W = (\bar\theta - \theta_0)'[V_{\theta \theta} (\bar\nu)]^{-1} (\bar\theta - \theta_0) \overset{d}{\rightarrow} \chi^2(q_\theta)$$

- $\theta$ is the parameter(s) of interest ($\bar\theta$ refers to the posterior mean), 

- $q_\theta$ the dimension of $\theta$, 

- $V_{\theta \theta} (\bar\nu)$ the portion of the posterior covariance matrix relevant to $\theta$ ($\nu$ refers to all estimated parameters),


### Results for posterior-based Wald interpretation of $\alpha$

#### Type II error

```{r}
wald_thresh <- 1 / qchisq(1 - (0.05 / 104), df = 1)
t2_sum <- sum(final_alphas[, 1:4] > wald_thresh)
t2_sum # T2 errors
```

Results: `r t2_sum` errors total out of a possible 400 (4 true covariates, 100 simulations)

I.e. T2 error rate of `r mean(final_alphas[, 1:4] > wald_thresh)`.


#### Type I error

T1 errors (out of 100 nuisance variables * 100 simulations):

```{r}
sum(final_alphas[, 5:104] < wald_thresh)
```

so a T1 error rate of `r mean(final_alphas[, 5:104] < wald_thresh)`









### Problem with posterior Wald interpretation?

Below is a histogram of the $\alpha$ parameters, _for the 100 nuisance parameters_,  appearing in the last training epoch of the 100 simulations, followed by a histogram of 1000 draws from a $\chi^2(1)$.

```{r, out.height="60%", out.width="60%"}
hist(1/final_alphas[, 5:104], xlim = c(0, 5), breaks = 250)
hist(rchisq(1000, df=1), xlim = c(0, 5), breaks = 250)
```

The two do not match well.  However, there are a few possible explanations:

1. variational inference is known to underestimate variance (which would push the mode to the right);

1. maybe explaining why there are NO values below 0.7: in LLY 2020, Theorem 3.1, it is clarified that the Bayesian $W$ and the Frequentist $Wald$ are not quite the same:

$$W = Wald + o_p(1) \overset{d}{\rightarrow} \chi^2(q_\theta)$$


Last, we are just looking at the final training epochs.  These simulations simply set a maximum number of 70k training epochs, so it's possible that the network should have trained for a longer period of time.





## notes / thoughts:

- any way to get ELBOs for competing models to approximate bayes factors?  Is this even useful / desirable?  Avoiding this kind of computation is kind of the reason we're using NN's in the first place....



# Questions from 70k max epochs simulations: 

1. Have we trained long enough?

1. Signs of convergence?




# Granular sim

Ran 5 simulations collecting info at every epoch for 250k epochs (usually only collect every 1000 epochs).

```{r LOAD_GRANULAR}
load(here::here("sims", "results", "hshoe_linreg_maxepochs_granular.RData")) # 410 MB

loss_arr <- sapply(
  contents$res, 
  function(X) X$loss_mat, 
  simplify = "array"
)  # returns array with dims: 1: epoch;   2: kl, mse_train, mse_test;   3: sim number

alph_arr <- sapply(
  contents$res,
  function(X) X$alpha_mat,
  simplify = "array"
)
dim(alph_arr)
  
```






# Rerun simulations: 250K max epochs











s5 <- contents$res$sim_5$alpha_mat[, 2]
s6 <- contents$res$sim_6$alpha_mat[, 2]
s8 <- contents$res$sim_8$alpha_mat[, 2]
s9 <- contents$res$sim_9$alpha_mat[, 2]
df <- data.frame(
  s5, s6, s8, s9,
  "x" = 1:length(s9)
)

df %>% 
  # filter(x > 60) %>% 
  pivot_longer(cols = -"x") %>% 
  ggplot(aes(y = value, x = x, color = name)) + 
  geom_line()



loss_arr <- sapply(contents$res,
                 function(X) X$loss_mat,
                 simplify = "array")
kl_mat <- loss_arr[, 1, ]
tr_mat <- loss_arr[, 2, ]
te_mat <- loss_arr[, 3, ]
sp_mat <- tr_mat - te_mat



kl_roll <- apply(kl_mat, 2, function(X) zoo::rollmean(X, k = 10))
tr_roll <- apply(loss_arr[, 2, ], 2, function(X) zoo::rollmean(X, k = 10))
te_roll <- apply(loss_arr[, 3, ], 2, function(X) zoo::rollmean(X, k = 10))












