---
title: "Model writeup"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{boldmath}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
  theme: cerulean
  highlight: tango
  toc: yes
  toc_depth: 3
  toc_float:
    collapsed: false
  smooth_scroll: true
  code_fold: show
  urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---
  
  
<style type="text/css">
  
  #TOC>ul>li, h1, h2{
    font-weight: bold;
  }
  
  blockquote{
    background-color: #d5f2f5;
      font-size: small;
  }
  
  b, strong {
    color: #11579e;
  }
  
  caption {
    color: #11579e;
      font-weight: bold;
    font-size: 1.0em;
  }
  
  h1 { font-size: 2em }
  h2 { font-size: 1.6em  }
  h3 { font-size: 1.25em }
  h4 { font-size: 1.10em }
  h5, h6 { font-size: 1em    }
  
  h1, h2, h3 {font-weight: bold;}
  
  a {font-weight: bold; text-decoration: underline}
  
  .nav-pills>li>a:hover, 
  .nav-pills>li>a:focus, 
  .nav-pills>li.active>a,     
  .nav-pills>li.active>a:hover, 
  .nav-pills>li.active>a:focus{
    background-color: #11579e;
  }
  
  .nav-pills{
    background-color: #d5f2f5;
  }
</style>


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# model packages
library(torch)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

# notin
`%notin%` <- Negate(`%in%`)

source(here("Rcode", "torch_horseshoe_klcorrected.R"))
source(here("Rcode", "sim_functions.R"))
source(here("Rcode", "analysis_fcns.R"))
```





# Setting:

## data-generating mechanism

We assume a setting in which data $X$ is collected on many characteristics, of which only an unknown subset $X^\star \subseteq X$ have any relation (whether linear or not) to the response $y$.  That is, we assume the response $y_i$ is generated by an unknown function $f^\star$ of a subset $X^\star_i$ of the covariate vector $X_i$ such that $f^\star(X^\star_i) = f^\star(X_i)$.  For now, we invoke the usual assumptions of i.i.d. samples and homoskedastic, Gaussian noise.  The assumed data-generating mechanism can be then expressed as

$$y_i \sim N \left( f^\star (X_i), \sigma^2 \right)$$


Our primary goal is to recover the subset of covariates $X^\star$ that account for variation in $y$ while also providing control over false positives via Bayesian FDR.

Our secondary goals include

1. recovery of the functional form of relationships between covariates in $X^\star$ and the response;

1. competitive predictive accuracy;

1. uncertainty quantification.




Our idealized setting: 

  - large-ish $n$ (large enough to sustain use of a neural network);
  
  - i.i.d. random sample
  
  - many measured covariates, only a small handful of which actually have an effect on the response;
  
  - normally-distributed, homoscedastic, i.i.d. noise
  
  

## Model

We choose to model $f^\star(X_i)$ using a Bayesian neural network $f(X_i, W)$ equipped with a group Horseshoe prior structured as proposed by Louizos, Ullrich, Welling 2017.

Notation is as follows:

- $i \in \{1, 2, ..., n\}$ indexes the observations;

- $l \in \{1, 2, ..., L\}$ indexes the layers of the neural network;

- $d_l$ is the number of neurons in layer $l$;

- $j \in \{1, 2, .., d_{l-1}\}$ indexes the rows of the weight matrix (or, equivalently, the columns of the input matrix) for layer $l$;

- $k \in \{1, 2, .., d_{l}\}$ indexes the columns of the weight matrix in layer $l$.

Note that $W_l$, the weight matrix in layer $l$, has dimension $d_{l-1} \times d_l$, and that $d_0 = p$, the number of covariates in $X$.  We equip the neural network weights with a horseshoe prior structured in such a way to allow for variable selection: rather than giving every weight its own local scale parameter (thus shrinking individual weights), the local scale parameters $\lambda_j$ are shared by the weights in the $j$'th _row_ of $W_l$, effectively coupling all weights that correspond to the $j$'th column of the input for layer $l$ .  

$$\begin{align}
  y_i | W 
    & \sim N\left( f(X_i, W), \sigma^2 \right)
  \\
  W | \lambda, \tau
    & \sim 
    \prod_{l \in \{1, 2, ..., L\}}
    \prod_{
    \substack{
      j \in \{1, 2, .., d_{l-1}\}, \\ 
      k \in \{1, 2, ... d_l\}}
    } 
    N(w_{j,k} | 0, \lambda_{j}^2 \tau_l^2)
  \\
  \lambda_j 
    & \sim C^+ (0, 1)
  \\
  \tau_l 
    & \sim C^+ (0, \tau_0)
\end{align}$$

Note that the individual weights $w_{l,j,k}$ in $W$ can be rewritten as the product of independent standard Normal random variables with the corresponding elements of $\lambda$ and $\tau$, i.e. given $\lambda, \tau$, if $\tilde{w} \sim N(0,1)$, we can express $w_{j,k} = \tilde{w}_{j,k} \lambda_j \tau_l$.

Following Louizos, Ullrich, Welling 2017, we decompose the half-Cauchy priors into products of independent Gamma and inverse-Gamma distributed variables in order to easily compute the KL-divergence for the log-Normal variational distributions.  Dropping the layer subscript $l$ to ease notation, we have:

$$\lambda_j^2 = \alpha\beta;  
\quad \alpha_j \sim \text{Gamma} (1/2, 1); 
\quad \beta_j \sim \text{inv-Gamma} (1/2, 1)$$

$$\tau^2 = a b;
\quad a \sim \text{Gamma} (1/2, \tau_0^2); 
\quad b \sim \text{inv-Gamma} (1/2, 1)$$

> Question: should I include my derivation of this decomposition since I can't find a source for it anywhere?  The [usual version](https://www.maths.usyd.edu.au/u/jormerod/JTOpapers/NevilleOrmerodWand.pdf) is "If $a \sim IG(1/2, A^{-2})$ and $x|a \sim IG(1/2, a^{-1})$ then $\sqrt{x} \sim C^+(A)$."  What I am doing is from LUW2017 (who cite the linked paper), but they don't elaborate/justify it and I don't see it anywhere else.  Or is it obvious enough that it's not needed?

Then we can re-express the hierarchy as:


$$\begin{align}
  W 
    & = 
    \left\{ 
      w_{l, j,k}: l \in \{1, 2, .., L \},
                  j \in \{1, 2, .., d_{l-1} \}, 
                  k \in \{1, 2, ... d_l \}
    \right\}
  \\
  w_{l, j, k}
    & = \tilde{w}_{l, j, k} \lambda_{l,j} \tau_l
      = \tilde{w}_{l, j, k} \sqrt{\alpha_{l, j} \beta_{l, j} a_l b_l}
  \\
  \tilde{w}_{l, j, k}
    & \sim N(0, 1)
  \\
  \alpha_{l, j} 
    & \sim \text{Gamma} (1/2, 1) 
  \\
  \beta_{l, j} 
    & \sim \text{inv-Gamma} (1/2, 1)
  \\
  a_l 
    & \sim \text{Gamma} (1/2, \tau_0^2)
  \\
  b_l 
    & \sim \text{inv-Gamma} (1/2, 1)
\end{align}$$

> having trouble with indices because trying to make expressions generalizable to all layers, e.g. making clear the layers can have different $j$'s and $k$'s.  Do the $j$'s and $k$'s need sub-indices?  $W$ is the collection of $L$ weight matrices $W_l$, not an 3D array ($W_l$'s are not necessarily of the same dimension).  Should I superscript $l$??  Just write everything out as separate for each layer (since that is essentially how it works)?  ugh.




Note that proper calibration of the prior global scale parameter $\tau_0$ is imperative in practical usage, and that the commonly used default of $\tau_0 = 1$ is in most cases far too large (Piironen & Vehtari 2017).  Instead, we follow Pirronen & Vehtari 2017 and set $\tau_0 = \dfrac{p_0}{D - p_0}  \dfrac{\sigma}{\sqrt{n}}$, where $p_0$ is a prior guess at the number of covariates (or, more generally in a deep neural network, input matrix columns) that should be included in the model, $D$ is the total number of covariates or input matrix columns, and $\sigma$ is the noise standard deviation.  Calibration of $\tau_0$ is most important for the input layer, but still matters in hidden layers as we rely on secondary layers to furnish estimates used to correct for the effect of neuron pruning in previous layers.  For hidden layers, we set $p_0$ to be a reasonably agnostic $D / 2$.    



## Variational Distributions

We employ a mean-field variational inference algorithm to approximate the posterior distributions of model parameters.  Every variational distribution apart from $q_\phi(\tilde{w}_{l,j,k}) = N(\mu_{w_{l,j,k}}, \sigma^2_{w_{l,j,k}})$ is log-Normal with their own mean and variances.


$$\begin{align}
  q_\phi(\alpha_{l, j}) 
  & 
    = \text{log-Normal}(\mu_{\alpha_{l, j}}, \sigma^2_{\alpha_{l, j}}),
    & 
    q_\phi(\beta_{l,j})
    & 
    = \text{log-Normal}(\mu_{\beta_{l, j}}, \sigma^2_{\beta_{l, j}})
  \\
  q_\phi(a_l) 
  & 
    = \text{log-Normal}(\mu_{a_l}, \sigma^2_{a_l}),
    & 
    q_\phi(b_l)
    & 
    = \text{log-Normal}(\mu_{b_l}, \sigma^2_{b_l})
  \\
\end{align}$$


### Kullback-Leibler Divergences

$$\begin{align}
  KL \left( LN(\mu, \sigma^2) ~ || ~ Gamma(\alpha, k) \right)
  & =
    - \dfrac{1}{2} \left( \log(2 \pi \sigma^2) + 1 \right)
    + \log \left( \Gamma(\alpha) \right)
    + \alpha (\log k - \mu)
    + \dfrac{1}{k} \exp \left( \mu + \dfrac{\sigma^2}{2} \right)
  \\
  KL \left( LN(\mu, \sigma^2) ~ || ~ InvGamma(\alpha, k) \right)
  & =
    - \dfrac{1}{2} \left( \log(2 \pi \sigma^2) + 1 \right)
    + \log \left( \Gamma(\alpha) \right)
    - \alpha (\log k - \mu)
    + k \exp \left( - \mu + \dfrac{\sigma^2}{2} \right)
  \\
  KL \left( LN(\mu, \sigma^2) ~ || ~ InvGamma(\alpha, k) \right) 
  & = 
    KL \left( LN(- \mu, \sigma^2) ~ || ~ Gamma(\alpha, \dfrac{1}{k}) \right)
\end{align}$$






## Inference

Regression models using the Horseshoe prior typically use the shrinkage weight for the $j$'th covariate $1 - \kappa_{j}$ (where $\kappa_{j} = \left( 1 + \lambda_{j} \tau \right)^{-1}$) as a proxy for the posterior inclusion probability for covariate $j$ (Carvalho, Scott, Polson 2010).  The more complex structure in the deep neural network setting requires an easy but impactful correction.  

We expect the starting network structure to be overparameterized, including unrelated input covariates as well as unnecessary network connections, both of which motivate enriching the network with the grouped horseshoe construction.  

In particular, shrinkage of rows in $W_l$ are equivalent to "shrinkage" of columns in the input to layer $l$ in networks with RELU activations.  Let $Z_l$ be a diagonal $d_{l-1} \times d_{l-1}$ matrix with the scale parameters $\lambda_{l,j} \tau_l$ on the diagonals, and let $\sigma_l$ denote the activation function for layer $l$.  Then we can express the network as follows:

$$\begin{align}
  \hat{y}_i
    & = f(X_i, W)
  \\
    & =  
    \sigma_{L-1}\left\{ ... \left\{ 
      \sigma_2 \left[
        \sigma_1 \left(
          X_i Z_1 \tilde W_1 + b_1
        \right)
        Z_2 \tilde W_2 + b_2
      \right]
      Z_3 \tilde W_3 + b_3 
    \right\} ... \right\} Z_L \tilde W_L + b_L
\end{align}$$

Since the activation functions are RELU, we can move the non-negative diagonal matrix $Z_l$ inside the RELU activation for layer $l-1$, $\sigma_{l-1}$.  Thus we can express the neural network as

$$\begin{align}
  \hat{y}_i
    & =  
    \sigma_{L-1}\left\{ ... \left\{ 
      \sigma_2 \left[
        \sigma_1 \left(
          X_i Z_1 \tilde W_1 + b_1
        \right)
        Z_2 \tilde W_2 + b_2
      \right]
      Z_3 \tilde W_3 + b_3 
    \right\} ... \right\} Z_L \tilde W_L + b_L
  \\
    & =  
    \sigma_{L-1}\left\{ ... \left\{ 
      \sigma_2 \left[
        \sigma_1 \left(
          X_i Z_1 \tilde W_1 Z_2 + b_1 Z_2
        \right)
        \tilde W_2 Z_3 + b_2 Z_3
      \right]
      \tilde W_3 Z_4 + b_3 Z_4 
    \right\} ... \right\} \tilde W_L + b_L
  \\
\end{align}$$


Thus, whereas the shrinkage implied by $Z_l$ acts on groupings of weights related to inputs in layer $l$ (i.e. rows in $W_l$), the shrinkage implied by layer $l+1$ can be seen to act on _the neurons_ in layer $l$ (i.e. columns in $W_l$).  While this has the major benefit of inducing a parsimonious network structure, shrinkage from the 2nd layer effectively pollutes the network estimate for $\tau_1$, the global scale parameter.  

The primary issue is that $\tau_l$, the global scale parameter for layer $l$, is pushed further towards zero because of the presence of neurons which are implied by the next layer's local scale parameters to be unnecessary.  Thus, to calculate the proxy posterior probabilities $1 - \kappa_j$ for the input layer, we borrow Piironen & Vehtari 2017's concept of the number of effective coefficients $m^{eff} = \sum_{j = 1}^{p} \left(1 - \kappa_{j} \right)$.  Extending this to the deep neural network case, we say that the effective number of inputs to layer $l$ is determined by $$m_l^{eff} = \sum_{j = 1}^{d_{l-1}} \left(1 - \kappa_{j} \right)$$.  Thus we propose using a corrected version of $\tau_1$:

$$\tilde \tau_1 = \tau_1 \times \sqrt{  \dfrac{d_1}{m_2^{eff}}    }$$
.


We then calculate the input layer $\kappa$'s as $\kappa_j = (1 + \lambda_j^2 \tilde\tau_1^2)^{-1}$ (using the variational posterior modes for $\lambda$ and $\tau$).  Finally, we use the resulting  $1 - \kappa_j$ for the posterior inclusion probabilities (or $\kappa_j$ for exclusion probabilities) when calculating the Bayesian False Discovery Rate (BFDR):

$$BFDR(\eta) = 
  \dfrac{  
    \sum_j \kappa_j \mathbb{I}(\kappa_j < \eta)
  }{  
    \sum_j \mathbb{I}(\kappa_j < \eta)
  }$$,

i.e. the sum of the posterior probabilities that an included variable (inclusion into the model being determined by the exclusion probability $\kappa_j$ being less than the threshold $\eta$) should actually have been excluded, divided by the total number of included variables.

We find in simulation that the correction to the global scale parameter $\tilde \tau_1 = \tau_1 \times \sqrt{  \dfrac{d_1}{m_2^{eff}}    }$ results in Bayesian FDR calculations that track the frequentist FDR much more closely, even in very sparse settings, than when using an uncorrected $\tau$.



## notes on prior work

  - prior ML-focused work here stems from Louizos, Ullrich, Welling 2017 whose focus is on compression of neural networks, i.e. reducing network layer size, by pruning weight matrix rows when $(\sigma^2_{\lambda_j} + \sigma^2_\tau) - (\mu_{\lambda_j} + \mu_\tau) \geq c$, with $c$ chosen to tune the desired size of the network.  While they show large reductions in storage requirements with little cost to predictive performance, there is little to no exploration of its use for variable selection.  Similarly, most ML-focused work on variable selection in deep neural networks tends to be focused on predictive performance, with little discussion or exploration of inferential operating characteristics.  

    - there is some work on mirror statistics and using credible intervals
    
    - there are a few papers that inspect their methods' variable selection properties in extremely simplified test cases (i.e. 1 neuron, no hidden layers), i.e. linear regression via gradient descent.

  - prior statistical work on variable selection in Bayesian neural networks:
  
    - Ghosh, Yao, Doshi-Velez 2019: "Model selection" here is primarily about choice of NN architecture, i.e. how many layers, how many nodes

    - Polson and Rockova 2018 (provides theory for posterior consistency of Bayesian Deep NN's with spike-and-slab priors and RELU activation)





Correction to global scale parameter $\tau$

- Also should note: Piironen & Vehtari 2017 indicate the need for calibration of $\tau_0$ 










## Contributions:

- Few methods exist for controlling false positive rates for variable selection in neural networks;  

- To our knowledge, no previous work has shown control over the Bayesian FDR in Bayesian neural networks;  (need to look at [Molinari, Thoresen 2025 on mirror statistics) more closely](https://arxiv.org/abs/2510.00875)




