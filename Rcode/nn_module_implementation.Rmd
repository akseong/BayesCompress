---
title: nn_module implementation of "Bayesian Compression" (LUW 2017)
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}
```


# Setup

## generate data.  

```{r GENERATE_DATA}
library(torch)
n <- 100
# input dimensionality (# of features)
d_in <- 5
d_true <- 2   # the rest are nuisance vars
# output dimensionality (response dim)
datagen_out <- 3

x <- torch_randn(n, d_in)
w1_true <- torch_randn(size = c(d_in, datagen_out))
w1_true[(d_true+1):d_in, ] <- 0
b1_true <- torch_randint(-3, 5, size = c(1, datagen_out))$'repeat'(c(n, 1))
epsilon <- torch_randn(size = c(n, datagen_out))
# plot(density(as_array(epsilon)))
# qqnorm(y = as_array(epsilon))

# multivariate response
y_vector <- x$mm(w1_true) + b1_true + epsilon

# scalar response
y_scalar <- y_vector$sum(dim = 2) + epsilon[ , 1]
y <- y_scalar$unsqueeze(2)
```


- `r n` obs generated as basic linear regression $y = X\beta + \epsilon$, with $\epsilon_i \sim N(0,1)$
- Only first `r d_true` covariates in $X$ (out of `r d_in`) actually have an effect (rest are nuisance var.s).  
- multivariate response also generated (but not used for now)


```{r LMFIT}
df <- data.frame(
  "y" = as_array(y), 
  "X" = as_array(x)
)

lmfit <- lm(y ~ ., df)
summary(lmfit)
b1_true[1, ]$sum()
tru <- as_array(abs(w1_true$sum(dim = 2)) > 0)


binary_err_mat <- function(est, tru){
  # returns 4-row matrix of FP, TP, FN, TN
  FP <- est - tru == 1
  TP <- tru + est == 2
  FN <- tru - est == 1
  TN <- abs(tru) + abs(est) == 0   
  return(rbind(FP, TP, FN, TN))
}

binary_err_rate <- function(est, tru){
  # returns FP, TP, FN, TN rates
  rowSums(binary_err_mat(est, tru)) / length(tru)  
}

lm_binary_err_rate <- binary_err_rate(est = summary(lmfit)$coef[-1, 4]< 0.05, tru)
```





```{r}

reparameterize <- function(mu, logvar, use_cuda = FALSE, sampling = TRUE) {
  if (sampling) {
    std <- logvar$mul(0.5)$exp_()
    if (use_cuda) {
      eps <- torch_randn(std$size(), device = "gpu", requires_grad = TRUE)
    } else {
      eps <- torch_randn(std$size(), device = "cpu", requires_grad = TRUE)
    }
    return(mu$add(eps$mul(std)))
  } else {
    return(mu)
  }
}

BayesianLayerNJ <- nn_module(
  
  classname = "BayesianLayerNJ",
  
  initialize = function(
    in_features, out_features,
    use_cuda = FALSE,
    init_weight = NULL,
    init_bias = NULL,
    clip_var = NULL
    ) {
    
    
    # #### code for testing only----
    # in_features <- 5
    # out_features <- 3
    # use_cuda = FALSE
    # init_weight = NULL
    # init_bias = NULL
    # clip_var = NULL
    # self <- nn_module()
    # ### end testing code
    
    
    self$use_cuda <- use_cuda
    self$in_features <- in_features
    self$out_features <- out_features
    self$clip_var <- clip_var
    self$deterministic <- FALSE
    
    # trainable parameters
    self$z_mu <- nn_parameter(torch_randn(in_features))
    self$z_logvar <- nn_parameter(torch_randn(in_features))
    self$weight_mu <- nn_parameter(torch_randn(out_features, in_features))
    self$weight_logvar <- nn_parameter(torch_randn(out_features, in_features))
    self$bias_mu <- nn_parameter(torch_randn(out_features))
    self$bias_logvar <- nn_parameter(torch_randn(out_features))
    
    # initialize parameters randomly or with pretrained net
    self$reset_parameters(init_weight, init_bias)
    
    # # Activations for KL
    # self$sigmoid <- nnf_sigmoid                # MAY NEED TO USE nn_sigmoid() 
    # self$softplus <- nnf_softplus
    
    # numerical stability param
    self$epsilon <- 1e-8
    
  },
  
  reset_parameters = function(init_weight, init_bias){
    
    # feel like there are issues with using nn_parameter here again 
    # to define each of these as parameters again.  
    # not sure how to modify in-place without losing `is_nn_parameter() = TRUE`
    
    
    # initialize means
    stdv <- 1 / sqrt(self$weight_mu$size(1)) # self$weight_mu$size(1) = out_features
    self$z_mu <- nn_parameter(torch_normal(1, 1e-2, size = self$z_mu$size()))      # potential issue (if not considered leaf node anymore?)  wrap in nn_parameter()?
    if (!is.null(init_weight)) {
      self$weight_mu <- nn_parameter(torch_tensor(init_weight))
    } else {
      self$weight_mu <- nn_parameter(torch_normal(0, stdv, size = self$weight_mu$size()))
    }
    
    if (!is.null(init_bias)) {
      self$bias_mu <- nn_parameter(torch_tensor(init_bias))
    } else {
      self$bias_mu <- nn_parameter(torch_zeros(self$out_features))
    }
    
    # initialize log variances
    self$z_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = self$in_features))
    self$weight_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = c(self$out_features, self$in_features)))
    self$bias_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = self$out_features))
  },
  
  
  clip_variances = function() {
    if (!is.null(self$clip_var)) {
      self$weight_logvar <- nn_parameter(self$weight_logvar$clamp(max = log(self$clip_var)))
      self$bias_logvar <- nn_parameter(self$bias_logvar$clamp(max = log(self$clip_var)))
    }
  },
  
  
  get_log_dropout_rates = function() {
    log_alpha = self$z_logvar - torch_log(self$z_mu$pow(2) + self$epsilon)
    return(log_alpha)
  },
  
  
  compute_posterior_param = function() {
    weight_var <- self$weight_logvar$exp()
    z_var <- self$z_logvar$exp()
    self$post_weight_var <- self$z_mu$pow(2) * weight_var + z_var * self$weight_mu$pow(2) + z_var * weight_var
    self$post_weight_mu <- self$weight_mu * self$z_mu
    return(list(
      "post_weight_mu" = self$post_weight_mu,
      "post_weight_var" = self$post_weight_var
    ))
  },
  
  
  forward = function(x){
    if (self$deterministic) {
      cat("argument deterministic = TRUE.  Should not be used for training")
      return(
        nnf_linear(
          input = x, 
          weight = self$post_weight_mu, 
          bias = self$bias_mu
        )
      )
    }
    batch_size <- x$size(1)
    z <- reparameterize(
      mu = self$z_mu$'repeat'(c(batch_size, 1)), 
      logvar = self$z_logvar$'repeat'(c(batch_size, 1)),
      sampling = !self$deterministic,
      use_cuda = self$use_cuda
    )
    xz <- x*z
    mu_activations <- nnf_linear(
      input = xz, 
      weight = self$weight_mu, 
      bias = self$bias_mu
    )
    var_activations <- nnf_linear(
      input = xz$pow(2), 
      weight = self$weight_logvar$exp(), 
      bias = self$bias_logvar$exp()
    )
    
    return(
      reparameterize(
        mu = mu_activations, 
        logvar = var_activations$log(), 
        use_cuda = self$use_cuda, 
        sampling = !self$deterministic
      )
    )
  },
  
  
  get_kl = function() {
    k1 = 0.63576
    k2 = 1.87320
    k3 = 1.48695
    log_alpha = self$get_log_dropout_rates()
    
    # KL(q(z) || p(z))
    kl_z <- -torch_sum(
      k1 * nnf_sigmoid(k2 + k3*log_alpha) - 0.5 * nnf_softplus(-log_alpha) - k1
    )
    
    # KL(q(w|z) || p(w|z))
    kl_w_z <- torch_sum(
      -0.5 * self$weight_logvar + 0.5 * (self$weight_logvar$exp() + self$weight_mu$pow(2)) - 0.5
    )
    
    # KL for bias term
    kl_bias <- torch_sum(
      -0.5 * self$bias_logvar + 0.5 * (self$bias_logvar$exp() + self$bias_mu$pow(2)) - 0.5
    )
    
    # sum
    kl <- kl_z + kl_w_z + kl_bias
    return(kl)
  }
)



```






```{r}
d_hidden <- 10
d_out <- 1


net <- nn_module(
  
  "NJ_testing",
  
  initialize = function() {
    # in_channels, out_channels, kernel_size, stride = 1, padding = 0
    
    self$fc1 = BayesianLayerNJ(    
      in_features = d_in, 
      out_features = d_hidden,
      use_cuda = FALSE,
      init_weight = NULL,
      init_bias = NULL,
      clip_var = NULL
    )
    
    self$fc2 = BayesianLayerNJ(
      in_features = d_hidden,
      out_features = d_out,
      use_cuda = FALSE,
      init_weight = NULL,
      init_bias = NULL,
      clip_var = NULL
    )
    
  },
  
  forward = function(x) {
    x %>% 
      self$fc1() %>%
      nnf_relu() %>%
      self$fc2()
  },
  
  
  get_masks = function(thresholds){
    # implement later?
  },
  
  
  get_model_kld = function(){
    kl1 = self$fc1$get_kl()
    kl2 = self$fc2$get_kl()
    kld = kl1 + kl2
    return(kld)
  }
  
)

model = net()


y_pred <- model(x)
model$parameters


optimizer <- optim_adam(model$parameters)

mse <- nnf_mse_loss(y_pred, y)
kl <- model$get_model_kld() / n

loss <- mse + kl

```

































