---
title: nn_module implementation of "Bayesian Compression" (LUW 2017)
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}
```


# Setup

## generate data.  

```{r GENERATE_DATA}
library(torch)
n <- 100
# input dimensionality (# of features)
d_in <- 5
d_true <- 2   # the rest are nuisance vars

x <- torch_randn(n, d_in)
coefs <- c(0, 0, -0.5, 2, -5)
intercept <- 0

# scalar response
y <- x$matmul(coefs)$unsqueeze(2) + intercept + torch_normal(mean = 0, std = sig, size = c(n, 1))

```


- `r n` obs generated as basic linear regression $y = X\beta + \epsilon$, with $\epsilon_i \sim N(0,1)$
- Only first `r d_true` covariates in $X$ (out of `r d_in`) actually have an effect (rest are nuisance var.s).  
- multivariate response also generated (but not used for now)


```{r LMFIT}
df <- data.frame(
  "y" = as_array(y), 
  "X" = as_array(x)
)

lmfit <- lm(y ~ ., df)
summary(lmfit)

binary_err_mat <- function(est, tru){
  # returns 4-row matrix of FP, TP, FN, TN
  FP <- est - tru == 1
  TP <- tru + est == 2
  FN <- tru - est == 1
  TN <- abs(tru) + abs(est) == 0   
  return(rbind(FP, TP, FN, TN))
}

binary_err_rate <- function(est, tru){
  # returns FP, TP, FN, TN rates
  rowSums(binary_err_mat(est, tru)) / length(tru)  
}

lm_binary_err_rate <- binary_err_rate(est = summary(lmfit)$coef[-1, 4]< 0.05, coefs != 0)
```





```{r}

reparameterize <- function(mu, logvar, use_cuda = FALSE, sampling = TRUE) {
  if (sampling) {
    std <- logvar$mul(0.5)$exp_()
    if (use_cuda) {
      eps <- torch_randn(std$size(), device = "gpu", requires_grad = TRUE)
    } else {
      eps <- torch_randn(std$size(), device = "cpu", requires_grad = TRUE)
    }
    return(mu$add(eps$mul(std)))
  } else {
    return(mu)
  }
}

BayesianLayerNJ <- nn_module(
  
  classname = "BayesianLayerNJ",
  
  initialize = function(
    in_features, out_features,
    use_cuda = FALSE,
    init_weight = NULL,
    init_bias = NULL,
    clip_var = NULL
    ) {
    
    
    # #### code for testing only----
    # in_features <- 5
    # out_features <- 3
    # use_cuda = FALSE
    # init_weight = NULL
    # init_bias = NULL
    # clip_var = NULL
    # self <- nn_module()
    # ### end testing code
    
    
    self$use_cuda <- use_cuda
    self$in_features <- in_features
    self$out_features <- out_features
    self$clip_var <- clip_var
    self$deterministic <- FALSE
    
    # trainable parameters
    self$z_mu <- nn_parameter(torch_randn(in_features))
    self$z_logvar <- nn_parameter(torch_randn(in_features))
    self$weight_mu <- nn_parameter(torch_randn(out_features, in_features))
    self$weight_logvar <- nn_parameter(torch_randn(out_features, in_features))
    self$bias_mu <- nn_parameter(torch_randn(out_features))
    self$bias_logvar <- nn_parameter(torch_randn(out_features))
    
    # initialize parameters randomly or with pretrained net
    self$reset_parameters(init_weight, init_bias)
    
    # # Activations for KL
    # self$sigmoid <- nnf_sigmoid                # MAY NEED TO USE nn_sigmoid() 
    # self$softplus <- nnf_softplus
    
    # numerical stability param
    self$epsilon <- 1e-8
    
  },
  
  reset_parameters = function(init_weight, init_bias){
    
    # feel like there are issues with using nn_parameter here again 
    # to define each of these as parameters again.  
    # not sure how to modify in-place without losing `is_nn_parameter() = TRUE`
    
    
    # initialize means
    stdv <- 1 / sqrt(self$weight_mu$size(1)) # self$weight_mu$size(1) = out_features
    self$z_mu <- nn_parameter(torch_normal(1, 1e-2, size = self$z_mu$size()))      # potential issue (if not considered leaf node anymore?)  wrap in nn_parameter()?
    if (!is.null(init_weight)) {
      self$weight_mu <- nn_parameter(torch_tensor(init_weight))
    } else {
      self$weight_mu <- nn_parameter(torch_normal(0, stdv, size = self$weight_mu$size()))
    }
    
    if (!is.null(init_bias)) {
      self$bias_mu <- nn_parameter(torch_tensor(init_bias))
    } else {
      self$bias_mu <- nn_parameter(torch_zeros(self$out_features))
    }
    
    # initialize log variances
    self$z_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = self$in_features))
    self$weight_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = c(self$out_features, self$in_features)))
    self$bias_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = self$out_features))
  },
  
  
  clip_variances = function() {
    if (!is.null(self$clip_var)) {
      self$weight_logvar <- nn_parameter(self$weight_logvar$clamp(max = log(self$clip_var)))
      self$bias_logvar <- nn_parameter(self$bias_logvar$clamp(max = log(self$clip_var)))
    }
  },
  
  
  get_log_dropout_rates = function() {
    log_alpha = self$z_logvar - torch_log(self$z_mu$pow(2) + self$epsilon)
    return(log_alpha)
  },
  
  
  compute_posterior_param = function() {
    weight_var <- self$weight_logvar$exp()
    z_var <- self$z_logvar$exp()
    self$post_weight_var <- self$z_mu$pow(2) * weight_var + z_var * self$weight_mu$pow(2) + z_var * weight_var
    self$post_weight_mu <- self$weight_mu * self$z_mu
    return(list(
      "post_weight_mu" = self$post_weight_mu,
      "post_weight_var" = self$post_weight_var
    ))
  },
  
  
  forward = function(x){
    if (self$deterministic) {
      cat("argument deterministic = TRUE.  Should not be used for training")
      return(
        nnf_linear(
          input = x, 
          weight = self$post_weight_mu, 
          bias = self$bias_mu
        )
      )
    }
    batch_size <- x$size(1)
    z <- reparameterize(
      mu = self$z_mu$'repeat'(c(batch_size, 1)), 
      logvar = self$z_logvar$'repeat'(c(batch_size, 1)),
      sampling = !self$deterministic,
      use_cuda = self$use_cuda
    )
    xz <- x*z
    mu_activations <- nnf_linear(
      input = xz, 
      weight = self$weight_mu, 
      bias = self$bias_mu
    )
    var_activations <- nnf_linear(
      input = xz$pow(2), 
      weight = self$weight_logvar$exp(), 
      bias = self$bias_logvar$exp()
    )
    
    return(
      reparameterize(
        mu = mu_activations, 
        logvar = var_activations$log(), 
        use_cuda = self$use_cuda, 
        sampling = !self$deterministic
      )
    )
  },
  
  
  get_kl = function() {
    k1 = 0.63576
    k2 = 1.87320
    k3 = 1.48695
    log_alpha = self$get_log_dropout_rates()
    
    # KL(q(z) || p(z))
    kl_z <- -torch_sum(
      k1 * nnf_sigmoid(k2 + k3*log_alpha) - 0.5 * nnf_softplus(-log_alpha) - k1
    )
    
    # KL(q(w|z) || p(w|z))
    kl_w_z <- torch_sum(
      -0.5 * self$weight_logvar + 0.5 * (self$weight_logvar$exp() + self$weight_mu$pow(2)) - 0.5
    )
    
    # KL for bias term
    kl_bias <- torch_sum(
      -0.5 * self$bias_logvar + 0.5 * (self$bias_logvar$exp() + self$bias_mu$pow(2)) - 0.5
    )
    
    # sum
    kl <- kl_z + kl_w_z + kl_bias
    return(kl)
  }
)



```






```{r}
d_hidden1 <- 8
d_hidden2 <- 4
d_out <- 1


net <- nn_module(
  
  "NJ_testing",
  
  initialize = function() {
    # in_channels, out_channels, kernel_size, stride = 1, padding = 0
    
    self$fc1 = BayesianLayerNJ(    
      in_features = d_in, 
      out_features = d_hidden1,
      use_cuda = FALSE,
      init_weight = NULL,
      init_bias = NULL,
      clip_var = NULL
    )
    
    self$fc2 = BayesianLayerNJ(
      in_features = d_hidden1,
      out_features = d_hidden2,
      use_cuda = FALSE,
      init_weight = NULL,
      init_bias = NULL,
      clip_var = NULL
    )
    
    self$fc3 = BayesianLayerNJ(
      in_features = d_hidden2,
      out_features = d_out,
      use_cuda = FALSE,
      init_weight = NULL,
      init_bias = NULL,
      clip_var = NULL
    )    
  },
  
  forward = function(x) {
    x %>% 
      self$fc1() %>%
      nnf_relu() %>%
      self$fc2() %>% 
      nnf_relu() %>% 
      self$fc3()
  },
  
  
  get_masks = function(thresholds){
    # implement later?
  },
  
  
  get_model_kld = function(){
    kl1 = self$fc1$get_kl()
    kl2 = self$fc2$get_kl()
    kl3 = self$fc3$get_kl()
    kld = kl1 + kl2 + kl3
    return(kld)
  }
  
)

model = net()
```



## Training with ADAM



```{r TRAIN}
optim <- optim_adam(model$parameters)


train_epochs <- 2000
for (epoch in 1:train_epochs){
  
  
  y_pred <- model(x)
  
  mse <- nnf_mse_loss(y_pred, y)
  kl <- model$get_model_kld() / n
  
  loss <- mse + kl
  
  if(epoch %% 10 == 0){
    cat(
      "Epoch: ", epoch, 
      "MSE + KL/n = ", mse$item(), " + ", kl$item(), 
      " = ", loss$item(), 
      "\n"
    )
    cat(model$fc1$weight_logvar$exp()$mean()$item(), "\n")
  }
  
  # zero out previous gradients
  optim$zero_grad()
  # backprop
  loss$backward()
  # update weights
  optim$step()

}
```


# troubleshooting

## `lm()` results
```{r}
summary(lmfit)
beta_true
```


## comparison against basic MLP

```{r}

mlp <- nn_sequential(
  nn_linear(d_in, d_hidden1),
  nn_relu(),
  nn_linear(d_hidden1, d_hidden2),
  nn_relu(),
  nn_linear(d_hidden2, d_out)
)

optim_mlp <- optim_adam(mlp$parameters)


for (epoch in 1:train_epochs){
  
  
  y_pred <- mlp(x)
  
  loss_mlp <- nnf_mse_loss(y_pred, y)
  
  
  if(epoch %% 10 == 0){
    cat(
      "Epoch: ", epoch, 
      "MSE = ", loss_mlp$item(), 
      "\n"
    )
  }
  
  # zero out previous gradients
  optim_mlp$zero_grad()
  # backprop
  loss_mlp$backward()
  # update weights
  optim_mlp$step()

}


```





# single layer performance

```{r}
SLNJ <- nn_module(
  
  "SLNJ",
  
  initialize = function() {
    # in_channels, out_channels, kernel_size, stride = 1, padding = 0
    
    self$fc1 = BayesianLayerNJ(    
      in_features = d_in, 
      out_features = 1,
      use_cuda = FALSE,
      init_weight = NULL,
      init_bias = NULL,
      clip_var = NULL
    )

  },
  
  forward = function(x) {
    x %>% 
      self$fc1() 
  },
  
  
  get_masks = function(thresholds){
    # implement later?
  },
  
  
  get_model_kld = function(){
    kl1 = self$fc1$get_kl()
    kld = kl1
    return(kld)
  }
  
)

slnj_net <- SLNJ()
slnj_net(x)

```


```{r}
optim_slnj <- optim_adam(slnj_net$parameters)


train_epochs <- 2000

for (epoch in 1:train_epochs){
  
  
  y_pred <- slnj_net(x)
  
  mse <- nnf_mse_loss(y_pred, y)
  kl <- slnj_net$get_model_kld() / n
  
  loss <- mse + kl
  
  if(epoch %% 10 == 0){
    cat(
      "Epoch: ", epoch, 
      "MSE + KL/n = ", mse$item(), " + ", kl$item(), 
      " = ", loss$item(), 
      "\n"
    )
    cat(slnj_net$fc1$weight_logvar$exp()$mean()$item(), "\n")
    cat(round(as_array(slnj_net$fc1$z_mu), 4), "\n")
  }
  
  # zero out previous gradients
  optim_slnj$zero_grad()
  # backprop
  loss$backward()
  # update weights
  optim_slnj$step()

}
```



# log_alpha

**log_alpha is calculated as:**
`log_alpha = self.z_logvar - torch_log(self.z_mu.pow(2) + self.epsilon)`
(`self.epsilon` is a stability parameter = 1e-8, I'm assuming so that if `z_mu` = 0 we don't get $-\infty$)

Or, essentially: 

$\log \alpha = \log(\sigma^2_z) - \log(\mu_z^2)$, so that as $\mu_z \rightarrow 0$, then $\alpha$ is large.



**for single-layer NJ**:
It looks like the parameter `z_mu` for the layers is not the actual mixing parameter.  Instead, it looks like the real mixing parameter is what LUW 2017 call the "dropout rate", i.e. the `alpha` parameters (e.g. `slnj_net$fc1$get_log_dropout_rates()$exp()`). 


### Q: Is this ($\alpha$ being the real mixing parameter instead of $\mu_z$) a result of all the reparameterizations?


```{r}
as_array(slnj_net$fc1$get_log_dropout_rates()$exp())
```


**for heavily over-parameterized network**
```{r}
as_array(model$fc1$get_log_dropout_rates()$exp())
as_array(model$fc2$get_log_dropout_rates()$exp())

```

not sure how to interpret this, unfortunately.  Weird stuff in `utils.py`:

In `visualise_weights`, Kullrich seems to be adding the log alphas of the current layer + the layer after it (i.e. for layer $i$, `log_alphas` from layers $i$ and $i+1$) to get the importance weights mask.  

```{r}

la1 <- model$fc1$get_log_dropout_rates()
la2 <- model$fc2$get_log_dropout_rates()
la1
la1$unsqueeze(2)
la2
la2$unsqueeze(1)

mask <- la1$unsqueeze(2) + la2$unsqueeze(1)
mask  # ah! i see.  The vectors are broadcasted so we get a matrix.

plotted_values <- model$fc1$weight_mu$t() * mask
abs_range <- as_array(max(abs(model$fc1$weight_mu)))

in_plot <- abs(plotted_values) < abs_range
in_plot
plotted_values * in_plot

```

Interesting.  So only these weights "matter"?

or is it the opposite?  what happens if a value to be plotted is outside of `vmin`/`vmax` in `matplotlib`?
```{r}
plotted_values$clip(-abs_range, abs_range)
```


## pixel_importance
`visualise_pixel_importance` also interesting:
(in `example.py` it's given just the data (`x`) and the first layer's `log_alpha` as input)
This should be the analogue to variable importance (?), for each observation.

```{r}

mask = 1 - la1$exp()
mask <- mask$clip(0,1)
# kullrich scales the data values to -0.5 to 0.5,
x_transformed <- x/max(x) - 0.5
# then makes the plot of the mask * data only show values from -0.5 to 0.5
shown <- (x_transformed*mask)$clip(-0.5, .5)
shown


```









Also need to look at the *posterior parameters*.
```{r}
as_array(slnj_net$fc1$compute_posterior_param()$post_weight_mu)
coefs

```


```{r}

as_array(model$fc1$compute_posterior_param()$post_weight_mu)
```





#  need to add:
- clip variances after each epoch

