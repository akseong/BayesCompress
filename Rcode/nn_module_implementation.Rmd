---
title: nn_module implementation of "Bayesian Compression" (LUW 2017)
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}
```


# Setup

## generate data.  

```{r GENERATE_DATA}
library(torch)
n <- 100
# input dimensionality (# of features)
d_in <- 5
d_true <- 2   # the rest are nuisance vars
# output dimensionality (response dim)
datagen_out <- 3

x <- torch_randn(n, d_in)
w1_true <- torch_randn(size = c(d_in, datagen_out))
w1_true[(d_true+1):d_in, ] <- 0
b1_true <- torch_randint(-3, 5, size = c(1, datagen_out))$'repeat'(c(n, 1))
epsilon <- torch_randn(size = c(n, datagen_out))
# plot(density(as_array(epsilon)))
# qqnorm(y = as_array(epsilon))

# multivariate response
y_vector <- x$mm(w1_true) + b1_true + epsilon

# scalar response
y_scalar <- y_vector$sum(dim = 2) + epsilon[ , 1]
y <- y_scalar$unsqueeze(2)
```


- `r n` obs generated as basic linear regression $y = X\beta + \epsilon$, with $\epsilon_i \sim N(0,1)$
- Only first `r d_true` covariates in $X$ (out of `r d_in`) actually have an effect (rest are nuisance var.s).  
- multivariate response also generated (but not used for now)


```{r LMFIT}
df <- data.frame(
  "y" = as_array(y), 
  "X" = as_array(x)
)

lmfit <- lm(y ~ ., df)
summary(lmfit)
b1_true[1, ]$sum()
tru <- as_array(abs(w1_true$sum(dim = 2)) > 0)


binary_err_mat <- function(est, tru){
  # returns 4-row matrix of FP, TP, FN, TN
  FP <- est - tru == 1
  TP <- tru + est == 2
  FN <- tru - est == 1
  TN <- abs(tru) + abs(est) == 0   
  return(rbind(FP, TP, FN, TN))
}

binary_err_rate <- function(est, tru){
  # returns FP, TP, FN, TN rates
  rowSums(binary_err_mat(est, tru)) / length(tru)  
}

lm_binary_err_rate <- binary_err_rate(est = summary(lmfit)$coef[-1, 4]< 0.05, tru)
```





```{r}

reparameterize <- function(mu, logvar, cuda = FALSE, sampling = TRUE) {
  if (sampling) {
    std <- logvar$mul(0.5)$exp_()
    if (cuda) {
      eps <- torch_randn(std$size(), device = "gpu", requires_grad = TRUE)
    } else {
      eps <- torch_randn(std$size(), device = "cpu", requires_grad = TRUE)
    }
    return(mu$add(eps$mul(std)))
  } else {
    return(mu)
  }
}

BayesianLayerNJ <- nn_module(
  
  classname = "BayesianLayerNJ",
  
  initialize = function(
    in_features, out_features,
    cuda = FALSE,
    init_weight = NULL,
    init_bias = NULL,
    clip_var = NULL
    ) {
    
    
    #### code for testing only----
    in_features <- 5
    out_features <- 3
    cuda = FALSE
    init_weight = NULL
    init_bias = NULL
    clip_var = NULL
    self <- list()
    ### end testing code
    
    
    self$cuda <- cuda
    self$in_features <- in_features
    self$out_features <- out_features
    self$clip_var <- clip_var
    self$deterministic <- FALSE
    
    # trainable parameters
    self$z_mu <- nn_parameter(torch_randn(in_features))
    self$z_logvar <- nn_parameter(torch_randn(in_features))
    self$weight_mu <- nn_parameter(torch_randn(out_features, in_features))
    self$weight_logvar <- nn_parameter(torch_randn(out_features, in_features))
    self$bias_mu <- nn_parameter(torch_randn(out_features))
    self$bias_logvar <- nn_parameter(torch_randn(out_features))
    
    # initialize parameters randomly or with pretrained net
    self$reset_parameters(init_weight, init_bias)
    
    # Activations for KL
    self$sigmoid <- nnf_sigmoid                # MAY NEED TO USE nn_sigmoid() 
    self$softplus <- nnf_softplus
    
    # numerical stability param
    self$epsilon <- 1e-8
    
  },
  
  reset_parameters = function(init_weight, init_bias){
    
    # feel like there are issues with using nn_parameter here again 
    # to define each of these as parameters again.  
    # not sure how to modify in-place without losing `is_nn_parameter() = TRUE`
    
    
    # initialize means
    stdv <- 1 / sqrt(self$weight_mu$size(1)) # self$weight_mu$size(1) = out_features
    self$z_mu <- nn_parameter(torch_normal(1, 1e-2, size = self$z_mu$size()))      # potential issue (if not considered leaf node anymore?)
    if (!is.null(init_weight)) {
      self$weight_mu <- nn_parameter(torch_tensor(init_weight))
    } else {
      self$weight_mu <- nn_parameter(torch_normal(0, stdv, size = self$weight_mu$size()))
    }
    
    if (!is.null(init_bias)) {
      self$bias_mu <- nn_parameter(torch_tensor(init_bias))
    } else {
      self$bias_mu <- nn_parameter(torch_zeros(self$out_features))
    }
    
    # initialize log variances
    self$z_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = self$in_features))
    self$weight_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = c(self$out_features, self$in_features)))
    self$bias_logvar <- nn_parameter(torch_normal(-9, 1e-2, size = self$out_features))
  },
  
  clip_variances = function() {
    if (!is.null(self$clip_var)) {
      self$weight_logvar <- nn_parameter(self$weight_logvar$clamp(max = log(self$clip_var)))
    }
    
  }
  
  
  
  
  
  forward = function(x){
    
    
  }
  
  
  
  
  
  
)







```







































