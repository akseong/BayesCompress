---
title: "First simulations"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)


# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}



cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "BayesianLayers.R"))
source(here("Rcode", "sim_functions.R"))
```




# Background
The motivation for this model is to combine the variable selection properties of Bayesian spike-and-slab with the expressivity of a neural network.  We do this by placing a spike-and-slab prior on the _rows_ of a weight matrix $W$ representing one layer of a neural network.

Let $X$ be a $n \times I$ matrix representing $n$ observations of $I$ covariates, and $W$ be a $I \times J$ weight matrix.  We can then represent the computation performed by the first (input) layer of a neural network as $XW + b = H$, where $b$ is a vector of biases and $H$ is the pre-activation (that is, if $\sigma ()$ denotes the activation function, $\sigma(H) = \tilde{H}$ is the input for the next layer in the neural network).

If all of the weights $w_{ij}$ in row $i$ of $W$ are 0, then the covariate in column $i$ of $X$ do not contribute to $H$ and therefore also have no effect on the final prediction.  Thus by placing a spike-and-slab prior on the rows of $W$, we obtain variable selection.

We rely heavily here on the formulation and variational Bayes algorithm developed by _Louizos, Ullrich, Welling 2017_, who proposed using a group sparsity prior in order to reduce the size of the weight matrices in a neural network.  Their primary focus is on decreasing the size of the neural network while preserving quality of prediction --- the method leads to compression of the neural network since if an entire row or column of weights is 0, then the dimension of the weight matrix can be reduced.  Accordingly, their discussion of how to tune what they term "the dropout parameter" ($\alpha_i$), is based around desired compression rates rather than how to use it for principled variable selection.


## notes:

- Need to experiment to see if it's better to only do this for the input layer (as opposed to all layers, which is the current implementation)

- Ways to differentiate ourselves:  
    - a more rigorous interpretation of the dropout parameter $\alpha_i$ as the equivalent to a Wald statistic  
    - (Adam's paper) using normalizing flow to model non-local prior
    - some way to reconcile mean-field assumption of Variational Bayes with group sparsity?  
    - as _LUW 2017_ note, the log-uniform prior does require independence between parameters (i.e. grouping by row is not kosher), so the joint Bayesian model developed in LUW 2017 is not strictly accurate and is more of an approximation
    - obvious difference: provide empirical results on how good the method is at variable selection



# Model

We adopt a scale mixture of Normals as the prior for the weights:

$$\begin{align}
  z 
  & \sim p(z)
  \\
  w 
  & \sim N(0, z^2)
\end{align}$$

This is a fairly general formulation: letting $p(z)$ be Bernoulli, for example, leads to the discrete spike-and-slab.  



Following LUW 2017, we make two modifications:

1) we use the log-uniform prior for $z$, corresponding to a continuous relaxation of the discrete spike-and-slab;

2) rather than assigning each individual weight $w_{ij}$ its own $z_{ij}$, we constrain each row of weights $w_{i \cdot}$ to share a single $z_i$, thus enforcing group sparsity.

Under this formulation, the Bayesian model $p(w_{ij} | z_{i} p(z_{i})$ is:

$$\begin{align} 
  p(z_{i}) &\propto |z_{i}|^{-1}
  \\
  p(w_{ij} | z_{i}) &= N \left(w_{ij} | 0, z_{i}^2 \right)
\end{align}$$

In order to avoid MCMC on the potentially very large number of parameters, we adopt _LUW 2017_'s variational Bayes algorithm (in turn adapted from _Kingma, Salimans, Welling 2015_ and _Molchanov, Ashukha, Vetrov 2017_).  Thus, we use the variational distribution

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, \mu_{z_{i} }^2 \alpha_{i} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$

where $\alpha_i$ is the dropout rate of a given row of weights.  Furthermore, following _Molchanov, Ashukha, Vetrov 2017_, we reparameterize $\mu_{z_{i} }^2 \alpha_{i} = \sigma_{z_i}^2$ because the approximate posterior over $z$ suffers from high variance gradients.  Thus we arrive at the the variational distribution:

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, {\color{red}{\sigma_{z_i}^2}} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$


__One important item to notice here:__ the (now implied) dropout parameter $\alpha_{i} = \dfrac{\sigma_{z_i}^2}{\mu_{z_i}^2}$ is the inverse of a Wald statistic.  _(Perhaps this being a Wald statistic is just so obvious it doesn't need mentioning, but I don't believe any of the the papers positing this as a dropout parameter say this explicitly)_  This might be used to provide guidance on how to threshold $\alpha_{i}$ appropriately for variable selection (LUW2017 only addresses thresholding $\alpha_i$ in order to achieve a desired level of compression).



## multiple layers

In a deep neural network (i.e. with more than 1 layer), we place the sparsity prior on every layer (investigation whether results are better if only placing the sparsity prior on the first layer may be warranted, however).  Other than requiring another index $l$ denoting the layer, the formulation is similar:


Let $l$ index the layer of the weight matrix $W_l$ (here, $l \in \{1, 2\}$), and $i$ index the row of $W_l$, and $j$ index the columns. The Bayesian model $p(w_{lij} | z_{l i \cdot} p(z_{l i \cdot})$ and variational distribution $q(W_l, z)$ are:

$$\begin{align} 
  p(z_{li \cdot}) &\propto |z_{li \cdot}|^{-1}
  \\
  p(w_{lij} | z_{l i \cdot}) &= N \left(w_{lij} | 0, z_{l i \cdot}^2 \right)
  \\
  q(W_l, z) &= \prod_{l, i} N \left(z_{li \cdot} | \mu_{z_{l i \cdot}}, \mu_{z_{l i \cdot} }^2 \alpha_{l i \cdot} \right) \prod_{l,i,j} N \left( w_{lij} | z_{l i \cdot} \mu_{lij}, z_{l i \cdot}^2 \sigma_{lij}^2 \right)
\end{align}$$


The upshot of all of this is just that we can perform the same operations on each layer, and the results are easily separable (multiply / sum each layer's results to get results for entire network)


# variational algorithm and derivations

## deriving the ELBO (general)

For notational simplicity, in the following I use $z$ to denote both the weights ($W$) and scale parameters ($z$), and $phi$ to denote all parameters governing the variational distribution ($\{\mu_{z_i}, \alpha_i, \sigma_{z_i}^2, \mu_{ij}, \sigma_{ij}^2\}$).

Deriving ELBO:

$$\begin{align}
  KL\left(q_\phi(z) || p(z|x)\right)
    &=
    \sum_z q_\phi(z) \log \left( \dfrac{ q_\phi(z) }{ p(z|x) } \right)
    \\
    &= 
    \sum_z q_\phi(z) \log \left( q_\phi(z) \cdot \dfrac{ p(x) }{ p(z,x) } \right)
    \\
    &=
    \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\}
      + \sum_z q_\phi(z) \left( \log p(x) \right)
      \\
    &= \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\} 
    + \log p(x) 
      && \text{since } p(x) \text{ is constant w.r.t. } z
    \\
  \text{Thus } \log p(x)
  &=
  KL\left(q_\phi(z) || p(z|x) \right) 
    - \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\}
  \\
  &=
  KL\left(q_\phi(z) || p(z|x) \right) 
    \underbrace{- E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z, x) \right]}_{ELBO}
\end{align}$$


The second term is the ELBO $\mathcal{L}(\phi)$.  Since $\log p(x)$ is constant w.r.t. $q_\phi (z)$, maximizing the ELBO over $\phi$ minimizes the first term, the KL divergence between the variational distribution $q_\phi(z)$ and the posterior $p(z|x)$.


### (side note: interpretation of ELBO)

I mentioned a potentially interesting interpretation of the ELBO during one of our meetings.  Just wanted to follow up on this:

We can rearrange the ELBO such that:
$$\begin{align} 
  \mathcal{L}(\phi) 
  &= 
  - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z, x) \right]
  \\
  &=
  - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(x|z) - \log p(z) \right]
  \\
  &=
  E_{q_\phi(z)} \left[\log p(x|z) \right] - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z)\right]
  \\
  &=
  E_{q_\phi(z)} \left[\log p(x|z) \right] - KL \left(q_\phi(z) || p(z)\right)
\end{align}$$

The first term is the log-likelihood integrated over $z$, whereas the second term is the KL divergence between the variational distribution for $z$ and the prior on $z$.  I.e. the ELBO balances the likelihood of the data under the variational distribution against the "distance" the variational distribution must move from the prior.  As the variational distribution is our approximation to the posterior, we have a nice parallel here to the usual Bayesian idea of the data moving the posterior away from the prior.


A bit more rigorously: if use the interpretation from information theory of $\log \dfrac{1}{q_\phi(z)}$ as the "surprise" of obtaining $z$ if we assume that $z$ is generated via the variational distribution $q_\phi(z)$, we can think of the terms as follows:

- $E_{q_\phi(z)} \left[\log p(x|z) \right]$ is the (negative) expected surprise of the data under our model when $z$ is sampled under $q_\phi(z)$;

- $KL \left(q_\phi(z) || p(z)\right)$ is the expectation *when sampling $z$ under $q_\phi(z)$* of how surprised we are by $z$ given that we assume the variational distribution is true, minus how surprised we "should" have been by $z$ according to the model prior.




## derivations for this specific model

Reverting to previous notation ($W, z$ instead of just $z$), our last expression for the ELBO is:

$$\begin{align} 
  \mathcal{L}(\phi) 
  &= 
    E_{q_\phi(W, z)} \left[\log p(x|W, z) \right] - 
    KL \left(q_\phi(W, z) || p(W, z)\right)
  \\
  &= 
    E_{q_\phi(W|z) q_\phi(z)} \left[\log p(x|W, z) \right] -  
    E_{q_\phi(W|z) q_\phi(z)}\left[ \log \dfrac{q_\phi(W,z)}{p(W,z)} \right]
  \\
  &=
    E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(x|W, z) \right] -  
    E_{q_\phi(z)} \left\{ E_{q_\phi(W|z)}\left[ \log \dfrac{q_\phi(W|z)}{p(W|z)} + \log \dfrac{q_\phi(z)}{p(z)} \right] \right\}
  \\
  &=
    E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(x|W, z) \right] -  
    E_{q_\phi(z)} \left[ KL\left( q_\phi(W|z) || p(W|z) \right) \right]- 
    KL \left(q_\phi(z) || p(z) \right)
  \\
\end{align}$$



Using the distributions above

- $q_\phi(W|z) = \prod_{i,j} N(w_{ij} | z_i \mu_{ij}, z_i^2 \sigma_{ij}^2)$
- $p(W|z) = \prod_{i,j} N(w_{ij} | 0, z_i^2)$

we have:

$$\begin{align} 
  KL\left( q_\phi(W|z) || p(W|z) \right)
  &=
    E_{q_\phi(W|z)}
    \left[
      \log \dfrac{q_\phi(W|z)}{p(W|z)}
    \right]
  \\
  &=
    E_{q_\phi(W|z)}
    \left[
      \sum_{i,j} -\frac{1}{2} 
      \left(
        \log(2\pi z_i^2 \sigma_{ij}^2)
        + \frac{(w_{ij} - z_i \mu_{ij})^2}{z_i^2 \sigma_{ij}^2}
        -\log (2\pi z_i^2)
        - \frac{w_{ij}^2}{z_i^2}
      \right)
    \right]
  \\
  &=
  \frac{1}{2} E_{q_\phi(W|z)}
    \left[
      \sum_{i,j}
      \left(
        \log \frac{2 \pi z_i^2 }{2 \pi z_i^2 \sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left( 
          (1-\sigma_{ij}^2)w_{ij}^2 - 2 z_i \mu_{ij} w_{ij} + z_i^2 \mu_{ij}^2
        \right)
      \right)
    \right]
  \\
  &=
  \frac{1}{2} 
      \sum_{i,j}
      \left(
        \log \frac{1}{\sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left \{
          (1-\sigma_{ij}^2) \left( E_{q_\phi(W|z)} \left[ w_{ij}^2 \right]\right) 
          - 2 z_i \mu_{ij} \left( E_{q_\phi(W|z)} [w_{ij}] \right) 
          + z_i^2 \mu_{ij}^2  
        \right\}
      \right)
  \\
  &=
  \frac{1}{2} 
      \sum_{i,j}
      \left(
        \log \frac{1}{\sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left\{ 
          (1-\sigma_{ij}^2) \left( z_i^2 \sigma_{ij}^2  + z_i^2 \mu_{ij}^2 \right) - 2 z_i^2 \mu_{ij}^2 + z_i^2 \mu_{ij}^2
        \right\}
      \right)
  \\
  &=
    \dfrac{1}{2}
    \sum_{i,j}
    \left(
      -\log \sigma_{ij}^2 + \sigma_{ij}^2 + \mu_{ij}^2 - 1
    \right)
\end{align}$$






We rely on torch's automatic differentiation and gradient calculations to minimize the objective function.  In the scalar response case, we use $MSE + KL/n$.



$$\begin{align}
  \log P(X) 
    &=
    E_{q_{\theta}(z)} \left[ \log p(X|Z) \right]
    - D_{KL} \left( q_\theta (z) ~||~ p(z) \right)
    + D_{KL} \left( q_\theta (z) ~||~ p(z|x) \right)
\end{align}$$




## check:
- does the original code limit the range of $\alpha_i \in [0,1]$?
- the original code for _LUW 2017_"clips" variances ---- what is this about?




















# Linear data

basic simulation setting:  

- p > n
- sparse truth (4 true covariates, 100 nuisance)
- p = 104, n = 100
