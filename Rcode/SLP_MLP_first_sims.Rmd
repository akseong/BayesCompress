---
title: "First simulations"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: hide
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### Misc:
library(knitr)
library(here)
library(tidyr)
library(dplyr)
library(kableExtra)
library(ggplot2)
library(latex2exp)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}



cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap = NULL,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  # common additional unspecified args (...):
  #     digits = 2
  #     format.args = (list(big.mark = ","))
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "BayesianLayers.R"))
source(here("Rcode", "sim_functions.R"))
```

<style type="text/css">

  #TOC>ul>li, h1{
    font-weight: bold;
  }
  
</style>


# Background
The motivation for this model is to combine the variable selection properties of Bayesian spike-and-slab with the expressivity of a neural network.  We do this by placing a spike-and-slab prior on the _rows_ of a weight matrix $W$ representing one layer of a neural network.

Let $X$ be a $n \times I$ matrix representing $n$ observations of $I$ covariates, and $W$ be a $I \times J$ weight matrix.  We can then represent the computation performed by the first (input) layer of a neural network as $XW + b = H$, where $b$ is a length $J$ vector of biases (which is duplicated $n$ times into a $n \times J$ matrix) and $H$ is the pre-activation (that is, if $\sigma ()$ denotes the activation function, $\sigma(H) = \tilde{H}$ is the input for the next layer in the neural network).

If all of the weights $w_{ij}$ in row $i$ of $W$ are 0, then the covariate in column $i$ of $X$ does not contribute to $H$ and therefore also has no effect on the final prediction.  Thus by placing a spike-and-slab prior on the rows of $W$, we obtain variable selection.

We rely heavily here on the formulation and variational Bayes algorithm developed by _Louizos, Ullrich, Welling 2017_, who proposed using a group sparsity prior in order to reduce the size of the weight matrices in a neural network.  Their primary focus is on decreasing the size of the neural network while preserving quality of prediction --- the method leads to compression of the neural network since if an entire row or column of weights is 0, then the dimension of the weight matrix can be reduced.  Accordingly, their discussion of how to tune what they term "the dropout parameter" ($\alpha_i$), is based around desired compression rates rather than how to use it for principled variable selection.


## notes:

`r colorize("__How to differentiate ourselves from LUW 2017__")`:  

- Need to experiment to see if it's better to only do this for the input layer (as opposed to all layers, which is the current implementation)  
- a more rigorous interpretation of the dropout parameter $\alpha_i$ as the equivalent to a Wald statistic  
- (Adam's paper) using normalizing flow to model non-local prior
- some way to reconcile mean-field assumption of Variational Bayes with group sparsity?  
- as _LUW 2017_ note, the log-uniform prior does require independence between parameters (but grouping by row creates dependence), so the joint Bayesian model developed in LUW 2017 is not strictly accurate and is more of an approximation
- obvious difference: provide empirical results on how good the method is at variable selection


__Note on indices__: 

$i \in \{1, 2, ..., I\}$ indexes the rows of the weight matrix $W$, i.e. the inputs or covariates.  $j \in \{1, 2, ...., J\}$ indexes the columns of $W$, i.e. the output's dimensionality.  Thus, representing a fully connected layer of a neural network as $XW + b = H$, the components are:  

- $X$: $n \times I$ matrix of covariates ($n$ observations of $I$ covariates)  
- $W$: $I \times J$ weight matrix  
- $b$: length $J$ intercept vector (gets "broadcast", i.e. duplicated and stacked/rowbound to form a $n \times J$ matrix)  
- $H$: $n \times J$ pre-activation

In a single-layer neural network, $H$ is our prediction $\hat{y}$.  
In a multi-layer neural network, $H$ passes through some activation function $\sigma()$.  This $\sigma(H) = \tilde H$ then becomes the input for the next layer.  


# Model

We adopt a scale mixture of Normals as the prior for the weights:

$$\begin{align}
  z 
  & \sim p(z)
  \\
  w 
  & \sim N(0, z^2)
\end{align}$$

This is a fairly general formulation: letting $p(z)$ be Bernoulli, for example, leads to the discrete spike-and-slab.  



Following LUW 2017, we make two modifications:

1) we use the log-uniform prior for $z$, corresponding to a continuous relaxation of the discrete spike-and-slab;

2) rather than assigning each individual weight $w_{ij}$ its own $z_{ij}$, we constrain each row of weights $w_{i \cdot}$ to share a single $z_i$, thus enforcing group sparsity.

Under this formulation, the Bayesian model is:

$$\begin{align} 
  p(z_{i}) &\propto |z_{i}|^{-1}
  \\
  p(w_{ij} | z_{i}) &= N(w_{ij} | 0, z_{i}^2)
\end{align}$$

And therefore we have:

$$\begin{align} 
  p(W, \mathbf{z}) 
  &= \prod_i \frac{1}{|z_i|}\prod_{i,j} N(w_{ij} | 0, z_{i}^2)
\end{align}$$



In order to avoid MCMC on the potentially very large number of parameters, we adopt _LUW 2017_'s variational Bayes algorithm (in turn adapted from _Kingma, Salimans, Welling 2015_ and _Molchanov, Ashukha, Vetrov 2017_).  Thus, we use the variational distribution

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, \mu_{z_{i} }^2 \alpha_{i} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$

where $\alpha_i$ is the dropout rate of a given row of weights.  Furthermore, following _Molchanov, Ashukha, Vetrov 2017_, we reparameterize $\mu_{z_{i} }^2 \alpha_{i} = \sigma_{z_i}^2$ because the approximate posterior over $z$ suffers from high variance gradients.  Thus we arrive at the the variational distribution:

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, {\color{red}{\sigma_{z_i}^2}} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$


`r colorize("__One important item to notice here:__")` the (now implied) dropout parameter $\alpha_{i} = \dfrac{\sigma_{z_i}^2}{\mu_{z_i}^2}$ is the inverse of a Wald statistic for $z_i$.  _(Perhaps this being a Wald statistic is just so obvious it doesn't need mentioning, but I don't believe any of the the papers positing this as a dropout parameter say this explicitly)_  This might be used to provide guidance on how to threshold $\alpha_{i}$ appropriately for variable selection (LUW2017 only addresses thresholding $\alpha_i$ in order to achieve a desired level of compression).


### need help here

From what I can tell, the full formulation appears to be (letting $f(W,X)$ denote the function of the covariates $X$ parameterized by the neural network's weights $W$):


$$\begin{align}
   y & \sim N(y; f(W, X), \sigma^2)
  \\
  p(W | z) &= \prod_{ij} p(w_{ij} | z_i) = \prod_{ij} N \left(w_{ij} | 0, z_{i}^2 \right)
  \\
  p(z) &= \prod_i p(z_{i}) \propto \prod_{ij} |z_{i}|^{-1}
  \\
\end{align}$$

And the variational distributions (based on code):

$$\begin{align}
  q(W, z)
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, {\color{red}{\sigma_{z_i}^2}} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
  \\
  \mu_{ij} &\sim N(0,1/J)
  \\
  \sigma_{ij}^2 &\sim logNormal(-9, 0.01)
  \\
  \mu_{z_i} &\sim N(0, 1/I)
  \\
  \sigma_{z_i}^2 &\sim logNormal(-9, 0.01)
  \\
  \alpha_{i} &= \dfrac{\sigma_{z_i}^2}{\mu_{z_i}^2}
\end{align}$$


`r colorize("A few things that I do not understand are:")`  

1) the model for the data, i.e. $y \sim N(y; f(W, X), \sigma^2)$, is never addressed in _LUW 2017_.  I feel like I'm missing something fundamental here --- none of the literature appears to address this.  

  - the only place that a distributional model for the data is mentioned is in the ELBO, $\mathcal{L}(\phi) = E_{q_\phi(W|z) q_\phi(z)} \left[ \log \color{red}{p(y|W, z)} \right] - E_{q_\phi(z)} \left[ KL\left( q_\phi(W|z) || p(W|z) \right) \right]-KL \left(q_\phi(z) || p(z) \right)$.
  - it seems like this likelihood term simply gets replaced by MSE in the scalar response case (which would comport with an assumption that $y \sim N(f(W, X), \sigma^2)$, at least up to a multiplier), or cross-entropy when the data/response are pictures/labels (again, corresponding to an assumed multinomial dist'n).  This makes sense to me, BUT what doesn't make sense is that 1) the variance of the response is not modeled anywhere, and 2) the multiplier probably matters a little, no?


2) the priors for many parameters in the variational distribution are never mentioned in the paper / left implicit --- in particular, the priors placed on $\mu_{z_i}, \alpha_i, \mu_{ij}, \sigma_{ij}^2$.  

  - The code makes draws from Normal and log-Normal distributions _to initialize_ $\mu_{z_i}, \alpha_i, \mu_{ij}, \sigma_{ij}^2$, and these parameters are updated via the gradient of the ELBO.  However, their distributions are not part of the computation.  I suppose these parameters are only part of the _variational_ distribution, and so ... their distributions don't actually matter much?  We're just trying to optimize the ELBO using them?





## multiple layers

In a deep neural network (i.e. with more than 1 layer), we place the sparsity prior on every layer (investigation whether results are better if only placing the sparsity prior on the first layer may be warranted, however).  Other than requiring another index $l$ denoting the layer, the formulation is similar:


Let $l$ index the layer of the weight matrix $W_l$ (here, $l \in \{1, 2\}$), and $i$ index the row of $W_l$, and $j$ index the columns. The Bayesian model $p(w_{lij} | z_{l i \cdot} p(z_{l i \cdot})$ and variational distribution $q(W_l, z)$ are:

$$\begin{align} 
  p(z_{li \cdot}) &\propto |z_{li \cdot}|^{-1}
  \\
  p(w_{lij} | z_{l i \cdot}) &= N \left(w_{lij} | 0, z_{l i \cdot}^2 \right)
  \\
  q(W_l, z) &= \prod_{l, i} N \left(z_{li \cdot} | \mu_{z_{l i \cdot}}, \mu_{z_{l i \cdot} }^2 \alpha_{l i \cdot} \right) \prod_{l,i,j} N \left( w_{lij} | z_{l i \cdot} \mu_{lij}, z_{l i \cdot}^2 \sigma_{lij}^2 \right)
\end{align}$$


The upshot of all of this is just that we can perform the same operations on each layer, and the results are easily separable (multiply / sum each layer's results to get results for entire network)


# Derivations

## ELBO (general)

For notational simplicity, in the following I use $z$ to denote both the weights ($W$) and scale parameters ($z$), and $phi$ to denote all parameters governing the variational distribution ($\{\mu_{z_i}, \alpha_i, \sigma_{z_i}^2, \mu_{ij}, \sigma_{ij}^2\}$).

Deriving ELBO:

$$\begin{align}
  KL\left(q_\phi(z) || p(z|x)\right)
    &=
    \sum_z q_\phi(z) \log \left( \dfrac{ q_\phi(z) }{ p(z|x) } \right)
    \\
    &= 
    \sum_z q_\phi(z) \log \left( q_\phi(z) \cdot \dfrac{ p(x) }{ p(z,x) } \right)
    \\
    &=
    \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\}
      + \sum_z q_\phi(z) \left( \log p(x) \right)
      \\
    &= \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\} 
    + \log p(x) 
      && \text{since } p(x) \text{ is constant w.r.t. } z
    \\
  \text{Thus } \log p(x)
  &=
  KL\left(q_\phi(z) || p(z|x) \right) 
    - \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\}
  \\
  &=
  KL\left(q_\phi(z) || p(z|x) \right) 
    \underbrace{- E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z, x) \right]}_{ELBO}
\end{align}$$


The second term is the ELBO $\mathcal{L}(\phi)$.  Since $\log p(x)$ is constant w.r.t. $q_\phi (z)$, maximizing the ELBO over $\phi$ minimizes the first term, the KL divergence between the variational distribution $q_\phi(z)$ and the posterior $p(z|x)$.


### Bayesian interpretation of ELBO

I mentioned a potentially interesting interpretation of the ELBO at our face-to-face meeting at the MoonGoat.  Just wanted to follow up on this, mostly as a side note:

We can rearrange the above expression for the ELBO such that:
$$\begin{align} 
  \mathcal{L}(\phi) 
  &= 
  - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z, y) \right]
  \\
  &=
  - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(y|z) - \log p(z) \right]
  \\
  &=
  E_{q_\phi(z)} \left[\log p(y|z) \right] - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z)\right]
  \\
  &=
  E_{q_\phi(z)} \left[\log p(y|z) \right] - KL \left(q_\phi(z) || p(z)\right)
\end{align}$$

The first term is the log-likelihood integrated over $z$, whereas the second term is the KL divergence between the variational distribution $q_\phi(z)$ and the prior $p(z)$.  In other words, the ELBO balances the expected likelihood of the data (under the variational distribution) against the expected "distance" between the variational distribution and the prior.  As the variational distribution is our approximation to the posterior, we have a nice parallel here to the usual Bayesian idea of the data moving the posterior away from the prior.


A bit more rigorously: if we use the information theory interpretation of $\log \dfrac{1}{q_\phi(z)}$ as the "surprise" of drawing $z$ from the variational distribution $q_\phi(z)$, we can think of the terms as follows:

$$\begin{align} 
  \mathcal{L}(\phi) 
  &=
  E_{q_\phi(z)} \left[\log p(y|z) \right] - KL \left(q_\phi(z) || p(z)\right)
  \\
  &=
  -KL \left(q_\phi(z) || p(z)\right) - E_{q_\phi(z)} \left[\log \dfrac{1}{p(y|z)} \right] 
\end{align}$$

- $-KL \left(q_\phi(z) || p(z)\right)$ is the expectation, *when sampling $z$ from $q_\phi(z)$*, of how surprised we are by $z$ (given that we assume the variational distribution is true) minus how surprised we would have been by $z$ under the prior $p(z)$;

- $E_{q_\phi(z)} \left[\log \dfrac{1}{p(y|z)} \right]$ is the expected surprise of the data under *our model* when $z$ is sampled under *our approximation* $q_\phi(z)$.


So the ELBO, essentially, balances 1) the expected difference in surprise between the variational and prior distributions with 2) the expected surprise of the data according to the model BUT when sampling $z$ under the variational / approximate posterior.  So maximizing the ELBO involves minimizing the surprise of the data by adjusting the approximate posterior, while keeping the approximate posterior near the prior.


## derivations for this specific model

Reverting to previous notation ($W, z$ instead of just $z$), our last expression for the ELBO is:

$$\begin{align} 
  \mathcal{L}(\phi) 
  &= 
    E_{q_\phi(W, z)} \left[\log p(y|W, z) \right] - 
    KL \left(q_\phi(W, z) || p(W, z)\right)
  \\
  &= 
    E_{q_\phi(W|z) q_\phi(z)} \left[\log p(y|W, z) \right] -  
    E_{q_\phi(W|z) q_\phi(z)}\left[ \log \dfrac{q_\phi(W,z)}{p(W,z)} \right]
  \\
  &=
    E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(y|W, z) \right] -  
    E_{q_\phi(z)} \left\{ E_{q_\phi(W|z)}\left[ \log \dfrac{q_\phi(W|z)}{p(W|z)} + \log \dfrac{q_\phi(z)}{p(z)} \right] \right\}
  \\
  &=
    E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(y|W, z) \right] -  
    E_{q_\phi(z)} \left[ KL\left( q_\phi(W|z) || p(W|z) \right) \right]- 
    KL \left(q_\phi(z) || p(z) \right)
  \\
\end{align}$$


### Log-likelihood

Since $y$ depends on $z$ only through $W$, $p(y|W,z) = p(y|W)$.  Thus the first term, $E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(y|W, z) \right]$ is the expectation of the log-likelihood $\log p(y|W)$ under the variational distribution.

In the scalar response case, where the assumption appears to be (?) $p(y | W) = N(y; f(W, X), \sigma^2)$, we have

$$\begin{align}
  \log p(y|W)
  &=
    -\frac{1}{2} \log (2 \pi \sigma^2)
    -\frac{1}{2 \sigma^2} \left( y - f(W, X) \right)^2
  \\
\end{align}$$

`r colorize("This portion of the ELBO, in the code, only appears as MSE.")`  In other words, in the pytorch implementation posted on their github, LUW 2017 designates the objective function to be minimized as $MSE + KL\left( q_\phi(W|z) || p(W|z) \right) + KL \left(q_\phi(z) || p(z) \right)$.

So instead of the likelihood, the objective function only uses MSE.  Leaving out $\frac{1}{2} \log (2 \pi \sigma^2)$ is fine, but what about the $\frac{1}{2 \sigma^2}$ multiplier on the $MSE$?  

`r colorize("This seems problematic to me")` because this objective function is NOT an affine function of the ELBO (i.e. optimal parameter values will be different, no?).


### KL for W|z

Using the distributions for the weights

- $q_\phi(W|z) = \prod_{i,j} N(w_{ij} | z_i \mu_{ij}, z_i^2 \sigma_{ij}^2)$
- $p(W|z) = \prod_{i,j} N(w_{ij} | 0, z_i^2)$

we have:

$$\begin{align} 
  KL\left( q_\phi(W|z) || p(W|z) \right)
  &=
    E_{q_\phi(W|z)}
    \left[
      \log \dfrac{q_\phi(W|z)}{p(W|z)}
    \right]
  \\
  &=
    E_{q_\phi(W|z)}
    \left[
      \sum_{i,j} -\frac{1}{2} 
      \left(
        \log(2\pi z_i^2 \sigma_{ij}^2)
        + \frac{(w_{ij} - z_i \mu_{ij})^2}{z_i^2 \sigma_{ij}^2}
        -\log (2\pi z_i^2)
        - \frac{w_{ij}^2}{z_i^2}
      \right)
    \right]
  \\
  &=
  \frac{1}{2} E_{q_\phi(W|z)}
    \left[
      \sum_{i,j}
      \left(
        \log \frac{2 \pi z_i^2 }{2 \pi z_i^2 \sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left( 
          (1-\sigma_{ij}^2)w_{ij}^2 - 2 z_i \mu_{ij} w_{ij} + z_i^2 \mu_{ij}^2
        \right)
      \right)
    \right]
  \\
  &=
  \frac{1}{2} 
      \sum_{i,j}
      \left(
        \log \frac{1}{\sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left \{
          (1-\sigma_{ij}^2) \left( E_{q_\phi(W|z)} \left[ w_{ij}^2 \right]\right) 
          - 2 z_i \mu_{ij} \left( E_{q_\phi(W|z)} [w_{ij}] \right) 
          + z_i^2 \mu_{ij}^2  
        \right\}
      \right)
  \\
  &=
  \frac{1}{2} 
      \sum_{i,j}
      \left(
        \log \frac{1}{\sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left\{ 
          (1-\sigma_{ij}^2) \left( z_i^2 \sigma_{ij}^2  + z_i^2 \mu_{ij}^2 \right) - 2 z_i^2 \mu_{ij}^2 + z_i^2 \mu_{ij}^2
        \right\}
      \right)
  \\
  &=
    \dfrac{1}{2}
    \sum_{i,j}
    \left(
      -\log \sigma_{ij}^2 + \sigma_{ij}^2 + \mu_{ij}^2 - 1
    \right)
\end{align}$$

### KL for z

An approximation to $KL \left(q_\phi(z) || p(z) \right)$ is provided by $Molchanov, Ashukha, Vetrov 2017$:

$$KL \left(q_\phi(z) || p(z) \right) \approx \sum_i (k_1 \sigma (k_2 + k_3 \log(\alpha_i))) - 0.5 m (-\log \alpha_i) - k_3)$$
where $k_1 = 0.63576, k_2 = 1.87320, k_3 = 1.48695$; $\sigma$ is the sigmoid function; and $m$ is the softplus function.  Importantly, note that this term depends only on the implied dropout rate $\alpha_i = \frac{\sigma_{z_i}^2}{\mu_{z_i}^2}$ --- which we identified as having a form similar to a Wald statistic.



### optimization

`r colorize("Issue described below does not seem to be impacting results --- as far as I can tell --- but may lead to problems down the line?")`

We rely on torch's automatic differentiation and gradient calculations to minimize the objective function.  In the scalar response case, LUW 2017 and my current simulations use the objective function $MSE + KL/n$ (letting $KL$ stand in for both KL terms).

__Issue__: $MSE + KL/n$ contains the same components as the ELBO, but is not quite an affine function of the ELBO (i.e. optimal parameter values $\phi$ for the ELBO and for $MSE + KL/n$ are `r colorize("not")` necessarily the same, __which seems problematic__).  In the ELBO, the terms estimated by the $MSE$ and $KL$ are scaled differently relative to each other, i.e. 

$$\widehat{\mathcal{L}}(\phi) = -n \times \left( \frac{MSE}{2 \sigma^2}  + KL/n \right) + c$$
where $c$ is a constant not depending on $\phi$.




## other things to check:
- does the original code limit the range of $\alpha_i \in [0,1]$?
- the original code for _LUW 2017_"clips" variances ---- what is this about?



# Simulations

Simulation settings:

1) response $y$ is linearly related to covariates $X$.  100 nuisance variables, 4 "true" covariates.  n = 100.  

  a) `SLP` = single neuron model (i.e. basically a scale mixture of normals Spike-and-Slab on a linear regression model)  
  
  b) `MLP` = multiple neurons, multiple layers  
      - 2 hidden layers with 50 nodes and 25 nodes respectively


2) response $y$ is non-linearly related to covariates $X$.  96 nuisance variables, 4 "true" covariates.  Model used is identical to the one labeled `MLP` above.


## setting 1a)

_Linearly-related data, single neuron model_


__Data generation:__  

- __Covariates__: $\vec{x}_i \sim N(0, I)$ for $i \in \{1, 2, ..., I\}$.  $I$ is the number of covariates (what we usually denote using $p$); essentially, every entry in the matrix $X_{n \times I}$ is iid N(0,1).

- __Response__: letting $\epsilon \sim N(0,1)$, $$y = -0.5 x_1 + 1 x_2 - 2 x_3 + 4 x_4 + \epsilon$$ i.e. only 4 "true" covariates ($x_1, x_2, x_3, x_4$) and 100 nuisance variables ($x_5, x_6, ..., x_{104}$)  

- __number of observations__ $n$ = 100  

- __error variance__ $\sigma^2$ = 1  


__Model:__

`SLP` has only 1 neuron, i.e. we are essentially fitting a scale mixture of normals regression model.

Results averaged from running methods on 100 simulated datasets

```{r SLP_RESULTS}
fpath <- here("Rcode", "results", "linear_sim_sig1.Rdata")
load(fpath)

true_coefs = c(-0.5, 1, -2, 4, rep(0, times = 100))
tru <- true_coefs != 0 



# slnj bare results

# threshold log_alphas at -3 (exp(-3) \approx 0.05)
# what is a reasonable dropout probability?  
log_alpha_threshold <- -3
txt <- paste0("log_alpha_threshold = ", log_alpha_threshold, 
              " (i.e. dropout rate of <",
              round(exp(log_alpha_threshold), 5), 
              " required for inclusion)")

slnj_tab <- get_restab(
  res = res, 
  tru = tru, 
  log_alpha_threshold = log_alpha_threshold, 
  method_name = "SLP",
  verbose = FALSE
)

mykable(tab = slnj_tab, cap = "SLP results")

```

Model inclusion for `SLP` decided using the "implied dropout parameter" $\alpha_i$.  
`r colorize(txt)`

Model fit results measured in terms of MSE are in the table below.  Reported MSE for Spike-and-Slab models are calculated using each trial's posterior means only (i.e. each trial's `fit_MSE` and `coef_MSE` are calculated using the posterior means for the coefficients.  The mean and sd of these  across trials are then calculated.)



```{r SLNJ_other_metrics}
mse_tab <- get_mses(res, method_name = "SLP")
mykable(tab = round(mse_tab, 4), cap = "fit metrics")

```


I'm looking at the MSE's and realizing that I did not apply the dropout before calculating the coefficient MSE.  So for both the Spike-and-Slab and for our models, the MSEs use the estimated posterior mean even when the decision was to drop the variable from the model (instead of 0).  This applies to the next batch of results as well.  Unfortunately I did not save the network weights, so cannot recalculate it. 




## setting 1b) 

__Data generation__: identical to setting 1a).  

__Model__: `MLP` = multiple neurons, multiple layers  

  - 2 hidden layers with 50 nodes and 25 nodes respectively


```{r MLP_results}

fname <- "MLP_linear_sim_sig1.Rdata"
fname2 <- "MLP_linear_sim_sig1_part2.Rdata"
fpath <- here::here("Rcode", "results", fname)
fpath2 <- here::here("Rcode", "results", fname2)

load(fpath)
load(fpath2)

# some results null (stopped simulation b/c needed to use computer)
res_non_null <- res[!sapply(res, is.null)]
result_non_null <- result[!sapply(result, is.null)]

res <- append(res_non_null, result_non_null)

# replace with true coefs
tru <-c(rep(1, 4), rep(0, 100))

mlnj_tab <- get_restab(
  res = res, 
  tru = tru, 
  log_alpha_threshold = log_alpha_threshold, 
  method_name = "MLP",
  verbose = FALSE
)

mykable(tab = mlnj_tab, cap = "MLP results")
```


The highly over-parameterized version of this model still performs quite well in terms of variable selection for linear data.

Below are metrics for the model fit.  Included are the coefficient MSEs, though they are not really relevant for the `MLP` (calculated based on input layer's coefficients for these variables, but there are 2 hidden layers in the neural network).


```{r MLP_fit_results}
mse_tab <- get_mses(res, method_name = "MLP")
mykable(tab = round(mse_tab, 4), cap = "fit metrics")
```





### comments

__Setting 1a) (linear data, single neuron model)__ is largely meant as proof of concept.  

- The results of the `SLP` and the `Spike-Slab` models should be similar, since fitting the `SLP` model corresponds to a fitting Spike-and-Slab model with 2 modifications: 1) the discrete Spike-and-Slab model has been relaxed to a scale-mixture of Gaussians, 2) rather than MCMC, a variational algorithm is employed to find optimal parameter values.

- results are encouraging.  Both `SLP` and `Spike-Slab` are more conservative than LASSO, with only a very small loss in power.  `Spike-Slab` is more conservative than `SLP`, but has a somewhat large loss in power. 

- lowest fit MSE.  `SLP` Appears to be overfitting a bit (since we'd expect the fit MSE to be $\approx 1$)


__Setting 1b) (linear data, deep neural network)__:  the intention here is to show that simply using the intended form of our method will still work fine with data in which we have more covariates than observations AND the relationship between the response and true covariates is simple (linear) --- i.e. even though the deep neural network is highly overparameterized for the data, we still maintain competitive performance in terms of both variable selection and fit.

- results are again quite encouraging.  Only a very small loss in performance: though the False Positive Rate is about 2x as large as the previous (and precisely minimally parameterized) version of the model, we are still well within desirable limits (by like an order of magnitude).  

- same comments regarding fit MSE (lowest, may be slightly overfitting?)

__FWER__ is somewhat concerning in both settings, but I'm not sure what a "good" FWER is when we have 100 nuisance variables but only 4 "true" covariates.


__Note__: we might employ a more sophisticated method for setting the inclusion threshold for the dropout parameter $\alpha_i$.  Right now inclusion criteria for the `SLP` and `MLP` methods are based on setting the threshold (approximately) at $\alpha_i < 0.05$.  (For the Spike-and-Slab models, posterior inclusion probability threshold is set to 0.95 to be included in the model).


## setting 2

__Data generation__: 

- __covariates__: identical to setting 1a), except upped the number of observations to 10,000 observations.

- __response__:  

  - non-linear relationship between response $y$ and $x_1, x_2, x_3, x_4$: $$y = f_1(x_1) + f_2(x_2) + f_3(x_3) + f_4(x_4) + \epsilon$$ with $\epsilon \sim N(0,1)$.  Below is a plot showing $f_1(x), f_2(x), f_3(x), f_4(x)$ (plotted using same x-coords to fit in 1 plot).

```{r}
x_grid <- -50:50 / 10
y1 <- fcn1(x_grid)
y2 <- fcn2(x_grid)
y3 <- fcn3(x_grid)
y4 <- fcn4(x_grid)

fcn_df <- data.frame(
  x_grid, y1, y2, y3, y4
)

names(fcn_df) <- c(
  "x",
  "f_1(x)",
  "f_2(x)",
  "f_3(x)",
  "f_4(x)"
)


fcn_df %>% 
  pivot_longer(cols = -x) %>% 
  ggplot() + 
  geom_line(
    aes(
      y = value,
      x = x,
      color = name
    )
  ) + 
  labs(
    title = "individual contributions of x1, x2, x3, x4 to response",
    color = ""
  )

```



__Model__:

`MLP` = multiple neurons, multiple layers  
      - 2 hidden layers with 50 nodes and 25 nodes respectively

- `max_epochs` = 40k
- convergence tolerance = 1e-6


```{r}
fname <- "MLP_fcnl_sim_sig1"
partial_fpaths <- here("Rcode", "results", paste0(fname, "_PARTIAL", 1:7, ".Rdata"))


fname_part2 <- "MLP_fcnl_sim_sig1_part2"
partial_fpaths_part2 <- here("Rcode", "results", paste0(fname_part2, "_PARTIAL", 1:5, ".Rdata"))

partial_fpaths_all <- c(partial_fpaths, partial_fpaths_part2)

mlnj_res <- list()

for (i in 1:length(partial_fpaths_all)){
  load(file = partial_fpaths_all[i])
  mlnj_res <- append(mlnj_res, partial_res)
}

tru <- c(rep(1, 4), rep(0, 96))

mlp_tab <- get_restab(res = mlnj_res, tru = tru, method_name = "MLP")
mykable(tab = mlp_tab, cap = "MLP on functional data")
```


`r colorize("These results are pretty bad.")`


### `r colorize("troubleshooting")`

```{r}
# # # # # # # # # # # # # # # # # # # # # # # # #
## troubleshooting ----
# # # # # # # # # # # # # # # # # # # # # # # # #
stop_reasons <- t(sapply(mlnj_res, function(X) X$mlnj$stop_reason))
stop_epoch <- sapply(mlnj_res, function(X) X$mlnj$epoch)
stop_reasons[which(stop_epoch == max(stop_epoch)), 1] <- TRUE
colSums(stop_reasons)
```

Most stop b/c met convergence criteria.  A few stop b/c max epochs reached.  Convergence criteria appears to be too easy to meet?

Also, perhaps setting the prior at $E[alpha_i] = 0.5$ is making it take a longer time to converge.  Perhaps setting the prior to reflect that we think the truth is sparse will help.


__looking at thresholding for alpha__:

Below are summaries of $\log(\alpha_i)$ for the 4 true covariates:

```{r}
# alphas of true covariates
log_alpha_mat <- t(sapply(mlnj_res, function(X) X$mlnj$log_dropout_alphas))
summary(log_alpha_mat[, 1:4])
```

The expectation was that $f_2(x_2)$ would be the hardest to learn (it's a sine wave), and that appears to be true.  



And below is a summary of the minimum values of the $\log(\alpha_i)$ for the nuisance variables:
```{r}
summary(apply(log_alpha_mat[, 5:100], 2, min))
```

So the good news is that there is a fair bit of separation between the values of $\log(\alpha_i)$ for the nuisance and true covariates, and that the separation is happening in the expected direction (lower values of $\alpha_i$ correspond to lower dropout / higher inclusion)


It appears the main difficulty might simply be figuring out how to set the alpha threshold?

Perhaps not kosher, but meant just as a quick diagnostic: try a Bonferroni correction on the dropout parameter --- threshold at 0.05 / 100:

```{r}
get_restab(res = mlnj_res, tru = tru, log_alpha_threshold = log(0.05/100), method_name = "MLP", verbose = TRUE)
```

I mean, it's not bad.

- just tried this on the linear data simulations, and for both the SLP and MLP, applying a Bonferroni correction decimates the power (TP rate ends up being about 0.25).



__other idea:__

Could also try ordering based on alpha:

```{r}
ord_alpha_mat <- apply(log_alpha_mat, 1, order)
ord_alpha_mat[1:4,]
```

Good news is that in every trial, true covariates have the 3 lowest $\alpha_i$ values.  However, only a few are able to pick up on covariate $x_2$ (again, somewhat expected.  perhaps a larger nn architecture would be better?)


```{r}
# what percentage get all 4 as having the lowest \alpha_i?
all4 <- apply(ord_alpha_mat, 2, function(X){
  sum(c(1:4) %in% X[1:4]) == 4
})  
mean(all4)

# which trials get all 4 correct?
which(all4)
```

So only 1/10 of the trials get all 4 true covariates as having the lowest $\alpha_i$'s.

```{r}
# why did these simulations stop training?
all4_inds <- which(all4)
stop_reasons[all4_inds, ]
```

None of these reached `max_epochs`, and instead reached the convergence criteria.



```{r}
alpha_mat <- exp(log_alpha_mat)  
ord_alpha_mat <- t(apply(alpha_mat, 1, order))


for (i in 1:ncol(alpha_mat)){
  alphas <- alpha_mat[i, ]
  ord_alphas <- order(alphas)
  cumsum(alphas[ord_alphas])
}

1 - pnorm(1/sqrt(alphas))


```



# Tweak training parameters 

Just wanted to quickly try changing training params (`convergence_crit`, `max_epochs`) to increase performance, but it did not help (and seems like stumbling around in the dark a little).

I also reduced the number of observations to 100 (from 10k) to see what would happen, but the KL/n term seems to be pretty highly dependent on $n$ (since it's scaled to be KL/n, shouldn't it be similar in magnitude?).
  
```{r}
# current results' KLs
metrics <- matrix(NA, nrow = length(mlnj_res), ncol = 3)
colnames(metrics) <- c("epoch", "mse", "kl")

for (i in 1:length(mlnj_res)){ 
 mat_row_ind <- which(mlnj_res[[i]]$mlnj$log_alpha_mat[, 101] == stop_epoch[i]) 
 metrics[i,] <- mlnj_res[[i]]$mlnj$log_alpha_mat[mat_row_ind, 101:103]
}

hist(metrics[,"kl"], breaks = 30, main = "KL term, n = 10k")
```


  

```{r n100_MLP}
# n100 results (smaller convergence crit, longer max_epochs)
fname <- "MLP_fcnl_sim_sig1_n100"
partial_fpaths <- here("Rcode", "results", paste0(fname, "_PARTIAL", 1:3, ".Rdata"))

mlnj_res <- list()

for (i in 1:length(partial_fpaths)){
  load(file = partial_fpaths[i])
  mlnj_res <- append(mlnj_res, partial_res)
}

stop_reasons <- t(sapply(mlnj_res, function(X) X$mlnj$stop_reason))
stop_epoch <- sapply(mlnj_res, function(X) X$mlnj$epoch)
stop_reasons[which(stop_epoch == max(stop_epoch)), 1] <- TRUE
colSums(stop_reasons)
```

With n = 100, all simulations (not a ton, but still) reached `max_epochs` of 40,000.


```{r n100_res}
metrics <- matrix(NA, nrow = length(mlnj_res), ncol = 3)
colnames(metrics) <- c("epoch", "mse", "kl")

for (i in 1:length(mlnj_res)){ 
 mat_row_ind <- which(mlnj_res[[i]]$mlnj$log_alpha_mat[, 101] == stop_epoch[i]) 
 metrics[i,] <- mlnj_res[[i]]$mlnj$log_alpha_mat[mat_row_ind, 101:103]
}

hist(metrics[,"kl"], breaks = 30, main = "KL term, n = 100")
```


`r colorize("The range of the KL term is quite different, which seems really weird.")`  Perhaps because the convergence criteria was not reached and the nn just trained for 40k epochs?  I suppose I could re-run the n = 10k trial for a ridiculous number of epochs and see what happens....


Also, the overall performance results for n = 100, with 2 different thresholds for $\alpha_i$:

```{r}
restab <- get_restab(res = mlnj_res, tru = tru, log_alpha_threshold = log(0.05), method_name = "MLP", verbose = TRUE)

mykable(tab = restab, cap = "fcnal data, n = 100, alpha threshold = 0.05")

restab <- get_restab(res = mlnj_res, tru = tru, log_alpha_threshold = log(0.05/100), method_name = "MLP", verbose = TRUE)

mykable(tab = restab, cap = "fcnal data, n = 100, alpha threshold = 0.05/100")
```

so a Bonferroni-type adjustment doesn't work well here either.



# Next Steps:

1. does nn architecture need to change?  Make it much larger?  

1. does fcnal data simulation setting seem reasonable?  (functional forms, $n$)

1. figure out effect of $n$ on KL magnitude

1. try a different training paradigm --- train/test/validation split (reasonable $n$ for this?) --- and base stopping rule on train vs test performance?  

  - does this make sense for variational Bayes, since the whole point is making this an optimization problem for the ELBO?

1. alternative training idea --- make the objective function more like the ELBO by using some estimate of the response's error variance (set $\sigma^2$ equal to the average of last 50 epochs' MSE's?).

1. figure out how to set alpha threshold  

  - Benhamini-Hochberg analog?
  - figure out a Bayesian FDR decision rule?  seems premature(?)
  - pursue the idea of thinking about it as an analog to a Wald statistic?
  

1. eventually: need better competitors for the functional data analysis (VCBART?  LASSO neural network?)  


1. could also try to implement Adam's model (normalizing flow with non-local prior)  




