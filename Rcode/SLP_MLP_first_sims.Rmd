---
title: "First simulations"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)


# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}



cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap = NULL,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  # common additional unspecified args (...):
  #     digits = 2
  #     format.args = (list(big.mark = ","))
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "BayesianLayers.R"))
source(here("Rcode", "sim_functions.R"))
```

<style type="text/css">

  #TOC>ul>li, h1{
    font-weight: bold;
  }
  
</style>


# Background
The motivation for this model is to combine the variable selection properties of Bayesian spike-and-slab with the expressivity of a neural network.  We do this by placing a spike-and-slab prior on the _rows_ of a weight matrix $W$ representing one layer of a neural network.

Let $X$ be a $n \times I$ matrix representing $n$ observations of $I$ covariates, and $W$ be a $I \times J$ weight matrix.  We can then represent the computation performed by the first (input) layer of a neural network as $XW + b = H$, where $b$ is a vector of biases and $H$ is the pre-activation (that is, if $\sigma ()$ denotes the activation function, $\sigma(H) = \tilde{H}$ is the input for the next layer in the neural network).

If all of the weights $w_{ij}$ in row $i$ of $W$ are 0, then the covariate in column $i$ of $X$ does not contribute to $H$ and therefore also has no effect on the final prediction.  Thus by placing a spike-and-slab prior on the rows of $W$, we obtain variable selection.

We rely heavily here on the formulation and variational Bayes algorithm developed by _Louizos, Ullrich, Welling 2017_, who proposed using a group sparsity prior in order to reduce the size of the weight matrices in a neural network.  Their primary focus is on decreasing the size of the neural network while preserving quality of prediction --- the method leads to compression of the neural network since if an entire row or column of weights is 0, then the dimension of the weight matrix can be reduced.  Accordingly, their discussion of how to tune what they term "the dropout parameter" ($\alpha_i$), is based around desired compression rates rather than how to use it for principled variable selection.


## notes:

- Need to experiment to see if it's better to only do this for the input layer (as opposed to all layers, which is the current implementation)

- Ways to differentiate ourselves:  
    - a more rigorous interpretation of the dropout parameter $\alpha_i$ as the equivalent to a Wald statistic  
    - (Adam's paper) using normalizing flow to model non-local prior
    - some way to reconcile mean-field assumption of Variational Bayes with group sparsity?  
    - as _LUW 2017_ note, the log-uniform prior does require independence between parameters (i.e. grouping by row is not kosher), so the joint Bayesian model developed in LUW 2017 is not strictly accurate and is more of an approximation
    - obvious difference: provide empirical results on how good the method is at variable selection



# Model

We adopt a scale mixture of Normals as the prior for the weights:

$$\begin{align}
  z 
  & \sim p(z)
  \\
  w 
  & \sim N(0, z^2)
\end{align}$$

This is a fairly general formulation: letting $p(z)$ be Bernoulli, for example, leads to the discrete spike-and-slab (with unit variance).  



Following LUW 2017, we make two modifications:

1) we use the log-uniform prior for $z$, corresponding to a continuous relaxation of the discrete spike-and-slab;

2) rather than assigning each individual weight $w_{ij}$ its own $z_{ij}$, we constrain each row of weights $w_{i \cdot}$ to share a single $z_i$, thus enforcing group sparsity.

Under this formulation, the Bayesian model is:

$$\begin{align} 
  p(z_{i}) &\propto |z_{i}|^{-1}
  \\
  p(w_{ij} | z_{i}) &= N(w_{ij} | 0, z_{i}^2)
\end{align}$$

And therefore we have:

$$\begin{align} 
  p(W, \mathbf{z}) 
  &= \prod_i \frac{1}{|z_i|}\prod_{i,j} N(w_{ij} | 0, z_{i}^2)
\end{align}$$



In order to avoid MCMC on the potentially very large number of parameters, we adopt _LUW 2017_'s variational Bayes algorithm (in turn adapted from _Kingma, Salimans, Welling 2015_ and _Molchanov, Ashukha, Vetrov 2017_).  Thus, we use the variational distribution

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, \mu_{z_{i} }^2 \alpha_{i} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$

where $\alpha_i$ is the dropout rate of a given row of weights.  Furthermore, following _Molchanov, Ashukha, Vetrov 2017_, we reparameterize $\mu_{z_{i} }^2 \alpha_{i} = \sigma_{z_i}^2$ because the approximate posterior over $z$ suffers from high variance gradients.  Thus we arrive at the the variational distribution:

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, {\color{red}{\sigma_{z_i}^2}} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$


__One important item to notice here:__ the (now implied) dropout parameter $\alpha_{i} = \dfrac{\sigma_{z_i}^2}{\mu_{z_i}^2}$ is the inverse of a Wald statistic for $z_i$.  _(Perhaps this being a Wald statistic is just so obvious it doesn't need mentioning, but I don't believe any of the the papers positing this as a dropout parameter say this explicitly)_  This might be used to provide guidance on how to threshold $\alpha_{i}$ appropriately for variable selection (LUW2017 only addresses thresholding $\alpha_i$ in order to achieve a desired level of compression).


### need help here

From what I can tell, the full formulation appears to be (letting $f(W,X)$ denote the function of the covariates $X$ parameterized by the neural network's weights $W$):


$$\begin{align}
   y & \sim N(y; f(W, X), \sigma^2)
  \\
  p(W | z) &= \prod_{ij} p(w_{ij} | z_i) = \prod_{ij} N \left(w_{ij} | 0, z_{i}^2 \right)
  \\
  p(z) &= \prod_i p(z_{i}) \propto \prod_{ij} |z_{i}|^{-1}
  \\
\end{align}$$

And the variational distributions (based on code):

$$\begin{align}
  q(W, z)
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, {\color{red}{\sigma_{z_i}^2}} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
  \\
  \mu_{ij} &\sim N(0,1/J)
  \\
  \sigma_{ij}^2 &\sim logNormal(-9, 0.01)
  \\
  \mu_{z_i} &\sim N(0, 1/I)
  \\
  \sigma_{z_i}^2 &\sim logNormal(-9, 0.01)
  \\
  \alpha_{i} &= \dfrac{\sigma_{z_i}^2}{\mu_{z_i}^2}
\end{align}$$


A few things that I do not understand are:  
  

1) the model for the data, i.e. $y \sim N(y; f(W, X), \sigma^2)$, is never addressed in _LUW 2017_.  I feel like I'm missing something fundamental here --- none of the literature appears to address this.  

  - the only place that a distributional model for the data is mentioned is in the ELBO, $\mathcal{L}(\phi) = E_{q_\phi(W|z) q_\phi(z)} \left[ \log \color{red}{p(y|W, z)} \right] - E_{q_\phi(z)} \left[ KL\left( q_\phi(W|z) || p(W|z) \right) \right]-KL \left(q_\phi(z) || p(z) \right)$.
  - it seems like this likelihood term simply gets replaced by MSE in the scalar response case (which would comport with an assumption that $y \sim N(f(W, X), \sigma^2)$, at least up to a multiplier), or cross-entropy when the data/response are pictures/labels (again, corresponding to an assumed multinomial dist'n).  This makes sense to me, BUT what doesn't make sense is that 1) the variance of the response is not modeled anywhere, and 2) the multiplier probably matters a little, no?


2) the priors for many parameters in the variational distribution are never mentioned in the paper / left implicit --- in particular, the priors placed on $\mu_{z_i}, \alpha_i, \mu_{ij}, \sigma_{ij}^2$.  

  - The code makes draws from Normal and log-Normal distributions _to initialize_ $\mu_{z_i}, \alpha_i, \mu_{ij}, \sigma_{ij}^2$, and these parameters are updated via the gradient of the ELBO.  However, their distributions are not part of the computation.  I suppose these parameters are only part of the _variational_ distribution, and so ... their distributions don't actually matter much?  We're just trying to optimize the ELBO using them?





## multiple layers

In a deep neural network (i.e. with more than 1 layer), we place the sparsity prior on every layer (investigation whether results are better if only placing the sparsity prior on the first layer may be warranted, however).  Other than requiring another index $l$ denoting the layer, the formulation is similar:


Let $l$ index the layer of the weight matrix $W_l$ (here, $l \in \{1, 2\}$), and $i$ index the row of $W_l$, and $j$ index the columns. The Bayesian model $p(w_{lij} | z_{l i \cdot} p(z_{l i \cdot})$ and variational distribution $q(W_l, z)$ are:

$$\begin{align} 
  p(z_{li \cdot}) &\propto |z_{li \cdot}|^{-1}
  \\
  p(w_{lij} | z_{l i \cdot}) &= N \left(w_{lij} | 0, z_{l i \cdot}^2 \right)
  \\
  q(W_l, z) &= \prod_{l, i} N \left(z_{li \cdot} | \mu_{z_{l i \cdot}}, \mu_{z_{l i \cdot} }^2 \alpha_{l i \cdot} \right) \prod_{l,i,j} N \left( w_{lij} | z_{l i \cdot} \mu_{lij}, z_{l i \cdot}^2 \sigma_{lij}^2 \right)
\end{align}$$


The upshot of all of this is just that we can perform the same operations on each layer, and the results are easily separable (multiply / sum each layer's results to get results for entire network)


# Derivations

## ELBO (general)

For notational simplicity, in the following I use $z$ to denote both the weights ($W$) and scale parameters ($z$), and $phi$ to denote all parameters governing the variational distribution ($\{\mu_{z_i}, \alpha_i, \sigma_{z_i}^2, \mu_{ij}, \sigma_{ij}^2\}$).

Deriving ELBO:

$$\begin{align}
  KL\left(q_\phi(z) || p(z|x)\right)
    &=
    \sum_z q_\phi(z) \log \left( \dfrac{ q_\phi(z) }{ p(z|x) } \right)
    \\
    &= 
    \sum_z q_\phi(z) \log \left( q_\phi(z) \cdot \dfrac{ p(x) }{ p(z,x) } \right)
    \\
    &=
    \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\}
      + \sum_z q_\phi(z) \left( \log p(x) \right)
      \\
    &= \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\} 
    + \log p(x) 
      && \text{since } p(x) \text{ is constant w.r.t. } z
    \\
  \text{Thus } \log p(x)
  &=
  KL\left(q_\phi(z) || p(z|x) \right) 
    - \sum_z \left\{ q_\phi(z) \left( \log q_\phi(z) - \log p(z, x) \right) \right\}
  \\
  &=
  KL\left(q_\phi(z) || p(z|x) \right) 
    \underbrace{- E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z, x) \right]}_{ELBO}
\end{align}$$


The second term is the ELBO $\mathcal{L}(\phi)$.  Since $\log p(x)$ is constant w.r.t. $q_\phi (z)$, maximizing the ELBO over $\phi$ minimizes the first term, the KL divergence between the variational distribution $q_\phi(z)$ and the posterior $p(z|x)$.


### Bayesian interpretation of ELBO

I mentioned a potentially interesting interpretation of the ELBO at our face-to-face meeting at the MoonGoat.  Just wanted to follow up on this, mostly as a side note:

We can rearrange the above expression for the ELBO such that:
$$\begin{align} 
  \mathcal{L}(\phi) 
  &= 
  - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z, y) \right]
  \\
  &=
  - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(y|z) - \log p(z) \right]
  \\
  &=
  E_{q_\phi(z)} \left[\log p(y|z) \right] - E_{q_\phi(z)} \left[ \log q_\phi(z) - \log p(z)\right]
  \\
  &=
  E_{q_\phi(z)} \left[\log p(y|z) \right] - KL \left(q_\phi(z) || p(z)\right)
\end{align}$$

The first term is the log-likelihood integrated over $z$, whereas the second term is the KL divergence between the variational distribution $q_\phi(z_$ and the prior $p(z)$.  I.e. the ELBO balances the likelihood of the data under the variational distribution against the "distance" the variational distribution must move from the prior.  As the variational distribution is our approximation to the posterior, we have a nice parallel here to the usual Bayesian idea of the data moving the posterior away from the prior.


A bit more rigorously: if use the interpretation from information theory of $\log \dfrac{1}{q_\phi(z)}$ as the "surprise" of drawing $z$ from the variational distribution $q_\phi(z)$, we can think of the terms as follows:

- $E_{q_\phi(z)} \left[\log p(y|z) \right]$ is the (negative) expected surprise of the data under our model when $z$ is sampled under $q_\phi(z)$;

- $KL \left(q_\phi(z) || p(z)\right)$ is the expectation, *when sampling $z$ from $q_\phi(z)$*, of how surprised we are by $z$ (given that we assume the variational distribution is true) minus how surprised we "should" have been by $z$ under the actual prior $p(z)$.  




## derivations for this specific model

Reverting to previous notation ($W, z$ instead of just $z$), our last expression for the ELBO is:

$$\begin{align} 
  \mathcal{L}(\phi) 
  &= 
    E_{q_\phi(W, z)} \left[\log p(y|W, z) \right] - 
    KL \left(q_\phi(W, z) || p(W, z)\right)
  \\
  &= 
    E_{q_\phi(W|z) q_\phi(z)} \left[\log p(y|W, z) \right] -  
    E_{q_\phi(W|z) q_\phi(z)}\left[ \log \dfrac{q_\phi(W,z)}{p(W,z)} \right]
  \\
  &=
    E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(y|W, z) \right] -  
    E_{q_\phi(z)} \left\{ E_{q_\phi(W|z)}\left[ \log \dfrac{q_\phi(W|z)}{p(W|z)} + \log \dfrac{q_\phi(z)}{p(z)} \right] \right\}
  \\
  &=
    E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(y|W, z) \right] -  
    E_{q_\phi(z)} \left[ KL\left( q_\phi(W|z) || p(W|z) \right) \right]- 
    KL \left(q_\phi(z) || p(z) \right)
  \\
\end{align}$$


### Log-likelihood

Since $y$ depends on $z$ only through $W$, $p(y|W,z) = p(y|W)$.  Thus the first term, $E_{q_\phi(W|z) q_\phi(z)} \left[ \log p(y|W, z) \right]$ is the expectation of the log-likelihood $\log p(y|W)$ under the variational distribution.

In the scalar response case, where (__I THINK___) $p(y | W) = N(y; f(W, X), \sigma^2)$, we have

$$\begin{align}
  \log p(y|W)
  &=
    -\frac{1}{2} \log (2 \pi \sigma^2)
    -\frac{1}{2 \sigma^2} \left( y - f(W, X) \right)^2
  \\
\end{align}$$


### KL for W|z

Using the distributions for the weights

- $q_\phi(W|z) = \prod_{i,j} N(w_{ij} | z_i \mu_{ij}, z_i^2 \sigma_{ij}^2)$
- $p(W|z) = \prod_{i,j} N(w_{ij} | 0, z_i^2)$

we have:

$$\begin{align} 
  KL\left( q_\phi(W|z) || p(W|z) \right)
  &=
    E_{q_\phi(W|z)}
    \left[
      \log \dfrac{q_\phi(W|z)}{p(W|z)}
    \right]
  \\
  &=
    E_{q_\phi(W|z)}
    \left[
      \sum_{i,j} -\frac{1}{2} 
      \left(
        \log(2\pi z_i^2 \sigma_{ij}^2)
        + \frac{(w_{ij} - z_i \mu_{ij})^2}{z_i^2 \sigma_{ij}^2}
        -\log (2\pi z_i^2)
        - \frac{w_{ij}^2}{z_i^2}
      \right)
    \right]
  \\
  &=
  \frac{1}{2} E_{q_\phi(W|z)}
    \left[
      \sum_{i,j}
      \left(
        \log \frac{2 \pi z_i^2 }{2 \pi z_i^2 \sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left( 
          (1-\sigma_{ij}^2)w_{ij}^2 - 2 z_i \mu_{ij} w_{ij} + z_i^2 \mu_{ij}^2
        \right)
      \right)
    \right]
  \\
  &=
  \frac{1}{2} 
      \sum_{i,j}
      \left(
        \log \frac{1}{\sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left \{
          (1-\sigma_{ij}^2) \left( E_{q_\phi(W|z)} \left[ w_{ij}^2 \right]\right) 
          - 2 z_i \mu_{ij} \left( E_{q_\phi(W|z)} [w_{ij}] \right) 
          + z_i^2 \mu_{ij}^2  
        \right\}
      \right)
  \\
  &=
  \frac{1}{2} 
      \sum_{i,j}
      \left(
        \log \frac{1}{\sigma_{ij}^2}
        - \frac{1}{z_i^2 \sigma_{ij}^2}
        \left\{ 
          (1-\sigma_{ij}^2) \left( z_i^2 \sigma_{ij}^2  + z_i^2 \mu_{ij}^2 \right) - 2 z_i^2 \mu_{ij}^2 + z_i^2 \mu_{ij}^2
        \right\}
      \right)
  \\
  &=
    \dfrac{1}{2}
    \sum_{i,j}
    \left(
      -\log \sigma_{ij}^2 + \sigma_{ij}^2 + \mu_{ij}^2 - 1
    \right)
\end{align}$$

### KL for z

An approximation to $KL \left(q_\phi(z) || p(z) \right)$ is provided by $Molchanov, Ashukha, Vetrov 2017$:

$$KL \left(q_\phi(z) || p(z) \right) \approx \sum_i (k_1 \sigma (k_2 + k_3 \log(\alpha_i))) - 0.5 m (-\log \alpha_i) - k_3)$$
where $k_1 = 0.63576, k_2 = 1.87320, k_3 = 1.48695$; $\sigma$ is the sigmoid function; and $m$ is the softplus function.  Importantly, note that this term depends only on the implied dropout rate $\alpha_i = \frac{\sigma_{z_i}^2}{\mu_{z_i}^2}$ --- which we identified as having a form similar to a Wald statistic.



### optimization

`r colorize("Issue described below does not seem to be impacting results --- as far as I can tell --- but may lead to problems down the line?")`

We rely on torch's automatic differentiation and gradient calculations to minimize the objective function.  In the scalar response case, LUW 2017 and my current simulations use the objective function $MSE + KL/n$ (letting $KL$ stand in for both KL terms).

__Issue__: $MSE + KL/n$ contains the same components as the ELBO, but is not quite an affine function of the ELBO (i.e. optimal parameter values $\phi$ for the ELBO and for $MSE + KL/n$ are `r colorize("not")` necessarily the same, __which seems problematic__).  In the ELBO, the terms estimated by the $MSE$ and $KL$ are scaled differently relative to each other, i.e. 

$$\widehat{\mathcal{L}}(\phi) = n \times \left( \frac{MSE}{2 \sigma^2}  + KL/n \right) + c$$
where $c$ is a constant not depending on $\phi$.




## other things to check:
- does the original code limit the range of $\alpha_i \in [0,1]$?
- the original code for _LUW 2017_"clips" variances ---- what is this about?



# Simulations

Simulation settings:

1) response $y$ is linearly related to covariates $X$.  100 nuisance variables, 4 "true" covariates.  n = 100.  

  a) `SLP` = single neuron model (i.e. basically a scale mixture of normals Spike-and-Slab on a linear regression model)  
  
  b) `MLP` = multiple neurons, multiple layers  
      - 2 hidden layers with 50 nodes and 25 nodes respectively


2) response $y$ is non-linearly related to covariates $X$.  96 nuisance variables, 4 "true" covariates.  Model used is identical to the one labeled `MLP` above.


## setting 1a)

_Linearly-related data, single neuron model_


__Data generation:__  

- Covariates $x_i$ for $i \in \{1, 2, ..., 104\}$ generated as independent draws from a standard normal, i.e. $x_i \sim N(0,1)$  

- $y = -0.5 x_1 + 1 x_2 - 2 x_3 + 4 x_4 + \epsilon$ where $\epsilon \sim N(0,1)$, i.e. only 4 "true" covariates ($x_1, x_2, x_3, x_4$) and 100 nuisance variables ($x_5, x_6, ..., x_{104}$)  

- number of observations $n$ = 100  

- error variance $\sigma^2$ = 1  


Model `SLP` has only 1 neuron, i.e. we are essentially fitting a scale mixture of normals regression model.

Results averaged from running methods on 100 simulated datasets




```{r SLP_RESULTS}
fpath <- here("Rcode", "results", "linear_sim_sig1.Rdata")
load(fpath)

true_coefs = c(-0.5, 1, -2, 4, rep(0, times = 100))
tru <- true_coefs != 0 

# slnj bare results

# threshold log_alphas at -3 (exp(-3) \approx 0.05)
# what is a reasonable dropout probability?  
log_alpha_threshold <- -3
txt <- paste0("log_alpha_threshold = ", log_alpha_threshold, 
              " (i.e. dropout rate of <",
              round(exp(log_alpha_threshold), 5), 
              " required for inclusion)")

slnj_log_alphas <- t(sapply(res, function(X) X$slnj$log_dropout_alphas))
slnj_include <- slnj_log_alphas < log_alpha_threshold

slnj_binary_err <- t(apply(
  slnj_include, 1, 
  function(X) binary_err(est = X, tru = tru)
))
slnj_binary_err_rates <- t(apply(
  slnj_include, 1,
  function(X) binary_err_rate(est = X, tru = tru)
))

# slnj_binary_err_rates
slnj_fwer <- mean(slnj_binary_err_rates[,1] != 0)

# other slnj metrics
slnj_metrics <- t(sapply(res, function(X) X$slnj$other_metrics))


# # # # # # # # # # # # # # # # # # # # # # # # #
## LASSO ----
# # # # # # # # # # # # # # # # # # # # # # # # #

lasso_coefs <- t(sapply(res, function(X) as.vector(X$lasso$coefs)[-1]))
lasso_include <- lasso_coefs != 0 
lasso_binary_err_rates <- t(apply(
  lasso_include, 1, 
  function(X) binary_err_rate(est = X, tru = tru)
))

lasso_fwer <- mean(lasso_binary_err_rates[,1] != 0)


# # # # # # # # # # # # # # # # # # # # # # # # #
## Spike-&-Slab ----
# # # # # # # # # # # # # # # # # # # # # # # # #

ss_post_inclusion_probs <- t(sapply(res, function(X) X$ss$coefs[, 5]))
ss_dropout_equiv <- 1 - ss_post_inclusion_probs
ss_include <- ss_dropout_equiv < exp(log_alpha_threshold)


ss_binary_err_rates <- t(apply(
  ss_include, 1, 
  function(X) binary_err_rate(est = X, tru = tru)
))

ss_fwer <- mean(ss_binary_err_rates[,1] != 0)

slnj_res <- colMeans(slnj_binary_err_rates)
ss_res<- colMeans(ss_binary_err_rates)
lasso_res<- colMeans(lasso_binary_err_rates)

slnj_tab <- cbind(
  rbind(slnj_res, ss_res, lasso_res),
  c(slnj_fwer, ss_fwer, lasso_fwer)
)

rownames(slnj_tab) <- c("SLP", "Spike-Slab", "LASSO")
colnames(slnj_tab) <- c("FP", "TP", "FN", "TN", "FWER")

mykable(tab = slnj_tab, cap = "SLP results")

```

Model inclusion decisded using the "implied dropout parameter" $\alpha_i$.  
`r colorize(txt)`



## setting 1b) 

Data generation identical to setting 1a).  

__Model__:  

`MLP` = multiple neurons, multiple layers  

  - 2 hidden layers with 50 nodes and 25 nodes respectively




## setting 2

non-linear relationship between response $y$ and $x_1, x_2, x_3, x_4$.

$$y = f_1(x_1) + f_2(x_2) + f_3(x_3) + f_4(x_4) + \epsilon$$

with $\epsilon \sim N(0,1)$.

```{r}
# plots of f1, f2, f3, f4
```



__Model__:

`MLP` = multiple neurons, multiple layers  
      - 2 hidden layers with 50 nodes and 25 nodes respectively


