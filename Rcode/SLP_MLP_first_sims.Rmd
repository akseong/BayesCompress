---
title: "First simulations"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)
library(ggplot2)


# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}



cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "BayesianLayers.R"))
source(here("Rcode", "sim_functions.R"))
```




# Background
The motivation for this model is to combine the variable selection properties of Bayesian spike-and-slab with the expressivity of a neural network.  We do this by placing a spike-and-slab prior on the _rows_ of a weight matrix $W$ representing one layer of a neural network.

Let $X$ be a $n \times I$ matrix representing $n$ observations of $I$ covariates, and $W$ be a $I \times J$ weight matrix.  We can then represent the computation performed by the first (input) layer of a neural network as $XW + b = H$, where $b$ is a vector of biases and $H$ is the pre-activation (that is, if $\sigma ()$ denotes the activation function, $\sigma(H) = \tilde{H}$ is the input for the next layer in the neural network).

If all of the weights $w_{ij}$ in row $i$ of $W$ are 0, then the covariate in column $i$ of $X$ do not contribute to $H$ and therefore also have no effect on the final prediction.  Thus by placing a spike-and-slab prior on the rows of $W$, we obtain variable selection.

We rely heavily here on the formulation and variational Bayes algorithm developed by Louizos, Ullrich, Welling 2017, who proposed using a group sparsity prior in order to reduce the size of the weight matrices in a neural network.  Their primary focus is on decreasing the size of the neural network while preserving quality of prediction --- the method leads to compression of the neural network since if an entire row or column of weights is 0, then the dimension of the weight matrix can be reduced.  Accordingly, their discussion of how to tune what they term "the dropout parameter" ($\alpha_i$), is based around desired compression rates rather than how to use it for principled variable selection.


## other notes:

__Note:__ Need to experiment to see if it's better to only do this for the input layer (as opposed to all layers, which is the current implementation)

- Ways to differentiate ourselves:  
    - a more rigorous interpretation of the dropout parameter $\alpha_i$ as the equivalent to a Wald statistic  
    - (Adam's paper) using normalizing flow to model non-local prior
    - some way to reconcile mean-field assumption of Variational Bayes with group sparsity?  
    - as LUW 2017 note, the log-uniform prior does require independence between parameters (i.e. grouping by row is not kosher), so the joint Bayesian model developed in LUW 2017 is not strictly accurate and is more of an approximation
    - obvious difference: provide empirical results on how good the method is at variable selection



## Model

We adopt a scale mixture of Normals as the prior for the weights:

$$\begin{align}
  z 
  & \sim p(z)
  \\
  w 
  & \sim N(0, z^2)
\end{align}$$

This is a fairly general formulation: letting $p(z)$ be Bernoulli, for example, leads to the discrete spike-and-slab.  



Following LUW 2017, we make two modifications:

1) we use the log-uniform prior for $z$, corresponding to a continuous relaxation of the discrete spike-and-slab;

2) rather than assigning each individual weight $w_{ij}$ its own $z_{ij}$, we constrain each row of weights $w_{i \cdot}$ to share a single $z_i$, thus enforcing group sparsity.

Under this formulation, the Bayesian model $p(w_{ij} | z_{i} p(z_{i})$ is:

$$\begin{align} 
  p(z_{i}) &\propto |z_{i}|^{-1}
  \\
  p(w_{ij} | z_{i}) &= N \left(w_{ij} | 0, z_{i}^2 \right)
\end{align}$$

In order to avoid MCMC on the potentially very large number of parameters, we adopt LUW 2017's variational Bayes algorithm (in turn adapted from Kingma, Salimans, Welling 2015 and Molchanov, Ashukha, Vetrov 2017).  Thus, we use the variational distribution

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, \mu_{z_{i} }^2 \alpha_{i} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$

where $\alpha_i$ is the dropout rate of a given row of weights.  Furthermore, following Molchanov, Ashukha, Vetrov 2017, we reparameterize $\mu_{z_{i} }^2 \alpha_{i} = \sigma_{z_i}^2$ because the approximate posterior over $z$ suffers from high variance gradients.  Thus we arrive at the the variational distribution:

$$\begin{align}
  q(W, z) 
  &=  \prod_{i} N \left(z_{i} | \mu_{z_{i}}, {\color{red}{\sigma_{z_i}^2}} \right)
      \prod_{i,j} N \left( w_{ij} | z_{i} \mu_{ij}, z_{i}^2 \sigma_{ij}^2 \right)
\end{align}$$


One important item to notice here: the (now implied) dropout parameter $\alpha_{i} = \dfrac{\sigma_{z_i}^2}{\mu_{z_i}^2}$ is the inverse of a Wald statistic.  _(Perhaps this being a Wald statistic is just so obvious it doesn't need mentioning, but I don't believe any of the the papers positing this as a dropout parameter say this explicitly)_  This might be used to provide guidance on how to threshold $\alpha_{i}$ appropriately for variable selection (LUW2017 only addresses thresholding $alpha_i$ in order to achieve a desired level of compression).





# check:
- does the original code limit the range of $alpha_i \in [0,1]$?
- the original code "clips" variances ---- what is this about?



















Let $l$ index the layer of the weight matrix $W_l$ (here, $l \in \{1, 2\}$), and $i$ index the row of $W_l$, and $j$ index the columns. The Bayesian model $p(w_{lij} | z_{l i \cdot} p(z_{l i \cdot})$ and variational distribution $q(W_l, z)$ are:

$$\begin{align} 
  p(z_{li \cdot}) &\propto |z_{li \cdot}|^{-1}
  \\
  p(w_{lij} | z_{l i \cdot}) &= N \left(w_{lij} | 0, z_{l i \cdot}^2 \right)
  \\
  q(W_l, z) &= \prod_{l, i} N \left(z_{li \cdot} | \mu_{z_{l i \cdot}}, \mu_{z_{l i \cdot} }^2 \alpha_{l i \cdot} \right) \prod_{l,i,j} N \left( w_{lij} | z_{l i \cdot} \mu_{lij}, z_{l i \cdot}^2 \sigma_{lij}^2 \right)
\end{align}$$











# Linear data

basic simulation setting:  

- p > n
- sparse truth (4 true covariates, 100 nuisance)
- p = 104, n = 100
