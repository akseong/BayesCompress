---
title: Low Level `torch` implementation of "Bayesian Compression" (LUW 2017)
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}
```

```{r}
library(torch)
source(here("Rcode", "BayesianLayers.R"))
```


# Setup

## generate data.  

```{r GENERATE_DATA}

n <- 100
# input dimensionality (# of features)
d_in <- 5
d_true <- 2   # the rest are nuisance vars
# output dimensionality (response dim)
datagen_out <- 3

x <- torch_randn(n, d_in)
w1_true <- torch_randn(size = c(d_in, datagen_out))
w1_true[(d_true+1):d_in, ] <- 0
b1_true <- torch_randint(-3, 5, size = c(1, datagen_out))$'repeat'(c(n, 1))
epsilon <- torch_randn(size = c(n, datagen_out))
# plot(density(as_array(epsilon)))
# qqnorm(y = as_array(epsilon))

# multivariate response
y_vector <- x$mm(w1_true) + b1_true + epsilon

# scalar response
y_scalar <- y_vector$sum(dim = 2) + epsilon[ , 1]
y <- y_scalar$unsqueeze(2)
```


- `r n` obs generated as basic linear regression $y = X\beta + \epsilon$, with $\epsilon_i \sim N(0,1)$
- Only first `r d_true` covariates in $X$ (out of `r d_in`) actually have an effect (rest are nuisance var.s).  
- multivariate response also generated (but not used for now)


```{r LMFIT}
df <- data.frame(
  "y" = as_array(y), 
  "X" = as_array(x)
)

lmfit <- lm(y ~ ., df)
summary(lmfit)
b1_true[1, ]$sum()
w1_true$sum(dim = 2)


binary_err_mat <- function(est, tru){
  # returns 4-row matrix of FP, TP, FN, TN
  FP <- est - tru == 1
  TP <- tru + est == 2
  FN <- tru - est == 1
  TN <- abs(tru) + abs(est) == 0   
  return(rbind(FP, TP, FN, TN))
}

binary_err_rate <- function(est, tru){
  # returns FP, TP, FN, TN rates
  rowSums(binary_err_mat(est, tru)) / length(tru)  
}

```

`lm()` does OK.  Tends to miss when coefficient < .5  ($\epsilon \sim N(0,1)$



## set up network

```{r}
d_hidden <- 12
d_out <- length(y[1,])

```

- 2 layers: input (`fc1`) and hidden (`fc2`).  
- Weights for `fc1` and `fc2` denoted $W_1, W_2$; intercepts/biases as $b_1, b_2$.
- Dimensions included in some subscripts just to keep an eye on things.

$$\begin{align}
\tt{fc1:} \quad 
  &  X_{(n \times d_{in})} W_{1(d_{in} \times d_{hidden})}  + b_1 
  && = h_1 \overset{relu}{\rightarrow} \tilde{h}_{1(n \times d_{hidden})} 
\\
\tt{fc2:} \quad 
  &  \tilde{h}_{1(n \times d_{hidden})} W_{2(d_{hidden} \times d_{out})}  + b_2 
  && = \hat{y}
\end{align}$$


## Log-Uniform Model
Let $l$ index the layer of the weight matrix $W_l$ (here, $l \in \{1, 2\}$), and $i$ index the row of $W_l$, and $j$ index the columns. 

$$\begin{align} 
  p(z_{li \cdot}) &\propto |z_{li \cdot}|^{-1}
  \\
  p(w_{lij} | z_{l i \cdot}) &= N \left(w_{lij} | 0, z_{l i \cdot}^2 \right)
  \\
  q(W_l, z) &= \prod_{l, i} N \left(z_{li \cdot} | \mu_{z_{l i \cdot}}, \mu_{z_{l i \cdot} }^2 \alpha_{l i \cdot} \right) \prod_{l,i,j} N \left( w_{lij} | z_{l i \cdot} \mu_{lij}, z_{l i \cdot}^2 \sigma_{lij}^2 \right)
\end{align}$$


### initialize parameters

```{r INITIALIZE_PARAMS}
reparameterize <- function(mu, logvar, cuda = FALSE, sampling = TRUE) {
  if (sampling) {
    std <- logvar$mul(0.5)$exp_()
    if (cuda) {
      eps <- torch_randn(std$size(), device = "gpu", requires_grad = TRUE)
    } else {
      eps <- torch_randn(std$size(), device = "cpu", requires_grad = TRUE)
    }
    return(mu$add(eps$mul(std)))
  } else {
    return(mu)
  }
}






# initialize params
# LUW: 
#  z_mu ~ N(1, .01)
#  w_mu ~ N(0, 1 / sqrt(length(w_mu)))
#  z_logvar ~ N(-9, .01)
#  w_logvar ~ N(-9, .01)
#  bias_logvar ~ N(-9, .01)

# need to initialize using reparam trick!
# otherwise gradients won't propagate 
# (initializing with torch_normal() --- a sampling 
#   operation --- does not store gradient info)
# so POPULATE torch_tensor() with torch_normal()
z1_mu <- torch_tensor(
  torch_normal(mean = 1, std = 0.1, size = d_in), 
  requires_grad = TRUE)
z2_mu <- torch_tensor(
  torch_normal(mean = 1, std = 0.1, size = d_hidden), 
  requires_grad = TRUE)
z1_logvar <- torch_tensor(
  torch_normal(mean = -9, std = 0.1, size = d_in),
  requires_grad = TRUE)
z2_logvar <- torch_tensor(
  torch_normal(mean = -9, std = 0.1, size = d_hidden), 
  requires_grad = TRUE)

w1_mu <- torch_tensor(
  torch_normal(mean = 0, std = 1/sqrt(d_in), size = c(d_hidden, d_in)),
  requires_grad = TRUE)
w2_mu <- torch_tensor(
   torch_normal(mean = 0, std = 1/sqrt(d_hidden), size = c(d_out, d_hidden)),
   requires_grad = TRUE)
w1_logvar <- torch_tensor(
   torch_normal(mean = -9, std = 0.1, size = c(d_hidden, d_in)),
   requires_grad = TRUE)
w2_logvar <- torch_tensor(
   torch_normal(mean = -9, std = 0.1, size = c(d_out, d_hidden)),
   requires_grad = TRUE)


b1_mu <- torch_zeros(d_hidden, requires_grad = TRUE)
b2_mu <- torch_zeros(d_out, requires_grad = TRUE)
b1_logvar <- torch_tensor(
   torch_normal(mean = -9, std = 0.1, size = d_hidden),
   requires_grad = TRUE)
b2_logvar <- torch_tensor(
  torch_normal(mean = -9, std = 0.1, size = d_out),
  requires_grad = TRUE)



# calculate log dropout rates
stability_param <- 1e-8

log_alpha1 = z1_logvar - torch_log(z1_mu$pow(2) + stability_param)
log_alpha2 = z2_logvar - torch_log(z2_mu$pow(2) + stability_param)
```





### forward function

- fill in weights, using the "reparameterization trick" (essentially, if $X ~ N(\mu, \sigma^2))$, can rewrite as $X = \mu + \sigma*Z$, where $Z \sim N(0,1)$.


```{r REPARAMETERIZE_AND_FORWARD}

# compute z1 & forward through fc1
# note:
#   z1 has shape n x d_in (because of `z1_mu$`repeat`(c(n, 1))
#   because 1) x * z1  

z1 <- reparameterize(
  mu = z1_mu$`repeat`(c(n, 1)), 
  logvar = z1_logvar$`repeat`(c(n, 1))
)

# h1 = fc1 activations (noisy; h1 ~ N(h1_mu, h1_var))
xz1 <- x*z1
h1_mu <- nnf_linear(input = xz1, weight = w1_mu, bias = b1_mu)
h1_var <- nnf_linear(input = xz1$pow(2), weight = w1_logvar$exp(), bias = b1_logvar$exp())

h1 <- reparameterize(mu = h1_mu, logvar = h1_var$log())
h1_tilde <- h1$relu()


# compute z2 & forward through fc2
z2 <- reparameterize(
  mu = z2_mu$`repeat`(c(n,1)), 
  logvar = z2_logvar$`repeat`(c(n,1))
)

# y_preds also noisy; y_pred ~ N(y_pred_mu, y_pred_var))
h1z2 <- h1_tilde * z2
y_pred_mu <- nnf_linear(input = h1z2, weight = w2_mu, bias = b2_mu)
y_pred_var <- nnf_linear(input = h1z2$pow(2), weight = w2_logvar$exp(), bias = b2_logvar$exp())

y_pred <- reparameterize(mu = y_pred_mu, logvar = y_pred_var$log())

# # calculate loss fcn (MSE for now)
# mse <- (y - y_pred)$pow(2)$sum()
# 
# mse$backward()
```




### Loss

```{r}

mse <- nnf_mse_loss(y_pred, y, reduction = "sum")

calc_kld <- function(log_alpha, weight_mu, weight_logvar, bias_mu, bias_logvar){
  k1 = 0.63576
  k2 = 1.87320
  k3 = 1.48695
  
  # KL(q(z) || p(z))
  KL_z <- -torch_sum(
    k1 * nnf_sigmoid(k2 + k3*log_alpha) - 
    0.5 * nnf_softplus(-log_alpha) - 
    k1
  )
  
  # KL(q(w|z) || p(w|z))
  KL_w_z <- torch_sum(
    -0.5 * weight_logvar + 
    0.5 * (weight_logvar$exp() + weight_mu$pow(2)) - 
    0.5
  )
  
  # KL for bias term
  KL_bias <- torch_sum(-0.5 * bias_logvar + 
    0.5 * (bias_logvar$exp() + bias_mu$pow(2)) - 0.5
  )
  
  # sum
  KL <- KL_z + KL_w_z + KL_bias
  return(KL)
}

KLD_1 <- calc_kld(
  log_alpha = log_alpha1, 
  weight_mu = w1_mu, 
  weight_logvar = w1_logvar, 
  bias_mu = b1_mu, 
  bias_logvar = b1_logvar
)

KLD_2 <- calc_kld(
  log_alpha = log_alpha2, 
  weight_mu = w2_mu, 
  weight_logvar = w2_logvar, 
  bias_mu = b2_mu, 
  bias_logvar = b2_logvar
)



loss <- mse + KLD_1 + KLD_2
loss$backward()

```


```{r CHECK_GRADIENTS, echo = FALSE, eval = FALSE}
# # leaf nodes (should have gradient, but no grad_fn)
# z1_mu$grad
# z1_mu$grad_fn
# z1_mu$grad
# z1_mu$grad_fn
# z2_mu$grad
# z2_mu$grad_fn
# z1_logvar$grad
# z1_logvar$grad_fn
# z2_logvar$grad
# z2_logvar$grad_fn
# w1_mu$grad
# w1_mu$grad_fn
# w2_mu$grad
# w2_mu$grad_fn
# w1_logvar$grad    # no grad - FIXED!
# w1_logvar$grad_fn
# w2_logvar$grad    # no grad - FIXED!
# w2_logvar$grad_fn
# b1_mu$grad
# b1_mu$grad_fn
# b2_mu$grad
# b2_mu$grad_fn
# b1_logvar$grad   # no grad - FIXED!
# b1_logvar$grad_fn
# b2_logvar$grad    # no grad - FIXED!
# b2_logvar$grad_fn
# 
# # should not be leaf tensors
# h1$grad
# h1$grad_fn
# h1_tilde$grad
# h1_tilde$grad_fn
# y_pred$grad
# y_pred$grad_fn
```



### backprop








# # # # # # # # # # # # 
##    training loop    
  
  
  # # # # # # # # # # # # 
  ##    Forward pass
  h <- x$mm(w1) + b1
  h_relu <- h$clamp(min = 0)
  y_pred <- h_relu$mm(w2) + b2
  
  
  # # # # # # # # # # # # 
  ##    compute loss
  KL <- 
  loss <- nnf_mse_loss(y_pred, y, reduction = "sum")
  KL
  
  if (t %% 10 == 0)
    cat("Epoch: ", t, "   Loss: ", loss$item(), "\n")
  
  # # # # # # # # # # # # 
  ##    backprop
  
  
  
  
  # # # # # # # # # # # # 
  ##    update weights    
  
  # Wrap in with_no_grad() because this is a part we DON'T 
  # want to record for automatic gradient computation
  with_no_grad({
    w1 <- w1$sub_(learning_rate * w1$grad)
    w2 <- w2$sub_(learning_rate * w2$grad)
    b1 <- b1$sub_(learning_rate * b1$grad)
    b2 <- b2$sub_(learning_rate * b2$grad)  
    
    # Zero gradients after every pass
    w1$grad$zero_()
    w2$grad$zero_()
    b1$grad$zero_()
    b2$grad$zero_()  
  })
  
  
  
  
  
  








