---
title: "LUW2017 Horseshoe testing"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}



cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

source(here("Rcode", "torch_horseshoe.R"))
source(here("Rcode", "sim_functions.R"))

```


```{r MISC_FUNCTIONS, echo = FALSE}
`%notin%` <- Negate(`%in%`)

vismat <- function(mat, cap = NULL, lims = NULL, leg = TRUE, na0 = TRUE, square){
  # outputs visualization of matrix with few unique values
  # colnames should be strings, values represented as factors
  # sci_not=TRUE puts legend in scientific notation
  require(ggplot2)
  require(scales)
  require(reshape2)
  
  melted <- melt(mat)
  melted$value <- ifelse(
    melted$value == 0 & na0,
    NA,
    melted$value
  )
  p <- ggplot(melted) + 
    geom_raster(aes(y = Var1, 
                    x = Var2, 
                    fill = value)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    scale_fill_viridis_c(limits = lims)
  
  if (is.numeric(melted$Var1)){
    p <- p + 
      scale_y_reverse()
  } else {
    p <- p + 
      scale_y_discrete(limits = rev(levels(melted$Var1)))
  }
  
  if (missing(square)) square <- nrow(mat) / ncol(mat) > .9 & nrow(mat) / ncol(mat) < 1.1
  if (square) p <- p + coord_fixed(1)
  
  if (!is.null(cap)) p <- p + labs(title=cap)
  
  if (!leg) p <- p + theme(legend.position = "none")
  
  return(p)
}

```



# Linear data (1 run)

basic simulation setting:  

- p > n
- sparse truth (4 true covariates, 100 nuisance)
- p = 104, n = 100


- 100 obs generated as basic linear regression $y = X\beta + \epsilon$, with $\epsilon_i \sim N(0,1)$
- Only first 4 covariates in $X$ (out of 104) actually have an effect (rest are nuisance var.s).  
- multivariate response also generated (but not used for now)


<!-- ## Normal-Jeffreys SLP fit -->

<!-- Let $l$ index the layer of the weight matrix $W_l$ (here, $l \in \{1, 2\}$), and $i$ index the row of $W_l$, and $j$ index the columns. The Bayesian model $p(w_{lij} | z_{l i \cdot} p(z_{l i \cdot})$ and variational distribution $q(W_l, z)$ are: -->

<!-- $$\begin{align}  -->
<!--   p(z_{li \cdot}) &\propto |z_{li \cdot}|^{-1} -->
<!--   \\ -->
<!--   p(w_{lij} | z_{l i \cdot}) &= N \left(w_{lij} | 0, z_{l i \cdot}^2 \right) -->
<!--   \\ -->
<!--   q(W_l, z) &= \prod_{l, i} N \left(z_{li \cdot} | \mu_{z_{l i \cdot}}, \mu_{z_{l i \cdot} }^2 \alpha_{l i \cdot} \right) \prod_{l,i,j} N \left( w_{lij} | z_{l i \cdot} \mu_{lij}, z_{l i \cdot}^2 \sigma_{lij}^2 \right) -->
<!-- \end{align}$$ -->

<!-- Since the data here is linear, we apply the variational Bayes model to a single neuron / single layer (i.e. regular multivariate linear regression model). -->


## Horseshoe model







```{r DEFINE_NET}
library(torch)
d_in <- 104
SLHS <- nn_module(
  "SLHS",
  initialize = function() {
    self$fc1 = torch_hs(    
      in_features = d_in, 
      out_features = 1,
      use_cuda = FALSE,
      tau = 1,
      init_weight = NULL,
      init_bias = NULL,
      init_alpha = 0.9,
      clip_var = NULL
    )
  },
  
  forward = function(x) {
    x %>% 
      self$fc1() 
  },

  get_model_kld = function(){
    kld = self$fc1$get_kl()
    return(kld)
  }
)

```



```{r TRAIN_SLNJ, eval=params$retrain}
# ## This chunk only runs if params$retrain == TRUE
# ## generate linear data
# n_obs <- 100
# true_coefs = c(-0.5, 1, -2, 4, rep(0, times = 100))
# set.seed(params$seed)
# torch_manual_seed(params$seed)
# slhs_net <- SLHS()
# err_sig <- 1
# lin_simdat <- sim_linear_data(
#   n = n_obs,
#   true_coefs = true_coefs,
#   err_sigma = err_sig
# )
# 
# train_epochs <- 200000
# 
# convergence_crit <- 1e-7
# 
# slhs_net(lin_simdat$x)
# optim_slhs <- optim_adam(slhs_net$parameters)
# loss_diff <- 1
# loss <- torch_zeros(1)
# 
# verbose <- TRUE
# epoch <- 1
# while (epoch < train_epochs & abs(loss_diff) > convergence_crit){
#   prev_loss <- loss
#   epoch <- epoch + 1
# 
#   y_pred <- slhs_net(lin_simdat$x)
# 
#   mse <- nnf_mse_loss(y_pred, lin_simdat$y)
#   kl <- slhs_net$get_model_kld() / lin_simdat$n
# 
#   loss <- mse + kl
#   loss_diff <- (loss - prev_loss)$item()
#   
#   
#   if(epoch %% 1000 == 0 & verbose){
#     cat(
#       "Epoch: ", epoch,
#       "MSE + KL/n = ", mse$item(), " + ", kl$item(),
#       " = ", loss$item(),
#       "\n"
#     )
#     # Eztilde <- slhs_net$fc1$get_Eztilde_i()
#     # Vztilde <- slhs_net$fc1$get_Vztilde_i()
#     # cat("E[ztilde_i]: ", round(as_array(Eztilde), 4), "\n \n")
#     # cat("V[ztilde_i]: ", round(as_array(Vztilde), 4), "\n \n \n")
#     # cat(round(as_array(slnj_net$fc1$z_mu), 4), "\n")
#     w_mu <- slhs_net$fc1$compute_posterior_param()$post_weight_mu
#     cat("E[W|y]: ", round(as_array(w_mu), 2), "\n")
#     dropout_alphas <- slhs_net$fc1$get_dropout_rates()
#     cat("alphas: ", round(as_array(dropout_alphas), 2), "\n \n")
#   }
# 
#   # zero out previous gradients
#   optim_slhs$zero_grad()
#   # backprop
#   loss$backward()
#   # update weights
#   optim_slhs$step()
# }
# 
# 
# 
# contents <- list(
#   "n_obs" = n_obs,
#   "true_coefs" = true_coefs,
#   "seed" = params$seed,
#   "err_sig" = err_sig
# )
# 
# save(contents, file = here::here("Rcode", "results", "hshoe_linfunc.Rdata"))
# torch_save(slhs_net, path = here::here("Rcode", "results", "hshoe_linfunc.pt"))

```




```{r LOAD}
load(file = here::here("Rcode", "results", "hshoe_linfunc.Rdata"))
slhs_net <- torch_load(path = here::here("Rcode", "results", "hshoe_linfunc.pt"))

# recreate lin_simdat from `contents` list
set.seed(contents$params$seed)
err_sig <- contents$err_sig
n_obs <- contents$n_obs
true_coefs <- contents$true_coefs

lin_simdat <- sim_linear_data(
  n = n_obs,
  true_coefs = true_coefs,
  err_sigma = err_sig
)

# posterior weight parameters
post_wmu <- slhs_net$fc1$compute_posterior_param()$post_weight_mu
post_wvar <- slhs_net$fc1$compute_posterior_param()$post_weight_var

post_wmu_mat <- matrix(as_array(post_wmu))
post_wvar_mat <- matrix(as_array(post_wvar))

mykable(
  matrix(round(post_wmu_mat, 3), nrow = 8, byrow = TRUE),
  cap = "posterior mean weights (in 8 rows)"
)

mykable(
  matrix(true_coefs, nrow = 8, byrow = TRUE),
  cap = "true regression coefficients (in 8 rows)"
)

post_params_waldish <- as_array(post_wmu^2 / post_wvar)
post_params_inclusion <- post_params_waldish > 2

mykable(
  matrix(round(post_params_waldish, 3), nrow = 8, byrow = TRUE),
  cap = "Wald-like posterior weights' (mean^2 / variance)"
)

mykable(
  matrix(post_params_inclusion, nrow = 8, byrow = TRUE),
  cap = "inclusion based on posterior weights' (mean^2 / variance) > 2"
)

```


- posterior weights look good  



### dropout rate:

Under the variational distribution, 

$$\begin{aligned}
\tilde{z_i} 
  &= \sqrt{ \tilde{\alpha_i} \tilde{\beta_i} }
  \\
\tilde{\beta_i} 
  & \sim Log-Normal \left( \mu_{\tilde{\beta_i}}, \sigma^2_\tilde{{\beta_i}}  \right)
  \\
\tilde{\alpha} 
  & \sim Log-Normal \left( \mu_{\tilde{\alpha}}, \sigma^2_{\tilde{\alpha}}  \right)
  \\ 
\end{aligned}$$

If $Y \sim N(\mu_y, \sigma^2_Y)$, then $X = \e^Y \sim Log-Normal(\mu_Y, \sigma^2_Y)$, and the mean and variance of the log-normal-distributed RV $X$ are given by:  
- $E[X] = \exp \left\{  {\mu + \frac{\sigma^2}{2}} \right\}$  
- $Var[X] = \left( \exp\{{\sigma^2_Y}\} - 1  \right) \exp \left\{2 \mu_Y + \sigma^2_Y \right\}$  

Then, for the variational dropout rate, we use the variational parameters of the local shrinkage parameter $\tilde z$, where

$$\tilde z = \sqrt{\tilde{\alpha} \tilde{\beta}} \sim Log-Normal \left( \dfrac{1}{2} (\mu_{\tilde{\alpha}} + \mu_{\tilde{\beta}}),  \dfrac{1}{4} (\sigma^2_{\tilde{\alpha}} + \sigma^2_{\tilde{\beta}})  \right)$$ 
Then, for the local dropout rate $\alpha_i$, we get:

$$\begin{aligned}
\alpha & = 
  \dfrac{Var[\tilde{z}]} { \left(  E[\tilde{z}]  \right)^2 }\\
  & = 
  \dfrac{    
    \left(  exp\{\sigma^2_{\tilde{z}}\} - 1  \right)  
    \exp \left\{2 \mu_{\tilde{z}} + \sigma^2_{\tilde{z}} \right\}
  }{
    \left(\exp \left\{  {\mu + \frac{\sigma^2}{2}} \right\} \right)^2
  }
  \\
  & = 
  \dfrac{    
    \left(  exp\{\sigma^2_{\tilde{z}}\} - 1  \right)  
    \exp \left\{2 \mu_{\tilde{z}} + \sigma^2_{\tilde{z}} \right\}
  }{
    \exp \left\{ 2 \mu_{\tilde{z}} + \sigma^2_{\tilde{z}} \right\} 
  }
  \\
  & = exp\{\sigma^2_{\tilde{z}}\} - 1 
  \\
  & = 
   exp \left\{
  \dfrac{1}{4} \left(  \sigma^2_{\tilde{\alpha}} + \sigma^2_{\tilde{\beta}}  \right)  \right \}  - 1
\end{aligned}$$

Interestingly, the local shrinkage parameter controls the variational dropout rate only via its variance, which is a nice result since the _model_ distribution $p(\tilde{z})$ is half-Cauchy.


Setting the inclusion threshold _ad hoc_ to requiring a dropout rate < 0.05 for inclusion, we get a binary error rate of:

```{r DROPOUT, results="hold"}
# alphas <- log_dropout(hslayer = slhs_net$fc1)
# slhs_net$fc1$get_log_dropout_rates()

alphas <- as_array(slhs_net$fc1$get_dropout_rates())
hskeep_05 <- alphas < .05
hskeep_50 <- alphas < .5

cat("binary error as percentages (sums to 1): \n")
binary_err(est = hskeep_05, tru = true_coefs != 0)

cat("binary error rates (FP + TN = 1, FN + TP = 1): \n")
binary_err_rate(est = hskeep_05, tru = true_coefs != 0)

```

without any evaluative training regimes besides the number of epochs.


If we were to use the median model (dropout rate < 50% required for inclusion):

```{r}
cat("MEDIAN MODEL binary error rates (FP + TN = 1, FN + TP = 1): \n")
binary_err_rate(est = hskeep_50, tru = true_coefs != 0)
```





## lm fit

```{r LM_PERFORMANCE}
calc_lm_stats <- function(lm_fit, true_coefs, alpha = 0.05){
  beta_hat <- summary(lm_fit)$coef[-1, 1]
  binary_err <- binary_err_rate(
    est = summary(lm_fit)$coef[-1, 4] < alpha,
    tru = true_coefs != 0)
  fit_mse <- mean(lm_fit$residuals^2)
  coef_mse <- mean((beta_hat - true_coefs)^2)
  list(
      "binary_err" = binary_err,
      "fit_mse" = fit_mse,
      "coef_mse" = coef_mse
    )
}

get_lm_stats <- function(simdat, include = NULL, alpha = 0.05){
  lm_df <- data.frame(
    "y" = as_array(simdat$y),
    "x" = as_array(simdat$x)
  )
  if (simdat$d_in > simdat$n){
    last_cov_ind <- ceiling(simdat$n/2)
    # ensure that "include" variables are included    
    if (!is.null(include) | length(include) > simdat$d_true){
      add_inds <- include[include %notin% 1:last_cov_ind]
      available_cols <- setdiff((simdat$d_true + 1):last_cov_ind, include)
      lm_df[, (available_cols[1:length(add_inds)])] <- lm_df[, add_inds + 1]
    } 
  lm_df <- lm_df[, 1:(last_cov_ind+1)]
    
  }

  lm_fit <- lm(y ~ ., lm_df)
  if (length(simdat$true_coefs) >= n_obs){
    warning("p >= n; (p - n) + floor(n/2) spurious covariates eliminated to accomodate lm")
    calc_lm_stats(
      lm_fit = lm_fit,
      true_coefs = simdat$true_coefs[1:ceiling(simdat$n/2)],
      alpha = alpha
    )
  } else {
    calc_lm_stats(lm_fit = lm_fit, true_coefs = simdat$true_coefs, alpha = alpha)
  }
}

get_lm_stats(simdat = lin_simdat, include = which(hskeep_50))
```

- note that these results are after giving `lm()` quite a large advantage (keeping only half of the nuisance variables)



# Functional Data setting


## checking data generation

```{r}
fcn1 <- function(x) exp(x/2)
fcn2 <- function(x) cos(pi*x) + sin(pi/1.2*x) - x
fcn3 <- function(x) abs(x)^(1.5)
# fcn4 <- function(x) -x^2 / 2 -3
fcn4 <- function(x) - log(abs(x))
flist = list(fcn1, fcn2, fcn3, fcn4)

xshow <- seq(-3, 3, length.out = 100)
yshow <- sapply(flist, function(fcn) fcn(xshow))
df <- data.frame(
  "f1" = yshow[, 1],
  "f2" = yshow[, 2],
  "f3" = yshow[, 3],
  "f4" = yshow[, 4],
  "x"  = xshow
)
df %>% 
  pivot_longer(cols = -x, names_to = "fcn") %>% 
  ggplot(aes(y = value, x = x, color = fcn)) +
  geom_line() + 
  labs(title = "functions used to create data")



x <- torch_randn(500, d_in)
  y <- rep(0, 500)
  for(j in 1:length(flist)){
    y <- y + flist[[j]](x[,j])
  }

# checking
td <- sim_func_data(flist = flist, err_sigma = 0)

y <- as_array(td$y)
x1 <- as_array(td$x[,1])
x2 <- as_array(td$x[, 2])
x3 <- as_array(td$x[, 3])
x4 <- as_array(td$x[, 4])

plot(x1, y, main = "y vs x1. \n f1 = exp(x/2) \n y = f1(x1) + f2(x2) + f3(x3) + f4(x4) + eps")
plot(x2, y, main = "y vs x2. \n f2 = cos(pi*x2) \n y = f1(x1) + f2(x2) + f3(x3) + f4(x4) + eps")
plot(x3, y, main = "y vs x3. \n f3 = abs(x3)^(1.5) \n y = f1(x1) + f2(x2) + f3(x3) + f4(x4) + eps")
plot(x4, y, main = "y vs x4. \n f4 = cos(pi*x4) + sin(pi/1.2*x4) - x4 \n y = f1(x1) + f2(x2) + f3(x3) + f4(x4) + eps")

```


```{r DOUBLECHECK, eval = F, echo = F}
y1 <- as_array(td$y$squeeze(2) - fcn2(td$x[,2]) - fcn3(td$x[,3])- fcn4(td$x[,4]))
y2 <- as_array(td$y$squeeze(2) - fcn1(td$x[,1]) - fcn3(td$x[,3])- fcn4(td$x[,4]))
y3 <- as_array(td$y$squeeze(2) - fcn2(td$x[,2]) - fcn1(td$x[,1])- fcn4(td$x[,4]))
y4 <- as_array(td$y$squeeze(2) - fcn2(td$x[,2]) - fcn3(td$x[,3])- fcn1(td$x[,1]))

plot(x1, y1, main = "y1 vs x1. \n y1 = y - [f2(x2) + f3(x3) + f4(x4)]")
plot(x2, y2, main = "y2 vs x2. \n y2 = y - [f1(x1) + f3(x3) + f4(x4)]")
plot(x3, y3, main = "y3 vs x3.")
plot(x4, y4, main = "y4 vs x4.")

```




## gen data, define NNet

Generate data with 4 covariates that are non-linearly related to the outcome (as above), and 100 nuisance covariates.  See if we can pick them out.


```{r}
n_obs <- 100000
d_in <- 104
set.seed(params$seed)
torch_manual_seed(params$seed)
fdat <- sim_func_data(
  n = n_obs,
  d_in = d_in,
  flist = flist,
  err_sigma = 1)

# fdat <- sim_linear_data(
#   n_obs = n_obs,
#   d_in = d_in,
#   d_true = 4,
#   err_sigma = 1,
#   true_coefs = true_coefs[1:d_in]
# )



d_hidden1 <- 16
d_hidden2 <- 8
d_out <- 1

MLHS <- nn_module(
  "Multi-layer Horseshoe",

  initialize = function() {

    self$fc1 = torch_hs(    
      in_features = d_in, 
      out_features = d_hidden1,
      use_cuda = FALSE,
      tau = 1,
      init_weight = NULL,
      init_bias = NULL,
      init_alpha = 0.9,
      clip_var = TRUE
    )

    self$fc2 = torch_hs(
      in_features = d_hidden1,
      out_features = d_hidden2,
      use_cuda = FALSE,
      tau = 1,
      init_weight = NULL,
      init_bias = NULL,
      init_alpha = 0.9,
      clip_var = TRUE
    )

    self$fc3 = torch_hs(
      in_features = d_hidden2,
      out_features = d_out,
      use_cuda = FALSE,
      tau = 1,
      init_weight = NULL,
      init_bias = NULL,
      init_alpha = 0.9,
      clip_var = TRUE
    )

  },

  forward = function(x) {
    x %>%
      self$fc1() %>%
      nnf_relu() %>%
      self$fc2() %>%
      nnf_relu() %>%
      self$fc3()
  },



  get_model_kld = function(){
    kl1 = self$fc1$get_kl()
    kl2 = self$fc2$get_kl()
    kl3 = self$fc3$get_kl()
    kld = kl1 + kl2 + kl3
    return(kld)
  }
)


```


## train

```{r TRAIN_MULTILAYER_HS, eval = params$retrain}

fcn_model <- MLHS()
x <- fdat$x
y <- fdat$y



# setup plotting while training
curvmat <- cbind(
  c(xshow, rep(0, 100*3)),
  c(rep(0, 100), xshow, rep(0, 100*2)),
  c(rep(0, 100*2), xshow, rep(0, 100)),
  c(rep(0, 100*3), xshow)
)
mat0 <- matrix(0, nrow = 400, ncol = 100)
x_plot <- torch_tensor(cbind(curvmat, mat0))
y_plot <- fcn_model(x_plot)
plotmat <- cbind(as_array(y_plot), curvmat)
colnames(plotmat) <- c("y", "x1", "x2", "x3", "x4")
plotdf <- as.data.frame(plotmat)





# training params / results
optim <- optim_adam(fcn_model$parameters)

train_epochs <- 2000000
loss_mat <- array(data = NA, dim = c(train_epochs/100, 4))
colnames(loss_mat) <- c("epoch", "mse", "kl", "loss")

for (epoch in 1:train_epochs){

  y_pred <- fcn_model(x)
  mse <- nnf_mse_loss(y_pred, y)
  kl <- fcn_model$get_model_kld() / fdat$n
  loss <- mse + kl

  if(epoch %% 10000 == 0){
    cat(
      "Epoch: ", epoch,
      "MSE + KL/n = ", mse$item(), " + ", kl$item(),
      " = ", loss$item(),
      "\n"
    )
    dropout_alphas <- fcn_model$fc1$get_dropout_rates()
    cat(round(as_array(dropout_alphas), 4), "\n")
    loss_mat[epoch/100, ] = c(epoch, mse$item(), kl$item(), loss$item())
    plotdf$y <- as_array(fcn_model(x_plot))
    plotdf %>% 
      gather(key = "fcn", value = "x", -y) %>% 
      ggplot(aes(y = y, x = x, color = fcn)) + 
      geom_line()
  }

  # zero out previous gradients
  optim$zero_grad()
  # backprop
  loss$backward()
  # update weights
  optim$step()
}

contents <- list(
  "n_obs" = n_obs,
  "true_coefs" = true_coefs,
  "seed" = params$seed,
  "err_sig" = err_sig,
  "d_in" = d_in,
  "d_hidden1" = d_hidden1,
  "d_hidden2" = d_hidden2,
  "d_out" = d_out,
  "plot_df" = plot_df
)

save(contents, file = here::here("Rcode", "results", "hshoe_funcdata_n100k.Rdata"))
torch_save(fcn_model, path = here::here("Rcode", "results", "hshoe_funcdata_n100k.pt"))

load(here::here("Rcode", "results", "hshoe_funcdata_n50k.Rdata"))
fcn_model <- torch_load(here::here("Rcode", "results", "hshoe_funcdata_n50k.pt"))


plotdf$y <- as_array(fcn_model(x_plot))
plotdf %>% 
  gather(key = "fcn", value = "x", -y) %>% 
  ggplot(aes(y = y, x = x, color = fcn)) + 
  geom_line()


df %>% 
  pivot_longer(cols = -x, names_to = "fcn") %>% 
  ggplot(aes(y = value, x = x, color = fcn)) +
  geom_line() + 
  labs(title = "functions used to create data")




# investigating --- alphas not moving, all staying ~ 0.05
as_array(fcn_model$fc1$get_dropout_rates())
as_array(fcn_model$fc2$get_dropout_rates())
as_array(fcn_model$fc3$get_dropout_rates())

fcn_model$fc1$compute_posterior_param()







fcn_model_keeps <- fcn_model$fc1$get_dropout_rates() < 0.05
true_gam <- c(
  rep(T, times = fdat$d_true),
  rep(F, times = fdat$d_in - fdat$d_true)
)
fcn_model_bin_err <- binary_err_rate(est = as_array(fcn_model_keeps), tru =  true_gam)

cat("binary error: \n")
fcn_model_bin_err


post_w1 <- fcn_model$fc1$compute_posterior_param()

post_w1$post_weight_mu


fcn_model$fc1$atilde_logvar
fcn_model$fc1$atilde_mu
fcn_model$fc1$btilde_logvar
fcn_model$fc1$btilde_mu


fcn_model$fc1$sa_logvar
fcn_model$fc1$sa_mu
fcn_model$fc1$sb_logvar
fcn_model$fc1$sb_mu
fcn_model$fc1$weight_logvar
fcn_model$fc1$weight_mu

fcn_model$fc1$bias_logvar
fcn_model$fc1$bias_mu




# check if mult done correctly --- check means and logvar products
# problem seems to be when adding extra layers




param_vis <- function(val_range = c(0,1)){
  
  
  
  
}

```



<!-- - takes a looong time to start to get to larger alpha (dropout) values -->
<!-- - maybe should initialize with dropout ~ 1/2 -->
<!-- - yup!  helps. -->
<!-- - test with more nuisance vars -->


<!-- # Functional Data setting with group diffs -->

<!-- - how to adapt to Saez data setting? -->
<!-- - how to decide on model complexity? -->
<!--    - 1 hidden layer, 32 nodes? -->
<!--    - better to start off overparameterizing since algorithm prunes itself? -->

<!-- - use Saez data and same basic simulation procedures (orthog design matrix, cut basis, etc.), and add nuisance vars? -->

<!-- - is there a way to avoid projecting time variable onto 0-deg spline basis?  i.e. work with time variable directly to identify when time variable is important? -->

<!-- ```{r} -->
<!-- # BoomSpikeSlab doesn't pick up on these; only on covariate 1 (which has somewhat linear relationship) -->
<!-- library(BoomSpikeSlab) -->
<!-- X <- cbind(1, as_array(x)) -->
<!-- prior = IndependentSpikeSlabPrior(X, as_array(y),  -->
<!--                                   expected.model.size = 1, -->
<!--                                   prior.beta.sd = rep(1, ncol(X)))  -->
<!-- lm.ss = lm.spike(as_array(y) ~ as_array(x), niter = 1000, prior = prior) -->
<!-- summary(lm.ss)$coef -->
<!-- ``` -->

```{r}


lnplot <- function(mu, sig){
  # plots density of log-normal dist'n
  grid <- 1:400 / 100
  dens <- dnorm(log(grid), mean = mu, sd = sig)
  plot(dens ~ grid)
}

```

