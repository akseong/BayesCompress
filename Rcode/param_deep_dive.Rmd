---
title: "param deep dive"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---
  
<style>
  .panel-tabs{
    opacity: 1;
    background-color: #2fa4e7;
      color: #ffffff;
      position: sticky;
    top: 0
  }
</style>

```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# model packages
library(torch)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)

# PANELSET
xaringanExtra::use_panelset()
xaringanExtra::style_panelset_tabs(font_family = "inherit")



# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

# notin
`%notin%` <- Negate(`%in%`)

source(here("Rcode", "torch_horseshoe_klcorrected.R"))
source(here("Rcode", "sim_functions.R"))
source(here("Rcode", "analysis_fcns.R"))
```






```{r LOAD_10kobs_xnorm}
# load ----
img_suffixes <- c(paste0("_e", 1:9, "e_+05.png"), "_e1e_+06.png")

stem_pvtau1 <- here::here("sims", "results", "hshoe_smooth_pvtau_1721632_12500obs_")
stem_pvtau2 <- here::here("sims", "results", "hshoe_smooth_pvtau_2721632_12500obs_")
seed_pvtau2 <- c(19709, 809872, 264744, 498729, 336130, 263808, 877164, 218489, 234821, 616240)
seed_pvtau1 <- c(561114, 453639, 663173, 108703, 165780)

resfile_pvtau1 <- paste0(stem_pvtau1, seed_pvtau1, ".RData")
modfile_pvtau1 <- paste0(stem_pvtau1, seed_pvtau1, ".pt")

resfile_pvtau2 <- paste0(stem_pvtau2, seed_pvtau2, ".RData")
modfile_pvtau2 <- paste0(stem_pvtau2, seed_pvtau2, ".pt")

res_fnames <- c(resfile_pvtau1, resfile_pvtau2)
mod_fnames <- c(modfile_pvtau1, modfile_pvtau2)
seeds <- c(seed_pvtau1, seed_pvtau2)
```



# 10k obs


```{r CHECK_FITS}
# check model fits across sims
load(res_fnames[1])
epoch <- 1:nrow(sim_res$loss_mat) * sim_res$sim_params$report_every
lmat <- cbind(1, epoch, sim_res$loss_mat)
colnames(lmat)[1] <- "sim"

for (sim_ind in 2:length(seeds)){
  load(res_fnames[sim_ind])
  epoch <- 1:length(sim_res$loss_mat) * sim_res$sim_params$report_every
  curr_lmat <- cbind(sim_ind, epoch, sim_res$loss_mat)
  lmat <- rbind(lmat, curr_lmat)
}

colnames(lmat)
lmat %>% 
  as_data_frame() %>% 
  mutate(sim = factor(sim)) %>% 
  ggplot(aes(y = kl, x = epoch, color = sim)) +
  geom_line() + 
  ylim(c(0.1, 0.2)) + 
  labs(title = "10k obs: KLs across epochs & sims")

lmat %>% 
  as_data_frame() %>% 
  mutate(sim = factor(sim)) %>% 
  pivot_longer(cols = -c("epoch", "sim")) %>% 
  filter(name != "kl") %>% 
  ggplot(aes(x = epoch, color = sim, linetype = name)) +
  geom_line(aes(y = value)) + 
  ylim(c(0.9, 1.4)) + 
  labs(title = "10k obs: test/train MSE across epochs & sims")
```


__Note:__ sims 1-5 trained for 1M epochs; 6-15 for 500k epochs.

- KL divergences continually decrease;

- neigther train (dotted) nor test MSE seem to benefit significantly from longer training;

  - however, slight upward trend in train MSE towards 1 (true value of $\sigma^2_\epsilon$) indicates further training actually alleviates overfitting somewhat.





## weight params

- looking at sim 11 (known good; can compare to known bad 15)


<!-- - no sqrt:     -->
<!--   - bad: 15     -->
<!--   - OK: 2, 3, 4, 5, 7, 10, 12, 13     -->
<!--   - good: 6     -->
<!--   - excellent: 8, 9, 11, 14     -->
<!-- - sqrted:     -->
<!--   - bad: 15     -->
<!--   - OK: 2, 4, 8, 14     -->
<!--   - excellent: 3, 5, 6, 7, 9, 10,11, 12, 13     -->



```{r weight_params}
sim_ind <- 11
load(res_fnames[sim_ind])
nn_mod <- torch_load(mod_fnames[sim_ind])

varmat_pltfcn(
  sim_res$kappa_local_mat,
  y_name = "local kappa",
  burn = 0
)$all_vars_plt

varmat_pltfcn(
  sim_res$kappa_mat,
  y_name = "kappa",
  burn = 0
)$all_vars_plt

k1 <- get_kappas(nn_mod$fc1)
k2 <- get_kappas(nn_mod$fc2)
wtil_params <- get_wtil_params(nn_mod$fc1)
w_mu <- wtil_params$wtil_mu
w_lvar <- wtil_params$wtil_lvar
w_var <- exp(w_lvar)

vismat(exp(w_lvar)) + 
  labs(title = "weight matrix variances")
vismat(abs(w_mu)) + 
  labs(title = "weight matrix means (abs val)")

k1_weighted_mu <- sweep(abs(w_mu), MARGIN = 2, STATS = (1 - k1), FUN = "*")
k12_weighted_mu <- sweep(abs(k1_weighted_mu), MARGIN = 1, STATS = (1 - k2), FUN = "*")
vismat(k1_weighted_mu) + 
  labs(title = "weight matrix means (abs val) weighted by layer 1 kappas")

vismat(k12_weighted_mu) + 
  labs(title = "weight matrix means (abs val) weighted by layer 1 AND 2 kappas")


k1_weighted_var <- sweep(w_var, MARGIN = 2, STATS = k1, FUN = "*")
k2_weighted_var <- sweep(w_var, MARGIN = 1, STATS = k2, FUN = "*")
k12_weighted_var <- sweep(k1_weighted_var, MARGIN = 1, STATS = k2, FUN = "*")
vismat(k1_weighted_var) + 
  labs(title = "weight matrix variances weighted by layer 1 kappas")
vismat(k12_weighted_var) + 
  labs(title = "weight matrix variances weighted by layer 1 AND 2 kappas")
```


### inspecting $\sigma_{\tilde{w}}$ across training epochs {.panelset}

layer 2 kappas (neuron dropout):

```{r}
k2 <- get_kappas(nn_mod$fc2)
k2
```



```{r results = "asis"}
for (i in 1:length(k2)){
  
  cat("\n  \n  \n#### ", i, "th neuron \n")
  
  plt_title <- paste0("var(tilde_w) ~ epochs; k2=", round(k2[i], 4))
  print(
    varmat_pltfcn(
      exp(sim_res$w_lvar_arr[i,,]),
      y_name = "var(tilde_w)",
      burn = 0
    )$all_vars_plt +
      labs(title = plt_title)
  )
}

```




# Corrections

$\tau$ correction, 


iterative $\tau$ correction:


__Frobenius norm correction to $Z$__: looked at correcting $Z_1$ (diagonal matrix of $\lambda \tau$): $Z_1^{corr} = Z_1 \times ||Z_2 W_2|| \times ||Z_3W_3||$ where $Z_l$ is diagonal matrix of scale parameters for layer $l$, and $W_l$ is the weight matrix for layer $l$, and $||$ denotes the Frobenius matrix norm.  __Appears to work fairly well.__  

  - rationale: tracks (to some degree at least) the impact of the scale parameters through all network layers.  
  
  - possible issues: 
  
    - Frobenius norm of a matrix in general grows with dimension.  This is, however, likely mitigated by the horseshoe scale parameters in each layer.
    
    - Not sure about theory.  Just working off the idea that the Frobenius norm measures in some sense the "magnitude" of a matrix.  Intuition is that the intervening layers all contribute to shrinkage.


```{r FROB}

# $k_1_corr = 1 - (1-k1) * ||diag(1-k2) W_2|| * ||diag(z3) W_3||$
# or, alternatively $z1_corr = Z1 * ||diag(Z2) W2|| * ||diag(Z3) W_3||$
frob <- function(mat){
  sqrt(sum(mat^2))
}

k1 <- get_kappas(nn_mod$fc1)
k2 <- get_kappas(nn_mod$fc2)
k3 <- get_kappas(nn_mod$fc3)
w2 <- get_wtil_params(nn_mod$fc2)$wtil_mu
w3 <- get_wtil_params(nn_mod$fc3)$wtil_mu

# doesn't work on kappa parameters (negative values)
# k1_c <- 1 - (1 - k1) * frob(diag(1-k2) %*% t(w2)) * frob(diag(1-k3)%*%t(w3))
# cbind(k1, k1_c)[1:10, ]

z1 <- sqrt(get_zsq(nn_mod$fc1))
z2 <- sqrt(get_zsq(nn_mod$fc2))
z3 <- sqrt(get_zsq(nn_mod$fc3))

z1_frob <- z1 * frob(diag(z2) %*% t(w2)) * frob(diag(z3) %*% t(w3))
k1_frob <- (1 + z1_frob^2)^(-1)

```

## iteratively-corrected kappas

- kappas generated by applying tau correction throughout network


```{r ITERATIVE_TAU}

# using layer 2 only
m2 <- m_eff(nn_mod$fc2)
k1_taucorr <- (1 + z1^2 * 16/m2)^(-1)

# iteratively over layers
get_kappas_taucorrected_iterative <- function(nn_mod){
  L <- length(nn_mod$children)
  m_eff_l <- sum(1 - get_kappas(nn_mod$children[[L]]))
  
  for (l in L:2){
    correction_l <- length(get_kappas(nn_mod$children[[l]])) / m_eff_l
    zsq_l <- get_zsq(nn_mod$children[[l-1]])
    k_l_corrected <- (1 + zsq_l*correction_l)^(-1)
    m_eff_l <- sum(1-k_l_corrected)
  }
  
  return(k_l_corrected)
}

# apply(w_mu^2 / w_var, 2, mean)
# W_var <- sweep(w_var, 2, zsq1, "*")
# wvar_weighted_zsq1 <- apply(W_var, 2, mean)
# 
# 
# 
# w_var <- exp(w_lvar)
# dim(w_var)
# 
# tm <- matrix(rep(1:5, 3), nrow = 5)
# tm
# sweep(tm, MARGIN = 2, STATS = 1:3, FUN = "*")


```








```{r COMPARISON}

err_by_max_bfdr(k1)$plt_fdrs + 
  labs(subtitle = "uncorrected kappas")

err_by_max_bfdr(get_kappas_taucorrected(nn_mod))$plt_fdrs + 
  labs(subtitle = "tau corrected with layer 2 only")

err_by_max_bfdr(get_kappas_taucorrected_iterative(nn_mod))$plt_fdrs + 
  labs(subtitle = "tau corrected iteratively")

err_by_max_bfdr(k1_frob)$plt_fdrs + 
  labs(subtitle = "frobenius correction to scale params")
# checking functions places in `analysis_fcns.R`
# err_by_max_bfdr(get_kappas_frobcorrected(nn_mod))$plt_fdrs
# err_by_max_bfdr(get_kappas_taucorrected(nn_mod, ln_mean))$plt_fdrs
```


- correction to $\tau$ with layer 2 appears best.

- rather than thinking about how shrinkage gets carried through the whole network, for feature selection it seems __more important to think about how the inputs get passed to the next layer__.  The poor performance of the Frobenius norm-based correction seems to suggest this as well.

  - would be interesting to compare performance against "relevance" detection techniques.  My guess would be that we would do better on smaller signal-to-noise covariates or when the effects are more localized.



```{r}

# zsq1 <- get_zsq(nn_mod$fc1)
# zsq2 <- get_zsq(nn_mod$fc2)
# 
# zmat <- t(outer(sqrt(zsq1), sqrt(zsq2)))
# apply(zmat, 2, geom_mean)
# 
# 
# 
# # older thoughts 
# # Wald_mat <- w_mu^2 / w_var
# # tstat <- sqrt(Wald_mat)
# # colMeans(Wald_mat)
# # colMeans(tstat)
# # 
# # apply(Wald_mat, 2, geom_mean)
# # apply(tstat, 2, geom_mean)
# # 
# # apply(k12_weighted_var, 2, geom_mean)
# 
# 
# 
# zsq1 <- get_zsq(nn_mod$fc1)
# zsq2 <- get_zsq(nn_mod$fc2)
# z2wvar <- sweep(w_var, MARGIN = 1, STATS = sqrt(zsq2), FUN = "*")
# vismat(z2wvar)
# 
# 
# apply(w_var, 2, geom_mean)
# 
# w_var_z2weighted <- geom_mean(sqrt(zsq2)) / apply(z2wvar, 2, geom_mean)
# 
# varmat_pltfcn(
#   sim_res$alpha_mat,
#   y_name = "alpha",
#   burn = 0
# )$all_vars_plt
# 
# 
# varmat_pltfcn(
#   sim_res$kappa_local_mat,
#   y_name = "local kappa",
#   burn = 0
# )$all_vars_plt
# 
# varmat_pltfcn(
#   sim_res$kappa_mat,
#   y_name = "kappa",
#   burn = 0
# )$all_vars_plt
# 
# k1 <- get_kappas(nn_mod$fc1)
# k2 <- get_kappas(nn_mod$fc2)
# wtil_params <- get_wtil_params(nn_mod$fc1)
# w_mu <- wtil_params$wtil_mu
# w_lvar <- wtil_params$wtil_lvar
# 
# vismat(exp(w_lvar))
# vismat(w_mu)
# 
# 
# 
# 
# 
# 
# 
# get_kappas(nn_mod$fc2)
# # 3, 4, 8, 10
# varmat_pltfcn(
#   exp(sim_res$w_lvar_arr[2,,]),
#   y_name = "var(tilde_w)",
#   burn = 0
# )$all_vars_plt
# 
# 
# varmat_pltfcn(
#   exp(sim_res$w_lvar_arr[3,,]),
#   y_name = "var(tilde_w)",
#   burn = 0
# )$all_vars_plt
# 
# varmat_pltfcn(
#   exp(sim_res$w_lvar_arr[10,,]),
#   y_name = "var(tilde_w)",
#   burn = 0
# )$all_vars_plt






# sim_ind <- 1
# 
# ztilsq_1 <- get_ztil_sq(nn_mod$fc1) 
# ssq_1 <- get_s_sq(nn_mod$fc1)
# ztilsq_2 <- get_ztil_sq(nn_mod$fc2) 
# ssq_2 <- get_s_sq(nn_mod$fc2)
# 
# zsq_1 <- ztilsq_1 * ssq_1
# zsq_2 <- ztilsq_2 * ssq_2
# 
# zsq_1_tc <- zsq_1 * (16 / m_eff(nn_mod$fc2))^2
# k_1_tc <- (1 + zsq_1_tc)^(-1)
# err_mat <- err_by_max_bfdr(k_1_tc)$err_mat
# err_mat[, 1:5] %>% 
#   as_data_frame() %>% 
#   pivot_longer(cols = -max_bfdr) %>% 
#   ggplot(aes(y = value, x = max_bfdr, color = name)) +
#   geom_line() + 
#   labs(
#     title = paste0("tau_corr 1: sim ", sim_ind, ": FP/TP vs max_BFDR")
#   )
# 
# 
# zsq_1_tc2 <- zsq_1 * (16 / m_eff(nn_mod$fc2))
# k_1_tc2 <- (1 + zsq_1_tc2)^(-1)
# err_mat2 <- err_by_max_bfdr(k_1_tc2)$err_mat
# err_mat2[, 1:5] %>% 
#   as_data_frame() %>% 
#   pivot_longer(cols = -max_bfdr) %>% 
#   ggplot(aes(y = value, x = max_bfdr, color = name)) +
#   geom_line() + 
#   labs(
#     title = paste0("tau_corr 2 (sqrt): sim ", sim_ind, ": FP/TP vs max_BFDR")
#   )
# 

```







































```{r LOAD_1kobs_xnorm}
# load ----
seeds <- c(
  215244, 
  272835, 
  124107, 
  782699, 
  798186, 
  68021, 
  849913, 
  283608, 
  658409, 
  130356
)

mod_stem <- here::here("sims", "results", "hshoe_smooth_pvtau_testfewer21632_1250obs_")
mod_fnames <- paste0(mod_stem, seeds, ".pt")
res_fnames <- paste0(mod_stem, seeds, ".RData")
```



# $X$ norm, 1k obs

```{r CHECK_FITS_1k}
# check model fits across sims
load(res_fnames[1])
epoch <- 1:nrow(sim_res$loss_mat) * sim_res$sim_params$report_every
lmat <- cbind(1, epoch, sim_res$loss_mat)
colnames(lmat)[1] <- "sim"

for (sim_ind in 2:length(seeds)){
  load(res_fnames[sim_ind])
  epoch <- 1:length(sim_res$loss_mat) * sim_res$sim_params$report_every
  curr_lmat <- cbind(sim_ind, epoch, sim_res$loss_mat)
  lmat <- rbind(lmat, curr_lmat)
}

colnames(lmat)
lmat %>% 
  as_data_frame() %>% 
  mutate(sim = factor(sim)) %>% 
  ggplot(aes(y = kl, x = epoch, color = sim)) +
  geom_line() + 
  labs(title = "xnorm, 1k obs: KLs across epochs & sims")

lmat %>% 
  as_data_frame() %>% 
  mutate(sim = factor(sim)) %>% 
  pivot_longer(cols = -c("epoch", "sim")) %>% 
  filter(name != "kl") %>% 
  ggplot(aes(x = epoch, color = sim, linetype = name)) +
  geom_line(aes(y = value)) + 
  labs(title = "xnorm, 1k obs: test/train MSE across epochs & sims")
```


- all of the fits appear to overfit generalize well --- high test MSE, especially 6'th sim.

- low number of obs probably makes this particularly difficult

- exacerbated by using normal distribution to generate $X$ values, i.e. xvals will tend to be clustered around 0.






























