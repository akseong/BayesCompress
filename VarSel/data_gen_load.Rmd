---
title: "VarSel"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: true
code_fold: hide
urlcolor: blue
params:
    retrain: FALSE
---
  
  
```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----
library(reticulate)

#### plotting:
library(ggplot2)
library(gridExtra)

#### Misc:
library(here)
library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)

# DOCUMENT SETUP ----
#### detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)
if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}
options(scipen=10)

# TEXT/TABLE FORMATTING----
purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}


#### kable NA handling
options(knitr.kable.NA = '')

#### detect if knitting
knitting <- !is.null(getOption("knitr.in.progress"))

#### mykable
mykable <- function(tab, cap = NULL,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  # common additional unspecified args (...):
  #     digits = 2
  #     format.args = (list(big.mark = ","))
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}
```





# generate data

## linear data

Choose from normal & binomial distributions, select variables, make $y$ a linear transformation of selected vars, return dataframe.

```{r generate_linear_data_func}

generate_linear_data <- function(
    ave_beta = 2,
    sd_beta = 10,
    sd_eps = 2,
    n_obs = 1E4,
    n_covars = 10,
    n_nuisance = 20
  ){
  
  beta_vec <- round(rnorm(n_covars + 1, mean = ave_beta, sd = sd_beta), 2)
  dat <- data.frame(matrix(0, nrow = n_obs, ncol = n_covars + n_nuisance))
  distn_vec <- rep(NA, n_covars + n_nuisance)
  
  for (j in 1:(n_covars+n_nuisance)) {
    distn <- sample(c("normal","binomial"), 1)
    if (distn == "normal"){
      mu_j <- round(runif(1, 0, 10))
      sig_j <- round(runif(1, 1, 5))
      dat[, j] <- rnorm(n_obs, mu_j, sig_j)
      distn_vec[j] <- paste0(distn, "(", mu_j, ", ", sig_j, ")")
      
    } else if (distn == "binomial"){
      p_j <- runif(1, 0.1, 1)
      n_j <- floor(runif(1, 1, 10))
      dat[,j] <- rbinom(n_obs, size = n_j, prob = p_j)
      distn_vec[j] <- paste0(distn, "(", n_j, ", ", p_j, ")")
    }
  }
  
  # construct y
  # dat <- round(dat, 4)
  covar_inds <- sort(sample(1:(n_covars + n_nuisance), n_covars))
  covars <- as.matrix(dat[, covar_inds])
  covar_distns <- distn_vec[covar_inds]
  eps <- rnorm(n_obs, mean = 0, sd = sd_eps)
  dat$y <- as.vector(cbind(1, covars) %*% beta_vec + eps)
  true_covars <- colnames(dat)[covar_inds]
  nuisance_vars <- colnames(dat)[setdiff(1:(n_nuisance + n_covars), covar_inds)]
  names(beta_vec) <- c("(Intercept)", true_covars)
  
  res <- list(
    "dat" = dat,
    "eps" = eps,
    "sd_eps" = sd_eps,
    "distn_vec" = distn_vec,
    "n_obs" = n_obs,
    "n_covars" = n_covars,
    "n_nuisance" = n_nuisance,
    "beta_vec" = beta_vec,
    "covar_inds" = covar_inds,
    "true_covars" = true_covars,
    "nuisance_vars" = nuisance_vars
  )
  
  return(res)
}

# vismat(mat, cap = NULL, leg = TRUE, na0 = TRUE, square)----
vismat <- function(mat, cap = NULL, leg = TRUE, na0 = TRUE, lims = NULL, square = NULL, preserve_rownums = TRUE){
  # outputs visualization of matrix with few unique values
  # colnames should be strings, values represented as factors
  # sci_not=TRUE puts legend in scientific notation
  require(ggplot2)
  require(scales)
  require(reshape2)
  
  if(!preserve_rownums) rownames(mat) <- NULL
  
  melted <- melt(mat)
  melted$value <- ifelse(
    melted$value == 0 & na0,
    NA,
    melted$value
  )
  p <- ggplot(melted) + 
    geom_raster(aes(y = Var1, 
                    x = Var2, 
                    fill = value)) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
    scale_fill_viridis_c(limits = lims) + 
    scale_x_discrete(expand = c(0,0))
  
  if (is.numeric(melted$Var1)){
    p <- p + 
      scale_y_reverse(expand = c(0,0))
  } else {
    p <- p + 
      scale_y_discrete(limits = rev(levels(melted$Var1)), expand = c(0,0))
  }
  
  

  if (is.null(square)) square <- nrow(mat) / ncol(mat) > .9 & nrow(mat) / ncol(mat) < 1.1
  if (square) p <- p + coord_fixed(1)
  
  if(is.null(cap)) cap <- paste0("visualization of matrix ", substitute(mat))
  
  p <- p + labs(title=cap)
  
  if (!leg) p <- p + theme(legend.position = "none")
  
  return(p)
}
```


### visualize

```{r}
set.seed(0)
lindat_obj <- generate_linear_data()
dat <- lindat_obj$dat
true_covars <- lindat_obj$true_covars
nuisance_vars <- lindat_obj$nuisance_vars
covar_inds <- lindat_obj$covar_inds
beta_vec <- lindat_obj$beta_vec
true_covars_inds <- as.integer(as.integer(unlist(lapply(strsplit(true_covars, "X"), function(x) x[2]))) - 1)
nuisance_covars_inds <- as.integer(as.integer(unlist(lapply(strsplit(nuisance_vars, "X"), function(x) x[2]))) - 1)

dat %>% 
  slice_sample(n = 500) %>% 
  gather(key = "covar", value = "value", -y) %>% 
  mutate(true_cov = if_else(
    covar %in% lindat_obj$true_covars,
    "covar_true",
    "nuisance"
    )
  ) %>% 
  ggplot() + 
  geom_density(
    aes(
      x = value,
      color = covar,
      fill = covar,
      alpha = I(ifelse(true_cov=="covar_true", .25, .1))
    )
  )


dat %>% 
  slice_sample(n = 500) %>% 
  gather(key = "covar", value = "value", -y) %>% 
  mutate(true_cov = if_else(
    covar %in% lindat_obj$true_covars,
    "covar_true",
    "nuisance"
    )
  ) %>% 
  ggplot() + 
  geom_point(
    aes(
      y = y,
      x = value,
      color = covar,
      alpha = I(ifelse(true_cov=="covar_true", .25, .1))
    )
  ) + 
  geom_smooth(
    aes(
      y = y,
      x = value
    ),
    method = 'lm'
  )

ols_varsel_err <- function(lmfit, true_covars, nuisance_vars, thresh = 0.05){
  coeftab <- summary(lmfit, na.rm = FALSE)$coef
  # if (is.character(true_covars)){
  #   true_covars <- as.numeric(unlist(lapply(strsplit(true_covars, "X"), function(X) X[2]))) + 1
  #   nuisance_vars <- as.numeric(unlist(lapply(strsplit(nuisance_vars, "X"), function(X) X[2]))) + 1
  # }
  
  TP <- sum(coeftab[true_covars, 4] < thresh)
  FN <- sum(coeftab[true_covars, 4] >= thresh)
  TN <- sum(coeftab[nuisance_vars, 4] >= thresh)
  FP <- sum(coeftab[nuisance_vars, 4] < thresh)
  c("TP" = TP, "FP" = FP, "TN" = TN, "FN" = FN)
}



lmfit <- lm("y ~ .", data = dat)
lmfit_coefs <- coef(lmfit)
ols_varsel_err(lmfit, true_covars, nuisance_vars)
# dat <- data.frame(apply(dat, 2, scale))
```























## make pytorch data loader

```{r python_env_setup, echo = FALSE}
# if doesn't exist, create conda environment and install dependencies
if (!"r-reticulate" %in% conda_list()[[1]]){
  conda_create("r-reticulate")
  use_condaenv("r-reticulate")
  conda_install("r-reticulate", "matplotlib", pip=TRUE)
  conda_install("r-reticulate", "scipy", pip=TRUE)
  conda_install("r-reticulate", "scikit-learn", pip=TRUE)
  conda_install("r-reticulate", "sklearn", pip=TRUE)
  conda_install("r-reticulate", "torch", pip=TRUE)
  conda_install("r-reticulate", "torchvision", pip = TRUE)
  conda_install("r-reticulate", "imageio", pip = TRUE)
  conda_install("r-reticulate", "seaborn", pip=TRUE)
  conda_install("r-reticulate", "numpy", pip = TRUE)
}
use_condaenv("r-reticulate")

# set file path
fpath <- here::here("VarSel")
setwd(fpath)
KUpath <- here::here("VarSel", "kullrich_files")
# py_install("scikit-image", pip = TRUE)
```




```{python load_python_libs}
from __future__ import print_function, division
import os
os.chdir(r.fpath)

import sys
sys.path.append(r.KUpath)

import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.switch_backend('tkagg')

import torch as torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader

# source Bayesian Compression .py files
import BayesianLayers
from compression import compute_compression_rate, compute_reduced_weights
from utils import visualize_pixel_importance, generate_gif, visualise_weights
import compression
import utils

# global arguments
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--epochs', type=int, default=50)
parser.add_argument('--batchsize', type=int, default=100)
parser.add_argument('--thresholds', type=float, nargs='*', default=[-2.8, -3., -5.])
FLAGS = parser.parse_args()
FLAGS.cuda = torch.cuda.is_available()  # check if we can put the net on the GPU

```



```{python TEST_TRAIN_SPLIT}

X = np.asarray(r.dat)[:, :r.dat.shape[1]-1]
y = np.asarray(r.dat)[:, r.dat.shape[1]-1]
X, y = torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(-1)

N = len(y)
N_train = N * 4 // 5


# Shuffles the indices
idx = np.arange(N)
np.random.shuffle(idx)

# Uses first 80% random indices for train
train_idx = idx[:N_train]
test_idx = idx[N_train:]

# Generates train and test
X_train, y_train = X[train_idx], y[train_idx]
X_test, y_test = X[test_idx], y[test_idx]


# checking that R and python return the same OLS fit
from sklearn.linear_model import LinearRegression
linr = LinearRegression()
linr.fit(X, y)
print(linr.intercept_, linr.coef_)
r.lmfit_coefs
```


```{r}

coef_check <- cbind(
  "linr.fit (python)" = c(py$linr$intercept_,  py$linr$coef_),
  "lm (R)" = lmfit_coefs
)
mykable(round(coef_check, 3), cap = "check that data from R and imported to python return same OLS fit")

```


```{python dataloader}
# merge into tuples
def merge(list1, list2):
    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]
    return merged_list

train_dat = merge(X_train, y_train)

train_loader = DataLoader(
  train_dat,
  batch_size=FLAGS.batchsize, 
  shuffle=True
)

test_dat = merge(X_test, y_test)

test_loader = DataLoader(
  test_dat,
  batch_size=FLAGS.batchsize, 
  shuffle=True
)

mask = 255. * (np.ones((1, 30)))
examples = X_train[0:5,]
images = np.vstack([mask, examples])

# check dataloader
# for batch_idx, (data, y) in enumerate(test_loader):
#   # print(data)
#   # print(y)
#   print(data.size())
#   print(y.size())

print("")


```




```{python define_Bayesian_NN}
# build a simple MLP
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # activation
        self.relu = nn.ReLU()
        # layers
        self.fc1 = BayesianLayers.LinearGroupNJ(1 * 30, 20, clip_var=0.04, cuda=FLAGS.cuda) # 28 x 28 <--- 1 * 31
        self.fc2 = BayesianLayers.LinearGroupNJ(20, 30, cuda=FLAGS.cuda)
        self.fc3 = BayesianLayers.LinearGroupNJ(30, 1, cuda=FLAGS.cuda)
        # layers including kl_divergence
        self.kl_list = [self.fc1, self.fc2, self.fc3]

    def forward(self, x):
        x = x.view(-1, 1*30)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)

    def get_masks(self,thresholds):
        weight_masks = []
        mask = None
        for i, (layer, threshold) in enumerate(zip(self.kl_list, thresholds)):
            # compute dropout mask
            if mask is None:
                log_alpha = layer.get_log_dropout_rates().cpu().data.numpy()
                mask = log_alpha < threshold
            else:
                mask = np.copy(next_mask)
            try:
                log_alpha = layers[i + 1].get_log_dropout_rates().cpu().data.numpy()
                next_mask = log_alpha < thresholds[i + 1]
            except:
                # must be the last mask
                next_mask = np.ones(10)

            weight_mask = np.expand_dims(mask, axis=0) * np.expand_dims(next_mask, axis=1)
            weight_masks.append(weight_mask.astype(np.float))
        return weight_masks

    def kl_divergence(self):
        KLD = 0
        for layer in self.kl_list:
            KLD += layer.kl_divergence()
        return KLD



# init model
model = Net()
if FLAGS.cuda:
    model.cuda()

# init optimizer
optimizer = optim.Adam(model.parameters())

# we optimize the variational lower bound scaled by the number of data
# points (so we can keep our intuitions about hyper-params such as the learning rate)
# discrimination_loss = nn.functional.cross_entropy
discrimination_loss = nn.MSELoss(reduction='none')

def objective(output, target, kl_divergence):
    discrimination_error = discrimination_loss(output, target)
    # print("kl.item value, type: {}, {}".format(kl_divergence.item(), type(kl_divergence.item())))
    # print("kl_divergence data type: {}".format(type(kl_divergence)))
    # print("loss data value, type: {}".format(discrimination_error, type(discrimination_error)))
    variational_bound = (discrimination_error + kl_divergence) / N_train
    # print("variational_bound calculated")
    # print("variational_bound value, type: {}, {}".format(torch.mean(variational_bound), type(torch.mean(variational_bound))))
    if FLAGS.cuda:
        variational_bound = variational_bound.cuda()
    return variational_bound

def train(epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        if FLAGS.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        # print("output = model(data)")
        loss = objective(output, target, model.kl_divergence())
        # print("loss = ...")
        # loss.backward()
        loss.sum().backward()
        # print("loss.sum().backward()")
        optimizer.step()
        # clip the variances after each step
        for layer in model.kl_list:
            layer.clip_variances()
    print('Epoch: {} \tTrain loss: {:.6f} \t'.format(
        epoch, torch.mean(loss.data)))

def test():
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if FLAGS.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        # test_loss += discrimination_loss(output, target, size_average=False).data
        test_loss += discrimination_loss(output, target).data
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()
    test_loss /= len(test_loader.dataset)
    print('Test loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        torch.mean(test_loss), correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


```


```{python run_load_model}
if r.params["retrain"]:
  # train the model and save some visualisations on the way
  print("--start training--")
  for epoch in range(1, FLAGS.epochs + 1):
      print("--epoch" + str(epoch) + "--")
      train(epoch)
      test()
      # visualizations
      weight_mus = [model.fc1.weight_mu, model.fc2.weight_mu]
      log_alphas = [model.fc1.get_log_dropout_rates(), model.fc2.get_log_dropout_rates(),
                    model.fc3.get_log_dropout_rates()]
      visualise_weights(weight_mus, log_alphas, epoch=epoch)
      log_alpha = model.fc1.get_log_dropout_rates().cpu().data.numpy()
      # visualize_pixel_importance(images, log_alpha=log_alpha, epoch=str(epoch))
  
  
  # compute compression rate and new model accuracy
  layers = [model.fc1, model.fc2, model.fc3]
  thresholds = FLAGS.thresholds
  compute_compression_rate(layers, model.get_masks(thresholds))
  
  weights = compute_reduced_weights(layers, model.get_masks(thresholds))
  for layer, weight in zip(layers, weights):
      if FLAGS.cuda:
          layer.post_weight_mu.data = torch.Tensor(weight).cuda()
      else:
          layer.post_weight_mu.data = torch.Tensor(weight)
  # for layer in layers: layer.deterministic = True
  
  torch.save(model.state_dict(), 'model_weights.pth')
  
  # generate_gif(save='pixel', epochs=FLAGS.epochs)
  generate_gif(save='weight0_e', epochs=FLAGS.epochs)
  generate_gif(save='weight1_e', epochs=FLAGS.epochs)
else:
  model.load_state_dict(torch.load('model_weights.pth'))
  
layers = [model.fc1, model.fc2, model.fc3]
thresholds = FLAGS.thresholds
weight_mus = [model.fc1.weight_mu, model.fc2.weight_mu]
weight_mus, weight_vars = compression.extract_pruned_params(layers, model.get_masks(thresholds))

def extract_pruned_layer(layer, mask):
  # extract non-zero columns
  layer = layer[:, mask.sum(axis = 0) != 0]
  # extract non-zero rows
  layer = layer[mask.sum(axis = 1) != 0, :]
  return layer

def extract_pruned_layers(layers, masks):
  res = []
  for layer, mask in zip(layers, masks):
    l = extract_pruned_layer(layer, mask)
    res.append(l)
  return(res)

pruned_weights = extract_pruned_layers(weight_mus, model.get_masks(thresholds))
# pruned_weights[0].shape

mask0 = model.get_masks(thresholds)[0]
```


```{r}

vismat(py$weight_mus[[1]], cap = "first weight layer")


gamma_hat <- colSums(abs(py$weight_mus[[1]])) != 0
gamma_true <- rep(FALSE, length(gamma_hat))
gamma_true[as.numeric(gsub("X", "", true_covars))] <- TRUE


binary_err <- function(pred, true){
  TP <- sum(true & pred)
  FP <- sum(!true & pred)
  TN <- sum(!true & !pred)
  FN <- sum(true & !pred)
  
  c("TP" = TP, "FP" = FP, "TN" = TN, "FN" = FN)
}

binary_err(pred = gamma_hat, true = gamma_true)
ols_varsel_err(lmfit, true_covars, nuisance_vars)
```





+ set up simulations --- need more python expertise
+ how to determine number and size of layers?
  - some rough idea of data complexity
+ testing on functional data











