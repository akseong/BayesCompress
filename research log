research log:


############
11/8
############

# need to monitor test-train split along with TP TN rate
# run 10 times
# keep track of test, train, FP, TP
# sweet spot tends to be btwn 5-8k epochs

# start storing at 3000 epochs
# need to get a sense of how moving average changes?
# is there a time when it gets all of them correct?
# check for ranking
# tt_mse
# earliest best is at 3500 epochs
# FP at 6000
# diverging at 6500
# just gets worse and worse 


# part of the problem might be that we have more parameters than data, so 
# the network can memorize the data.  So CV will keep going in circles.
# for the alphas, cap them at 1, then renormalize
# what if drop nodes in the hidden layers, then retrain? using starting points for hidden layers?
# some support for this as a VAE procedure



- training regimes tested
    - 5-fold CV, 1000 obs, hidden layers 50, 25 - same issues
        - KL seems inappropriately scaled?  at 2500 epochs in, around 23
    - no CV - bad
    - 5-fold CV, 10k obs, hidden layers 25, 25
        - seems more stable?  KL is better scaled 
            - 500 epochs, MSE = 5, KL = 1.6
            - 4000 epochs, MSE = 2.57, KL = 1.15
            - 7000 epochs, MSE = 1.8, KL = 0.74
    - tried with only 5 vars (so only 1 nuisance) to see how estimation goes. 
    	- fcn estimation is actually fairly good BUT
    	- the 1 nuisance variable's alpha appears to be going to 0 as training goes on
    	- for some reason all the predicted curves appear to be shifted up
    	- function estimation is really good around x = 0; because all the covs are generated as N(0,1), so there's the most data there.


    - tried simulating covariates from uniform dist'n
    - 




- monitoring predicted functions



ideas:
    - consider training 1x, eliminating nodes with high alpha in hidden layers, retraining
    - what about eliminating nodes with alpha > 1 at some point
    - function estimation seems a bit fucky, probably because keeping all these unused vars in?
    - look at the clip variances thing in Ullrich's code




############
Nov 12
############

Variable inclusion factor




############
Nov 20
############

testing over the last few weeks suggests that 
1) when the network's parameters outnumber the observations we have to work with, the unmodified alpha parameter is best to work with, and we should stop training early (otherwise the network will overfit by fitting nuisance variables to noise / fill in gaps)

2) when the number of observations is smaller than the number of parameters, training for a long time is helpful, but we should use the alphas after centering them at the geometric mean





3) perhaps the first hidden layer should be smaller in dimension?

16 -> 32 -> 64



need to save FP TP FN TN alongside tt_mse














David A. Van Dyk
- astrophysics data, specifically spectroscopy
- covariate shift - high-resolution histograms from 1 type of sensor/telescope, lower resolution histogram from other, more plentiful sensor
    - method "StratLearn" uses propensity scores to account for covariate shift


- patch priors to reconstruct images from noisy photon counts










contact ICS grad office
  - advancement
  - petition for Michele to attend

fMRI people in cog sci, particularly junior faculty

Ana Marie Kenney
Wenjuo






cog sci:

- Arron Bornstein
Nadia Chernyak
Anna Leshinskaya
Cherlyn Ng




practice --- 

ranger, BART
torch
ASR/TTS models








introBVS presentation:


problem statement - what is variable selection, when useful

- shrinkage / penalization
    - James Stein estimator / paradox: https://www.youtube.com/watch?v=cUqoHQDinCM







https://stats.stackexchange.com/questions/646260/variable-selection-strategies
Established Frequentist methods
   - Fank Harrell - p 56: file:///C:/Users/akseong/Downloads/978-1-4757-3462-1.pdf 
 - stepwise selection
 regularization / penalization --- loss unction = RSS + penalty * model complexity
 - Ridge regression
 - LASSO
 - elastic-net
 - Group LASSO

Newer Frequentist methods
 - knockoffs (2015)   
   - https://statweb.stanford.edu/~candes/publications/downloads/FDR_regression.pdf
   - https://web.stanford.edu/group/candes/knockoffs/papers.html




 Bayesian Methods
 - bayesian ridge
 - Bayesian LASSO - Park & Casella (2008)
 - Mixture
 - Horseshoe
 - Spike-and-slab
 - BVS via BART
Bayesian model averaging



https://www.stata.com/statanow/bayesian-variable-selection-linear-regression/









---
**Problem statement**
Why do model / variable selection?  Goals?
- practical:
  - observing variables may be costly
  - eliminate redundancies / inflation of variance
  - p < n easier computationally
- theoretical / philosophical
    - less complex models are more interpretable
    - if we know the correct model, we can obtain optimal parameter estimates and uncertainty quantification

- model vs. variable selection - model selection is the broader term, comprising things such as link function, assumptions about data generating process, etc.

---
Hurdles?
- don't want to eliminate variables that explain the outcome
- don't want to include variables that do not explain the outcome
- want reliable uncertainty quantification
- want to preserve T1 error / False discovery rates
- bonus: it would be great to be able to express our uncertainty about whether a variable has an appreciable effect on the outome of interest


---
Prediction / Inference
Prediction
- Variable selection is typically less important if the only goal is to make "good" predictions, but can still have an impact on the quality of prediction
  - overspecification --- including too many variables ---> overfitting, misleadingly good in-sample performance, poor out-of-sample performance
  - underspec --- including too few ---> underfitting, non-optimal performance
  - misspecification of functional relationship 

Inference (our focus)
"As measurements become more complex they often convolve meaningful phenomena with more extraneous phenomena. In order to limit the impact of these irrelevant phenomena on our inferences, and isolate the relevant phenomena, we need models that encourage sparse inferences."
https://betanalpha.github.io/assets/case_studies/modeling_sparsity.html
(Michael Betancourt, core developer for STAN)



---
How to proceed in an inferential setting:

> 1) Use subject matter knowledge before looking at the dataset; (2) Use redundancy analysis/data reduction blinded to Y; (3) Use a method that adequately penalizes for the huge multiple comparison problem caused by doing feature selection (Frank Harrell)
link: https://stats.stackexchange.com/questions/18214/why-is-variable-selection-necessary

Algorithmic model selection methods:
Stepwise methods:
- generally derided
- do not preserve important operating characteristics such as type I error
- methods that dropping variables based on significance tests (e.g. partial F-tests) belong here

Frequentist methods:
- penalization / shrinkage methods, e.g. Ridge or LASSO
- more recent: knockoffs

Bayesian:
- mixture models
- ABC

 


