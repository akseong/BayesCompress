---
title: "Lit Review"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}



cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}


```






## Louizos, Ullrich, Welling 2017, "Bayesian Compression for Deep Learning"



## Hron 2018 - Variational Dropout: pitfalls and fixes
  - failure of variational inference with Normal-Jeffreys prior



## 2017 Improved Bayesian Compression



## Shin 2017 Scalable BVS using non-local priors



## 2018 Polson & Rockova - Posterior Conentration for Sparse Deep Learning.  Spike-slab on individual coefficients.  

  - Applicable theory.  Worth a work through.


## 2019 Song, Li - SurvNet 

  - FDR control via augmenting with surrogate variables (seems similar to Candes' knockoffs.  Uses partial derivative of loss w.r.t. x_j as "importance measure".  **Training involves backward elimination of variables**.  Estimated FDR relies on an "upper bound" for the number of null variables in original data.


## Overweg 2019 Interpretable Outcome Prediction 

  - uses horseshoe similarly, but not a very good paper
  - look at implementation


## 2020 Ghosh, Doshi-Velez - Model Selection in BNN's via HShoe priors

- hshoe prior on node preactivations, i.e. trimming nodes
- regularized hshoe prior used


## 2020 Liu. Li, Yu, Zeng - Posterior-Based Wald-Type Statistics for Hypothesis Testing

  - theory for Wald-type stat and asymptotic chi-sq distribution


## 2021 Liu - Variable Selection with Rigorous Uncertainty Quantifcation using Deep Bayesian Neural Networks: Posterior Concentration and Bernstein-von Mises Phenomenon

- https://proceedings.mlr.press/v130/liu21g/liu21g.pdf



## 2021 DNN with controlled VarSel

  - uses knockoffs in input layer


## 2022 Learning Sparse DNNs with Spike-and-Slab

  - why is this a paper?

## 2022 BayesFormer


## 2022 Huix, Majewski, DUrmus, Moulines, Korba - VI of overparameterized BNNs

- indicate that KL needs to be re-scaled w.r.t. **the ratio between the number of obs and neurons**


## 2023 Hubin, Storvik - VI for Bayesian NNs under Model and Parameter Uncertainty
  - Spike-and-Slab on weights, Normal slab with IG prior on variance, Beta-binomial on inclusion probs
  - VI formulation includes some structure via MVN prior on inclusion probs.  HOWEVER, MVN is over ALL inclusion probs in a layer.  Discusses case when low-rank parametrization of MVN covariance is diagonal (i.e. independence), but no discussion of other structuring.
  - varsel on individual params (no structuring)


## 2023 BNN priors revisited



## 2023 Skaaret-Lund - "sparsifying Bayesian NNs with latent binary variables and normalizing flows."  

  - NFlow used to model mean response function, spike-and-slab on weights (individual weights independent).  

  - **This is the only paper with similar types of methods that reports feature selection metrics (TPR / FPR), but NOT IN DNN case**: variable selection performed in SINGLE NEURON logistic regression setting, i.e. basic logistic regression problem (20 variables, some correlated).  Good TPR/FPR (0.972 / 0.074) in this case.  






# Ideas

- check if kappas are any better earlier in training

- regularized hshoe

- reparameterize shrinkage params from factored to dependent, i.e.

  - current model: 
  
$$\begin{aligned}
  z_i & = \tilde{z}_i s
    \\
  \tilde{z_i} & \sim C^+(0, 1)
    && \text{local scale parameter}
    \\
  s & \sim C^+(0, \tau_0)
    && \text{global scale parameter}
    \\
  \text{reparameterization:} 
    \\
  \tilde{z}_i = \sqrt{\tilde{\alpha}_i \tilde{\beta}_i}
  \\ 
  s = \sqrt{s_a s_b}
  \\
    \tilde{\alpha}_i 
    & \sim Gamma(1/2, 1)
    \\
  \tilde{\beta}_i
    & \sim InverseGamma(1/2, 1)
    \\
  s_a & \sim Gamma(1/2, \tau_0^2)
    \\
  s_b & \sim InverseGamma(1/2, 1)
    
\end{aligned}$$
    
  - I'm actually having trouble finding justification for this particular parameterization of the half-Cauchy.  The closest I'm finding is from Makalic & Schmidt 2016, Gelman 2006:

$$x \sim C^+(0, A) \iff x^2|a \sim IG(1/2, 1/a) \text{ and } a \sim IG(1/2, 1/A^2)$$








