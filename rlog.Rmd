---
title: "research log"
author: "Arnie Seong"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
  - \usepackage{bm}
  - \usepackage{xcolor}
  - \usepackage{amssymb}
output: 
  html_document:
    df_print: paged
    theme: cerulean
    highlight: tango
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: true
    code_fold: show
urlcolor: blue
params:
  retrain: FALSE
  seed: 314
editor_options: 
  chunk_output_type: console
---


<style type="text/css">

  #TOC>ul>li, h1, h2{
    font-weight: bold;
  }
  
  blockquote{
    background-color: #d5f2f5;
    font-size: small;
  }
  
  b, strong {
   color: #11579e;
  }
  
  caption {
    color: #11579e;
    font-weight: bold;
    font-size: 1.0em;
  }
  
  h1 { font-size: 2em }
  h2 { font-size: 1.6em  }
  h3 { font-size: 1.25em }
  h4 { font-size: 1.10em }
  h5, h6 { font-size: 1em    }
  
  h1, h2, h3 {font-weight: bold;}
  
  a {font-weight: bold; text-decoration: underline}
  
  .nav-pills>li>a:hover, 
  .nav-pills>li>a:focus, 
  .nav-pills>li.active>a,     
  .nav-pills>li.active>a:hover, 
  .nav-pills>li.active>a:focus{
     background-color: #11579e;
  }
  
  .nav-pills{
     background-color: #d5f2f5;
  }
</style>

```{r setup, include=FALSE, message=F, echo=F, warning=F}
# LIBRARIES----

#### plotting:
library(ggplot2)
library(gridExtra)

# #### Misc:
library(here)
library(tidyr)
library(knitr)
library(kableExtra)
library(dplyr)

# model packages
library(torch)

# DOCUMENT SETUP ----
# detect pdf/html output, set chunk options, sci.notation 
latex_out <- knitr::is_latex_output()
knitr::opts_chunk$set(
  cache = FALSE, 
  message = FALSE, 
  echo = !knitr::is_latex_output(), 
  warning = FALSE
)


if (knitr::is_latex_output()) {
  knitr::opts_chunk$set(fig.height=4, 
                        fig.width=6)
} else {
  knitr::opts_chunk$set(out.width = "100%")
}

options(scipen=10)


# TEXT/TABLE FORMATTING----

custom_purple <- ifelse(
  knitr::is_latex_output(),
  "purple",
  "#b51ced"
)

custom_blue <- ifelse(
  knitr::is_latex_output(),
  "blue",
  "#11579e"
)

colorize <- function(x, color=custom_purple) {
  # text color conditional on latex/html output
  # from rmarkdown cookbook
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{ %s}{ %s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, x)
  } else x
}

cat_color <- function(txt, style = 1, color = 36){
  cat(
    paste0(
      "\033[0;",
      style, ";",
      color, "m",
      txt,
      "\033[0m","\n"
    )
  )  
}

# kable NA handling
options(knitr.kable.NA = '')

# mykable function
mykable <- function(tab, cap,
                    latex_options=c("hold_position", "scale_down", "striped"), 
                    bootstrap_options=c("striped", "hover", "condensed"), 
                    full_width=F, position="center", ...){
  # kable formatting conditional on latex or html output
  if (is.null(getOption("knitr.in.progress"))){
    print(tab)
  } else if (knitr::is_latex_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(latex_options = latex_options)
  } else if (knitr::is_html_output()){
    kable(x=tab, caption=cap, ...) %>%
      kableExtra::kable_styling(bootstrap_options = bootstrap_options, full_width=full_width, position=position)
  }
}

# notin
`%notin%` <- Negate(`%in%`)

source(here("Rcode", "torch_horseshoe_klcorrected.R"))
source(here("Rcode", "sim_functions.R"))
source(here("Rcode", "analysis_fcns.R"))
```

# formatted (most recent first)



## 2/3/26

TO DO this week:

- [x] implement corrections in code: 

  - $\tilde \tau_1 = \tau_1 \times \sqrt{  \dfrac{d_1}{m_2^{eff}}    }$
  
  - $Z_1^{corr} = Z_1 \times ||Z_2 W_2|| \times ||Z_3W_3||$

- [ ] fix model predictions to generate E[y]

- [ ] decide on simulation settings

- [ ] work on setting up simulated salary data

- [ ] work on setting up ECoG data 

  - just use dummy variables denoting the different intervals of interest?  I.e. is this going to work without orthogonalizing the bases?

__other notes__:

- reverted to normal X-dist

- feel like I might have to bite the bullet and do the scaling stuff.


## 2/2/26
- started simulations using $X \sim Unif(-\sqrt{12}, \sqrt{12})$ (i.e. mean 0, var 1), but realized that $f_4()$ doesn't create much variation in $y$ in that range of $x$ (range of $x$is $\approx [-1.73, 1.73]$).  


- looking at weight variances / possibility of using $\sigma_{\tilde W_1}$ to weight layer 2 $\kappa$'s

- __Frobenius norm correction to $Z$__: looked at correcting $Z_1$ (diagonal matrix of $\lambda \tau$): $Z_1^{corr} = Z_1 \times ||Z_2 W_2|| \times ||Z_3W_3||$ where $Z_l$ is diagonal matrix of scale parameters for layer $l$, and $W_l$ is the weight matrix for layer $l$, and $||$ denotes the Frobenius matrix norm.  __Appears to work fairly well.__  

  - rationale: tracks (to some degree at least) the impact of the scale parameters through all network layers.  
  
  - possible issues: 
  
    - Frobenius norm of a matrix in general grows with dimension.  This is, however, likely mitigated by the horseshoe scale parameters in each layer.
    
    - Not sure about theory.  Just working off the idea that the Frobenius norm measures in some sense the "magnitude" of a matrix.  Intuition is that the intervening layers all contribute to shrinkage.


## 1/30/26
continued writing up setting, model

__`sim_func_data` modified__ to simulate X from mean 0 var 1 uniform (Unif(-sqrt(12), sqrt(12)))


## 1/9/26

writing up setting, model


### Simulation setting ideas:

- unmeasured relevant covariates;
  
  - possibly correlated with measured covariates?
  
- non-linear group interactions 


### Discover interactions?

- is there some way to model how effects pass through the network using trees?

- [Yiang, Li, Zhou 2018, Bayesian Neural Networks for Selection of Drug Sensitive Genes](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1409122)

  - discussion of Figures 3a and 4 showing weight layer connections


### performance of BFDR & inference

- need to my model test in a variety of settings --- i.e. non-sparse, diff $n$ --- to support validity of BFDR (since on somewhat shaky theoretical grounds with correction to global scale parameter)

- Theory supporting inference / posterior concentration

  - [Polson, Rockova 2018, Posterior Concentration for Sparse Deep Learning](https://arxiv.org/abs/1803.09138)
    
    - do they write their actual models down anywhere?  Work through this with a fine toothed comb

  - building off Polson, Rockova 2018: 

    - [Zhe Liu, Variable Selection with rigorous uncertainty quantification using Deep Bayesian Neural Nets: Posterior Concentration and Bernstein-von Mises Phenomenon](https://arxiv.org/abs/1912.01189)

    - [Wang, Rockova 2020, Uncertainty Quantification for Sparse Deep Learning](http://arxiv.org/abs/2002.11815)


- __Identifiability__ issues? 

  - [Yacoby, Pan, Doshi-Velez 2022, Mitigating the Effects of Non-Identifiability on Inference for Bayesian Neural Networks with Latent Variables](https://arxiv.org/pdf/1911.00569)





## 1/7/26 Meeting with Michele

- write down the full model, rationale for correction

- credible interval method vs correcting $\tau$

  - [van der Pas, Szabó, Vaart, Uncertainty quantification for the horseshoe](https://arxiv.org/abs/1607.01892)

- think about realistic data - ECOG/ERP data, salary data
  - student performance data (R included data)
  - do we have some kind of hospital data?
  - get in touch with Joni re: NYC school data?


$y_i = f_j(x_{1,j}) + \epsilon_i$



Talk to Laura
- start putting together date / committee:  graduation by May
- Zhaoxia, Veronica, Babak?




## 1/7/26: correcting $\tau$

- `corrections.Rmd`

- pursued correcting $\tau_1$ by the ratio of the number of neurons in layer 1 and the number of neurons selected by layer 2.  I had pushed this to the wayside because I thought that __1)__ determining which neurons layer 2 had selected could seem somewhat arbitrary and __2)__ it seemed like it would not be large enough to change things meaningfully (capped at $d_1$).

  - Realized that I could use PV2017's "effective" number of coefficients idea instead, i.e. $m_{eff} = \sum_{j=1}^p (1 - \kappa_j)$.  So we take the $m_{eff}$ computed in the __second__ layer of the neural network, which indicates the shrinkage placed on the 1st layer outputs (i.e. the output of each neuron).  In other words, our corrected global shrinkage parameter is given by 
$$\tau^{corr}_1 = \hat{\tau}_1 \times \dfrac{d_1}{m_2}$$

Not sure if using $\dfrac{d_1}{m_2}$ or $\sqrt{\dfrac{d_1}{m_2}}$ makes more sense.  ($\sqrt{\dfrac{d_1}{m_2}}$ probably makes more sense since $\tau$ is a multiplicative scaling factor for the Normally-dist'd coefficients; it's also more conservative)


### Correct $\kappa_j$ (prob not):

Applying similar matrix algebra as below to the total shrinkage (essentially, instead of using the diagonal matrices of $z_j$'s from layers 1 and 2 --- $Z_1$ and $Z_2$ --- we replace these with the shrinkage, $1 - \kappa_{l=1, j}$ and $1 - \kappa_{l=2, k}$).  Then we get an expression for the $j,k$^th^ weight of layer 1 as:

$$\begin{aligned}
  w_{j,k} 
  &= 
    (1 - \kappa_{1,j}) (1 - \kappa_{2, k})) \hat{w}_{j,k}
  \\
  & = 
    \left( 
      1 - (\kappa_{1,j} + \kappa_{2,k} - \kappa_{1,j}\kappa_{2,k})
    \right)
    \hat{w}_{j,k}
\end{aligned}$$

In other words, the shrinkage placed on $w_{j,k}$ is equal to $\left( 1 - (\kappa_{1,j} + \kappa_{2,k} - \kappa_{1,j}\kappa_{2,k}) \right)$, suggesting corrected layer 1 $\kappa_j$:
  
$$\kappa_{1,j}^{corr} = \kappa_{1,j} + \kappa_{2,k} - \kappa_{1,j}\kappa_{2,k}$$

- for true covs, $\kappa_j$ is going to be smaller.  If $\kappa_{2,k}$ is larger, it will overpower it.  A similarly small $\kappa_{2,k}$ won't do a lot.

- for nuisance covs, $\kappa_j$ should be close to 1.  so $\kappa_{2,k}$'s effect won't be large.

- and actually, $\kappa_{1,j}^{corr} \geq \kappa_{1,j}$ since $\kappa \in (0,1)$

- if anything this is going to __decrease__ separation.  Quick enough to look at though, I suppose.


### Increase separation:

__What I want: stop the neurons that are 0'd out from ASYMMETRICALLY affecting the layer 1 true covariates' $\kappa_j$'s.__

- For true covariates, presence of 0'd out neurons pushes local scale parameter $\tilde{z}_j$ toward 0;

- presence of 0'd out neurons doesn't affect $\tilde{z}_j$'s for nuisance covariates since should already be near 0.


Seems like the only other things I can look at to mitigate this using the machinery I already have is to use the actual weights???  They already tend to be near 0 for the neurons corresponding to low $\tilde{z}_j$'s in layer 2?





## 1/6/26: effect of $z_j = \tilde{z}_j s$ over multiple layers

Continuing thoughts on possible fix 3 below (correcting the layer 1 $z_j$'s to account for pruning from later layers):

### 2 Layer Network:

$$\begin{align}
  \hat{y}_i 
  & =
    \sigma_1 \left\{  
      \underset{1 \times p}    {X_i} 
      \underset{p \times p}    {Z_1}
      \underset{p \times d_1}  {W_1}
      + \underset{1 \times d_1}{b_1}
    \right\}
    \underset{d_1 \times d_1}  {Z_2}
    \underset{d_1 \times d_2}  {W_2}
    + \underset{d_1 \times d_2}{b_2}
  \\
  & =
    \sigma_1 \left\{  
      \underset{1 \times p}    {X_i} 
      \underset{p \times p}    {Z_1}
      \underset{p \times d_1}  {W_1}
      \underset{d_1 \times d_1}  {Z_2}
      + \underset{1 \times d_1}{b_1}
        \underset{d_1 \times d_1}  {Z_2}
    \right\}
    \underset{d_1 \times d_2}  {W_2}
    + \underset{d_1 \times d_2}{b_2}
  \\
\end{align}$$


- $\sigma_l$, the RELU activation function for layer $l$;

- $Z_l$ = diagonal matrix for layer $l$ with (non-negative) diagonal entries $z_j = \tilde{z}_j s$ for $j \in \{1, 2, ..., d_{l-1} \}$;

  - since $Z_l$ is diagonal and non-negative, we can move it inside the RELU activation $\sigma_{l-1}$:

    - if $B$ is a diagonal matrix, right multiplying a matrix $A$ by $B$ scales the columns of $A$ by the diagonal entries: $$AB = \left[b_{11} \vec{a}_1  ~  b_{22} \vec{a}_2 ~ ... ~ b_{JJ} \vec a_J \right]$$ where $\vec{a}_j, j \in \{1, 2, ..., J\}$ are the column vectors composing $A$.

  - $\tilde{z}_j$, the local scale parameter ($\lambda_j$ in usual horseshoe notation); scalar, non-negative;

    - variational specification: $\tilde{z}_j = \sqrt{\tilde{\alpha} \tilde{\beta}} \sim LogNormal\left(  \dfrac{\mu_{\tilde\alpha} + \mu_{\tilde\beta}}{2} , \dfrac{\sigma_{\tilde\alpha}^2 + \sigma_{\tilde\beta}^2}{4}  \right)$;

  - $s$, the global scale parameter ($\tau$ in usual horseshoe notation); scalar, non-negative;

    - variational specification: $s = \sqrt{s_a s_b} \sim LogNormal\left(  \dfrac{\mu_{s_a} + \mu_{s_b}}{2} , \dfrac{\sigma_{s_a}^2 + \sigma_{s_b}}{4}  \right)$;

- $W_l$, the $d_{l-1} \times d_l$ weight matrix in layer $l$;

- $b_l$, the bias/intercept term in layer $l$;

- $p$, the # of covariates;

- $d_l$, the # of neurons in layer $l$;

  - $d_2$ = dimension of response (in a 2-layer network).  Here, $d_2 = 1$


Then the actual "coefficient" effect of the $j,k$^th^ entry in $W_1$ isn't just $z_{1_j} w_{1_{j,k}}$ for $j \in \{1, 2, ..., p\}, k \in \{1, 2, ..., d_1\}$.  Instead, letting $z_{1_j}$ refer to the $j$^th^ diagonal entry in $Z_1$ (and similarly for $z_{2_k}$) we have:

$$j,k^{\text{th}} \text{ entry of } Z_1 W_1 Z_2 = \left\{  z_{1_j} w_{j,k} z_{2_k} \right\}$$
___

__Thus:__

- one option might be to use some combination of $z_1$ and $z_2$ to determine shrinkage.

- alternatively, to carry the calculation all_all_ the way through the network, correcting the $z_j$'s with some measure of $Z_2 W_2$, e.g. matrix determinant or matrix norm.



### 3-layer ( and more) networks:

$$\begin{align}
  \hat{y}_i
  & =
    \sigma_2 \Big\{
      \sigma_1 \big[
        X_i Z_1 W_1 + b_1
      \big]
      Z_2 W_2 + b_2
    \Big\}
    Z_3 W_3 + b_3
  \\
  & =
   \sigma_2 \Big\{
      \sigma_1 \big[
        X_i Z_1 W_1 Z_2 + b_1 Z_2
      \big]
      W_2 Z_3 + b_2 Z_3
    \Big\}
    W_3 + b_3
\end{align}$$


- becomes rather difficult to track the shrinkage from $Z_l$ across all 3 layers.  The only option that I can think of tha makes sense in this context is using the determinants or some kind of norm (Frobenius?) of $Z_2 W_2$ and $Z_3 W_3$.

  - at least this becomes generalizable to deeper networks.


___

Ok, time to code some of this up.


## 1/5/26: Happy New Year

Let's figure this out.

- Thoughts re: fixing the issue affecting the local shrinkage params $\tilde{z}_j$: 0'd out neurons affect the estimation of the local kappas $\tilde\kappa_j = (1 + \tilde{z}_j^2)^{-1}$ for true covariates, but not for the nuisance covs; essentially, I think the $\tilde{z}_j$'s for true covs are lower than they should be because some neurons should be 0'd out (i.e. the pressure towards 0 for some neurons lowers the shared $\tilde{z}_j$), but the $\tilde{z}_j$'s for nuisance covariates are unaffected because they should _all_ be 0:

  - __possible fix 1__: re-training reduced model; tried already, did not work.

    - did not try with corrected KL and Kaiming initialization, which does improve things
  
  - __possible fix 2__: decoupling the 1st layer's neurons so that not all of them share $\kappa_j$'s or $\tilde{z}_j$'s.  
  
    - Seems difficult?  Perhaps using some kind of Bernoulli to turn them on/off for individual neurons, or assigning each covariate 2 $\kappa_j$'s?
  
  - __possible fix 3__: correcting $\tilde{z}_j$:
  
    - multiply by $\dfrac{\text{total}}{\text{included}}$ 1st layer neurons.  But need to set decision criteria (based on 2nd layer's $\tilde{z}_j$'s) then for inclusion.
    
      - may need to keep in mind that the 2nd layer's $\tilde{z}_j$'s depend on the 3rd layer as well.  Which is a little crazy-making.
      
    - do some matrix algebra to figure out the effective weights, i.e. how the weights in the first layer get multiplied through the network, and use this as the correction.  (Similar to what I've done for the effective shrinkage calculation.)  This seems nice since it doesn't require any thresholding / decision-making as part of the correction.
     
  - __possible fix 4__: correcting $\tilde{\kappa}_j = (1 + \tilde{z}_j^2)^{-1}$:
  
  
  - __possible fix 5__: correcting $\kappa_j = (1 + \tilde{z}_j^2 s^2)^{-1}$:
  
    - Since the effective weight (i.e. after applying shrinkage, moving through to the 2nd layer's preactivation) in the first layer is $\bar{w}_{j,k} = \left(1 - \kappa_{1,j}\right)\left(1 - \kappa_{2,k}\right) \hat{w}_{j,k} = \left[1 - \left(\kappa_{1,j} + \kappa_{2,k} - \kappa_{1,j} \kappa_{2,k}\right) \right]\hat{w}_{j,k}$, set $\gamma_{j,k} = \kappa_{1,j} + \kappa_{2,k} - \kappa_{1,j} \kappa_{2,k}$.  Then use the mean (simple?  geometric?) $\gamma_j = \bar{\gamma}_{j,k}$


- Could work on these in conjunction with a correction to the global scale parameter ($\tau$ in usual horseshoe prior notation, $s$ in mine): $\tau_1 = \hat{\tau_1} \times \left(  1 + \dfrac{p (d_1 - \hat{d_1})}{\hat{d_1}(p - \hat{p})}  \right)$.  Since the fractional term will always be non-negative, $\tau_1 \geq \hat{\tau}_1$.  The issue here is 1) setting a threshold for inclusion of neurons (determining $\hat{d}_1$) and 2) determining $\hat{p}$

## 12/27:

New models --- corrected KL, calibrating $\tau_0$ via Piironen Vehtari 2017 (PV2017) to assume half covars should be trimmed, Kaiming initialization ---, are doing well in terms of MSE and function fitting.  


__However, same problem persists__: small global scale param $\tau$ is still pushing global $\kappa_j$'s too high, whereas local $\tilde\kappa_j$'s exhibit extreme separation.  

  - is it extreme for the amount of observations (10k) that I'm training on?  
  
  - seems a little unintuitive that posterior $\tau$ should get smaller with more obs.  Why exactly is that?  Is it supposed to work this way at high $n$, where $\tau$ gets pushed towards 0 while $\lambda_j$'s go to extremes?  
  
__Wait.  Have I been looking at this the wrong way?  Maybe the $\lambda_j$'s need to be pushed further apart??__


___

Looking at effect of 2 possible corrections to global scale parameter / kappas:  

1. __correcting global scale parameter $\tau$__:    

  - Piironen Vehtari 2017: suggest setting $\tau_0 = \dfrac{p_0}{d - p_0} \dfrac{\sigma}{\sqrt{n}}$ because posterior expected number of effective parameters ($m_{eff}$) is given by $E(m_{eff} | \tau, \sigma) = D \dfrac{  \tau \sigma^{-1} \sqrt{n}  }{  1 + \tau \sigma^{-1} \sqrt{n}  }$, where $D$ is the dimension of $\beta$.    

  - However, in our setting, we have additional dropout from the 2nd layer, which pushes some neurons' output to 0, i.e. the ratio $\dfrac{p_0}{d - p_0}$ is not the same for every neuron.  For neurons that should be pruned (as indicated by 2nd layer's shrinkage), this is simply going to be 0.    

  - If we were to pretend that we simply had a $d_1 * p$ length covariate vector, instead of $p$ covariates fed into $d_1$ neurons, then what the BNN produces as the first-layer estimate $\hat\tau_1$ seems more like the following  
  $$\hat\tau = \dfrac{  \hat{p} \hat{d_1}  }{  p d_1 - \hat{p} \hat{d_1}  } \dfrac{\sigma}{\sqrt{n}}$$,
where $\hat{p}$ = the estimated number of nonzero coefs, $\hat{d_1}$ = the number of neurons that are _not_ dropped.    

  - thus our first-layer estimate $\hat\tau_1$ will always be $\leq$ than the "true" $\tau_1 = \dfrac{ \hat{p} }{ p - \hat{p} } \dfrac{\sigma}{\sqrt{n}}$, because $\hat{d_1} \leq d_1$.    

  - and to recover $\tau_1$ from $\hat{\tau_1}$, we have:
  $$\tau_1 = \hat{\tau_1} \times \left(  1 + \dfrac{p (d_1 - \hat{d_1})}{\hat{d_1}(p - \hat{p})}  \right)$$
  

2. __consider actual shrinkage by end of next network layer:__  

- in a 2-layer network, shrinkage of first layer weights $W_1$ ends up being:
$$\underset{p \times d_1}{\bar{W_1}} =   \underset{p \times p}{(1 - \kappa_1)}   \underset{p \times d_1}{\hat{W_1}}    \underset{d_1 \times d_1}{(1 - \kappa_2)}$$,
Where matrices $1 - \kappa_l$ are diagonal matrices with $1 - \kappa_j$ for the $l$'th layer on the diagonal.    

- so the actual shrinkage experienced by the weight in the $j$th row and $k$th column ends up being 
$$\bar{w}_{j,k} = \left(1 - \kappa_{1,j}\right)\left(1 - \kappa_{2,k}\right) \hat{w}_{j,k} = \left[1 - \left(\kappa_{1,j} + \kappa_{2,k} - \kappa_{1,j} \kappa_{2,k}\right) \right]\hat{w}_{j,k}$$


### new perspective: work on $\lambda_j$'s

- maybe I need to think about if the actual correction needs to be to the __local__ scale parameters to make up for the v. small global scale params?

- each covariate's $\kappa_j$ is shared between $d_l$ neurons (in our case, 16).

- the 2nd layer's $\kappa_j$'s will indicate that some of the 1st layer's neurons should be 0'd out.

- The 1st layer neurons that get 0'd out DON'T affect the $\lambda_j$'s when a coefficient should be 0.

- BUT when a coefficient should NOT be 0, the 1st layer neurons that get 0'd out (by the 2nd layer) make the $\lambda_j$ SMALLER than it should be.

- SO WE SHOULD HAVE EVEN MORE SEPARATION in the local scale params than we are seeing.  This makes more sense to me.




# unformatted notes

- kept research notes as .txt file before this point.  
- mostly unaltered apart from fixing into descending chronological order (most recent at top) and making each date a heading


#### 12/19
- currently running: hshoe_smooth_pvtau_1721632
  - 5 sims, 1M epochs
  - 3 layers, 16, 32 hidden; smoother fcns
  - first layer prior tau_0 set assuming 1/2 vars true / even odds
- to do:
  - make diagnostic functions so it's easier to check completed models
    - \# params
    - function plots
    - MSE / KL plots
    - different decision rules:
      - global/local kappas, BFDR
      - how to use alphas with BFDR?
      - decision rule based on Bayesian intervals


- leaky relu does not work.

- 5 layers, 8 units each; smoother fcns --- v. bad; MSE 2.8; just gives back noise
  - can I figure out a guideline for what model size works?   Does this just have too many params?
  - \# params: 104 * 8 + 3 * 8 * 8 + 8 = 1032 weights.  x2 (mu and sig)  
    - atil, btil: 2 * (104 + 4 * 8).  x2 (mu and var)
    - sa, sb: 5 layers x 2. x2 (mu and var)

- 3 layers, 16, 32 hidden; smoother fcns ---- mixed; MSE ~ 1, recovers functions well
  - first layer prior tau_0 set assuming 1/2 vars true / even odds
  - global kappas not great; 
  - alphas very good (all 4 chosen; 1 FP); continuing training might move more in right direction
  - need to look at local kappas
  - \# params: 104 * 16 + 16 * 32 + 32 = 2208 weights.  
  - hshoe_smoother_21632_12500obs_215244


params: 
- sa, sb:  4 params per layer
- atil, btil: 4 params per input dimension
- weights: 2 params per weight: input x output dim
- biases: 2 params per output dimension




#### 12/17:
- activation scaling doesn't work!  just ends up not training anymore, exarcebated extremely small global scale params
- smoother functions - 
   - consistently higher MSE?  1.82 or so (whereas old functions had ~ 1.2)
   - weirdly, "new" function 2 is not detected (function drawn is just noisy horizontal line)
     - strange b/c old function 2 was the same with an added "-x" term.  Periodic function and downward slope were modeled fine before.
   - seems to converge faster (stops improving around 300k iterations, actually)   

- current simulations:
  - does leaky relu work better?
  - kappas seem better, now need to check if it's from correction to KL and initialization, or from using smoother functions

- working on getting competitors (lm, spike-slab, BART) running
- need to consider better simulation data?  
   - data with interaction / effect modification?


Currently running:
- started first: (using `batching.R`) --- original functions and model (2 layers, 16 8), with corrected KL and kaiming init.
- started 2nd: smoother_fcns_kaiming --- smoother functions, 5 layers, leaky relu, 8 in all
- started 3rd: smoother_fcns_kaiming --- smoother functions, 2 layers, leaky relu, 32 32






#### 12/16
- ablation: try smoother_functions_kaiming with 
  - larger BNN?  original set of functions?
- test activation scaling - `torch_horseshoe_scaledacts.R`



#### 12/14
- `smoother_functions_kaiming.R`: smoother functions, kaiming initialization
  - tau_0 = 1
  - RESULTS: not bad:
    - fcn2 (high freq periodic function, asymmetric around origin) not captured
    - other fcns OK, a bit noisy
    - kappas look more reliable:
      - nuisance variables rejected with high certainty
      - true covs 0.26 0.97 0.49 0.78
    - global shrinkage looks about right
    




#### 11/20

testing over the last few weeks suggests that 
1) when the network's parameters outnumber the observations we have to work with, the unmodified alpha parameter is best to work with, and we should stop training early (otherwise the network will overfit by fitting nuisance variables to noise / fill in gaps)

2) when the number of observations is smaller than the number of parameters, training for a long time is helpful, but we should use the alphas after centering them at the geometric mean

3) perhaps the first hidden layer should be smaller in dimension?

16 -> 32 -> 64



need to save FP TP FN TN alongside tt_mse






11/12
- smoother functions X
- scale layer activations?
- use He initialization?  supposed to be better than Xavier initialization for RELU
  - each weight sampled from N(0, 2 / n_{l-1})
- Hu 2020 - orthogonal initialization
- maximal update parametrization

nn_init_kaiming_normal_(mod$fc1$weight_mu)





#### 11/10

ideas to try:
- scale outputs by layer dimension
  - divide pre-activations by d --- mitigate tau getting too small?
    - e.g. Y1 + Y1 + Y1 + Y1 vs Y1 + Y2 + Y3 + Y4


- better prior for tau 
  - based on Piironen & Vehtari 2017


- different / smoother data-generating functions


- justify multiplying / dividing scale param






#### 11/8

- need to monitor test-train split along with TP TN rate
- run 10 times
- keep track of test, train, FP, TP
- sweet spot tends to be btwn 5-8k epochs
- start storing at 3000 epochs
- need to get a sense of how moving average changes?
- is there a time when it gets all of them correct?
- check for ranking
- tt_mse
- earliest best is at 3500 epochs
- FP at 6000
- diverging at 6500
- just gets worse and worse


- part of the problem might be that we have more parameters than data, so 
- the network can memorize the data.  So CV will keep going in circles.
- for the alphas, cap them at 1, then renormalize
- what if drop nodes in the hidden layers, then retrain? using starting points for hidden layers?
- some support for this as a VAE procedure



- training regimes tested
    - 5-fold CV, 1000 obs, hidden layers 50, 25 - same issues
        - KL seems inappropriately scaled?  at 2500 epochs in, around 23
    - no CV - bad
    - 5-fold CV, 10k obs, hidden layers 25, 25
        - seems more stable?  KL is better scaled 
            - 500 epochs, MSE = 5, KL = 1.6
            - 4000 epochs, MSE = 2.57, KL = 1.15
            - 7000 epochs, MSE = 1.8, KL = 0.74
    - tried with only 5 vars (so only 1 nuisance) to see how estimation goes. 
    	- fcn estimation is actually fairly good BUT
    	- the 1 nuisance variable's alpha appears to be going to 0 as training goes on
    	- for some reason all the predicted curves appear to be shifted up
    	- function estimation is really good around x = 0; because all the covs are generated as N(0,1), so there's the most data there.


- monitoring predicted functions



ideas:
    - consider training 1x, eliminating nodes with high alpha in hidden layers, retraining
    - what about eliminating nodes with alpha > 1 at some point
    - function estimation seems a bit fucky, probably because keeping all these unused vars in?
    - look at the clip variances thing in Ullrich's code







#### March 18

Job  
by Joseph Millar  

I’ve just come from walking to and fro  
in the earth, Satan tells God  
before they make the wager  
standing for centuries  
as metaphor of man’s existence—   
trapped on the wheel like an insect  
under a microscope:  
his disastrous ecology,  
his ravaged immune system,  
even his broken-veined, wine-flushed face  
looking back from the rearview  
and parked alone by the river.  
He should have been born  
with fins, he thinks  
as the swans arch and preen  
and attack one another  
though everyone says they mate for life  
and the afternoon wind  
raises welts of sunlight  
over the torqued and rippling surface  
and the beautiful ravenous fish.  





#### 3/19 BVS for DNN lit review

---- Variational Inf and dropout/compression

- Gal & Ghahramani, 2015; Kingma et al 2015 

- Kingma et al 2015, "Variational Dropout and the local reparameterization trick" --- show that dropout can be seen as a special case of Bayesian regularization

- Molchanov, Ashukha, Vetrov 2017, "Variational Dropout Sparsifies Deep Neural Networks"
  - each weight has its own, individual dropout rate
  - __has a good lit review__
  - in discussion, "Another possible direction for future research is to find a way to obtain __structured sparsity using our framework__"



- Louizos, Ullrich, Welling 2017, Bayesian Compression for Deep Learning
  - apply sparsity-inducing Gaussian mixture models to ROWS of a neural network weight matrix (not the nodes, but it also induces sparsity in nodes)
  - network weights estimated via variational inference
    - improper log-normal Jeffrey's prior --- later shown to be misspecified
    - Horseshoe
  - focus is on compression; dropout parameters treated as tuning parameters
  - also compress via reducing bit precision



- Ghosh, Yao, Doshi-Velez 2019, Model Selection in Bayesian Neural Networks via Horseshoe Priors
  "Model selection" here is about choice of NN architecture, i.e. how many layers, how many nodes
  - also show that too many nodes in a BNN --> large predictive uncertainty; "more expressive models require more data to concentrate the posterior"


- Overweg et al, 2019, Interpretable Outcome Prediction with Sparse Bayesian Neural Networks in Intensive Care
  - only apply HShoe to input layer
  - no simulation results --- just applied to datasets in UCI ML repository, and report root mean-squared error and negative log-likelihood
  - no investigation of variable selection performance, e.g. T1 error, FDR
  - no decision rule / criteria
  - little investigation of using larger NN's


- Nguyen, et al, 2021, "Structured Dropout Variational Inference for Bayesian Neural Networks"
  - uses Householder transformation to learn representation for multiplicative Gaussian noise in Varitaionl Dropout
  - obtain Variational Dropout posterior with structured covariance
  - hierarchical dropout procedure --- equivalent to inferring a joint posterior 




---- Normalizing Flows

- Huang, Krueger, Lacoste, Courville 2018, "Neural Autoregressive Flows"
  - normalizing flows and autoregressive models
  - "universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions"



- look into: 
  - bits-back argument and bits-back coding, 
  - Ghosh, Yao Doshi-Velez paper
  - Nguyen paper
  - Automatic Relevance Determination effect
  - code for Molchanov, Ashukha, Vertrov 2017 ??



#### Feb 11
interesting talk at Qualcomm by Van Der Wilk:

https://youtu.be/m1dSrXBEZIQ
https://www.youtube.com/watch?v=m1dSrXBEZIQ
ELBO good in deep Gaussian Processes (Damianou Lawrence 2)

For linear models, when ensembling, if initialize weights from iid prior, then apply gradient descent w.r.t. squared loss (unregularized)
 ---> then resulting weights distributed exactly as posterior, i.e. estimated ensemble weights exactly equal to posterior (Matthews et al 2017)

 Also, can get lower bound by summing training losses over observations --- Lyle, Schut, Ru, Gal, van der Wilk

